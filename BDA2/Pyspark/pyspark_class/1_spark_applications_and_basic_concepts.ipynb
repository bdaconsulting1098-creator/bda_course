{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "461b9e51-dde0-4f43-9a9a-47cf92b44056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6278ab46-26c3-4ff8-9464-64f097f00f35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1: Spark Applications and Basic Spark Concepts\n",
    "[pyspark Doc](https://spark.apache.org/docs/2.4.5/api/python/index.html)\n",
    "\n",
    "Spark provides two sets of APIs, *Structured APIs* and *low-level APIs*. The Structured APIs are designed to implement the business logic of Spark applications and they hide the Spark internals of the *low-level API*. So for us , as a ETL developer, the Structured APIs are the best starting point to dive into data processing with Spark.\n",
    "\n",
    "Today's challenge is to write our first little Spark application to get to get a first impression of the *Structured APIs* like `DataFrame`,`Dataset`, *SQL Tables/Views* and *Structured Streaming* and to undertsand some basic concepts like lazy evaluation of transformations, and data processing actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9731bbee-df59-4781-9e68-f9578aa57b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66e07d62-592a-41d7-8253-b59aa4ac25d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "680a0a8b-706b-40fd-9a36-4e78fe32a9a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The first questions, that comes to our mind is, how to start a Spark application?\n",
    "\n",
    "## SparkSession\n",
    "Starting a Spark application generates a Spark job which is controlled and mangaged by exactly one *driver process* and several *executor processes* running across the cluster nodes doing the actual computational work. The driver process is controlled by a`SparkSession` object, which is the entry point of any Spark application, so there is always a one-to-one relationship between SparkSession and Spark application.\n",
    "\n",
    "So how can westart a Spark session?\n",
    "\n",
    "Since we don't want to type in all the code line by line into the interactive console, our Spark application must create it's own `SparkSession`object. So every Spark application starts with something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f41a6d8-a6d0-4783-9f02-91672337ed09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff1798d-4602-4c58-8a09-c274bf7aa048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3d46f2c-2439-4d7e-968d-54bc3790a443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark UI\n",
    "Spark provides an UI to monitor the status and progess of Spark jobs. It is available on port 4040 on the driver node in the Spark cluster. Since running Spark in local mode, i.e. all processes are running on our laptop, we can access the UI on http://localhost:4040.\n",
    "\n",
    "After having executed the next code block in the Spark DataFrames section, the UI showed this to us.\n",
    "\n",
    "<img src= \"./screenshots/day-002/day-002_Spark_UI.jpeg\">\n",
    "\n",
    "Clicking on a jobname provides further details and us trics figures regarding the job execution icluding a graphical representation of the execution DAG (directed acyclic graph).\n",
    "\n",
    "<img src= \"./screenshots/day-002/day-002_Spark_UI_job_details.jpeg\">\n",
    "\n",
    "\n",
    "## Spark DataFrames\n",
    "run hello world example which creates our first `DataFrame`, and similar to the interactive console, the starting point is the `SparkSession` object named *spark*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e6b3cf1-41e2-4f11-b86c-4f9bd12f0a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d948ba3d-7f66-4670-af13-1bfaae5173eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ourFirstDataFrame = spark.range(100).toDF(\"number\")\n",
    "ourFirstDataFrame.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61c98164-27e6-4d1e-b7b8-e02fd9915f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Spark `DataFrame` objects look quite similar to Pandas dataframes. In fact, we can easily transform a Spark `DataFrame` into a Pandas dataframe.\n",
    "\n",
    "***But caution:*** This us thod should only be used if the resulting Pandas’s DataFrame is expected to be small, as all the data is loaded into the driver’s us mory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b145a4-bb53-4ca2-a333-97c2da8bae76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandasDF = ourFirstDataFrame.toPandas()\n",
    "pandasDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f0f8e1b-4abb-4216-a8cc-2349e2106812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "By the way, tranforming data to JSON is also easy. Each row is turned into a JSON document as one element in the returned list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae15365-6728-4954-95b7-f7bf0cd200b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spark Connect does not support toJSON(). Use pandas for JSON conversion instead.\n",
    "import json\n",
    "json_records = ourFirstDataFrame.toPandas().to_dict(orient=\"records\")\n",
    "# Display as JSON string (first 10 rows)\n",
    "print(json.dumps(json_records[:10], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b43769bc-fdfc-4c78-ac29-85291d5ec468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ok, back to topic. The main difference bewteen Spark and Pandas is, that Pandas dataframes reside on a single machine whereas a Spark `DataFrame`ìs an abstraction of the in-memory optimized low-level API *Resilient Distributed Dataset* (`RDD`), which is designed to be split up data into partitions which can be spread across a cluster of potentially thousends of nodes. \n",
    "Spark `DataFrame`objects have a surprising characteristic, they are immutable once they are created. So how can data processing works when data structures are written in stone?\n",
    "## Lazy Evaluation, Transformations and Actions\n",
    "The few lines of our code already demonstrate that the Structured API has a functional design. Since `DataFrame`objects are immutable, we have to use functions which read `DataFrame` objects as input, do some kind of data transformation and create a new `DataFrame`which again can be the input of another function to do further transformations and generating another `DataFrame`. So finally we can simply concatenate functions to create a sequence of transformations to get our desired data result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f27204a-f061-4502-9ccc-a89e0b2cf9f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09a09dc4-795c-4456-9a18-3dfd47be95f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ok, let's do it. we want to see, if how it works. First, we want to read some data from CSV file into a dataframe. The file has a header line and we want Spark to derive the schema, i.e. name and type of the columns, from the file. Nevertheless we could also specify a schema explicitly instead of deriving it from file. Determining the schema processing time, instead of load time, is an example of the common *schema-on-read* design of Big Data architectures.\n",
    "\n",
    "Important to keep in mind is, that the column types are not Python types (or Scala or Java types if we use another API languages). All language API commands are mapped to the Spark internal language *Catalyst* having its own types. That's why all API languages provide the same performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6205eae9-60c9-4564-b038-7d273185c5af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Volume 配置 (假设与前述保持一致) ---\n",
    "CATALOG_NAME = \"workspace\"\n",
    "SCHEMA_NAME = \"default\"\n",
    "VOLUME_NAME = \"course_data\"\n",
    "# 根据你的 DATA_MAPPINGS，假设 flight-data 位于 BDA2_Data 中\n",
    "VOLUME_SUBFOLDER = \"BDA2_Data\" \n",
    "FILE_NAME = \"flight-data/2015-summary.csv\" \n",
    "\n",
    "# 构造完整的 Volume 路径\n",
    "file_path = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}/{VOLUME_SUBFOLDER}/{FILE_NAME}\"\n",
    "\n",
    "dfFlight = spark.read\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .csv(file_path)\n",
    "\n",
    "print(\"✅ DataFrame 已成功加载。\")\n",
    "dfFlight.printSchema()\n",
    "dfFlight.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6c9d227-38b4-424b-86fe-ead2fabe5bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Running this code doesn't show any observable result to us . That looks strange on first view. Actually, Spark hasn't done anything yet, except for deriving the schema by reading a small sample of rows. This is because Spark applies *lazy evaluation* of *transformations*, i.e. no data is moved or processed until Spark is forced to by an *action*, e.g. by calling the `write()` function. \n",
    "\n",
    "By defining transformations, we just give Spark a set of rules describing how a given `Dataframe` should be logically transformed into a new `Dataframe`object. By calling an action, we give Spark the command to apply these transformations, process the data and provideusthe results.\n",
    "\n",
    "The reason for the lazy evaluation approach is, that Spark first wants to know the whole story about *what* should be done effectively before it tries to determine an efficient way *how* to do this. Therefore Spark first compiles all transformations into a **logical** directed acyclic graph (DAG), than analyses this DAG, applies optimizations (e.g. predicate push-down to datasources) whenever possible and splits up the optimized **physical** DAG into stages and parallelised tasks of `RDD` manipulations before starting to execute them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a386c5f2-71ef-42a7-8197-01f1970b50d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd9dfd7d-7dc0-4102-84bc-25813301e769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Important to note is, tha `DataFrame`objects are kept in us mory when ever possible. In contrast to MapReduce, Spark tries to avoid writing intermediate results (i.e. `DataFrame` objects) to disk by *piplining* consecutive in-memory transformations, to gain better performance.\n",
    "\n",
    "This piplining is only possible for *narrow* transformations. These are transformations where each input partition contributes only to one output partition or where the transormation can be applied partion by partition, so the partitions can be processed locally on the same cluster node. Simple row-based filter rules or commutative operations like summing up values, are common examples of narrow transformations.\n",
    "\n",
    "On the other hand, in a *wide* transformation input partitions contribute to multiple output partitions, so data needs to be shuffled across cluster nodes. Sorting and average calculation are common wide transformations across multiple partitions. During shuffling Spark writes results to disk, so wide transformations are not performed in-memory.\n",
    "\n",
    "\n",
    "Ok, now we want to see some action and Spark to showusthe first 10 lines in our data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4085ccf4-3c2c-455f-b36c-d51a04942209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfFlight.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a43e2c9c-281f-4271-a6dc-1d400b7ffa93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we triggered actual data processing so we can see the results. Next to showing data, there are two other types of actions: writing output data, e.g. to file and actions to collect data to native objects in the respercitve language.\n",
    "\n",
    "we can even combine transformations and actions in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "276a3445-c5cb-44b9-aae6-d84c40c7f307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Volume 配置 (假设与前述保持一致) ---\n",
    "CATALOG_NAME = \"workspace\"\n",
    "SCHEMA_NAME = \"default\"\n",
    "VOLUME_NAME = \"course_data\"\n",
    "\n",
    "# 假设这个文件在 BDA2 文件夹中 (请根据你的 GitHub 仓库结构确认)\n",
    "VOLUME_SUBFOLDER = \"BDA2_Data\" \n",
    "FILE_NAME = \"flight-data/2015-summary.csv\" \n",
    "\n",
    "# 构造完整的 Volume 路径\n",
    "file_path = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}/{VOLUME_SUBFOLDER}/{FILE_NAME}\"\n",
    "\n",
    "print(f\"尝试读取路径: {file_path}\")\n",
    "\n",
    "# 修正后的 Spark 读取代码\n",
    "\n",
    "dfFlight = spark.read\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .csv(file_path) # <--- 使用了构造好的绝对路径\n",
    "   \n",
    "dfFlight.show(10)\n",
    "\n",
    "print(\"✅ DataFrame 已成功加载并显示前 10 行。\")\n",
    "\n",
    "# 再次读取并显示前 10 行（使用绝对路径）\n",
    "spark.read\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .csv(file_path)\\\n",
    "   .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6efbf4d-deeb-4c33-b130-8ca733129a25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Obviously we can split up any sequence of transformations by asigning the intermedate `Dataframe` object to a variable and have a look into the intermediate results by calling the`show()` function on that `Dataframe`. This is a very nice feature foruswhen we need to debug complex analytical queries or ETL jobs which compile severeal subqueries together. If our subqueries provide the expected result, the bug must reside in the remaining part of our transormation logic so we can focus our analysis on that area.\n",
    "## Query Explain Plans\n",
    "So we've learned so far, that we just need to define the business logic by concatinating transformation functions and Spark does the optimisation for us . Fortunately Spark givesusinsight, how it will perform our query by calling the `explain()` function.\n",
    "\n",
    "we want to see, how Spark would **physically** execute the sorting, which is a wide transformation. Each step in the explain plan actually generate a new `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb547be-94e5-4e8d-8c46-605c0e89dd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfFlight.sort(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eeea759-8551-4a6f-a7d0-f24485f8d5f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading the explain plan from bottom upwards, it tells us , that first, Spark performs a file scan and than range partitioning is applied shuffling the data over 200 output partitions by default to sort the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daaed32d-f56e-4a6d-9c67-8988c8575dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62b04e17-22d2-4f8f-a402-49449712caa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since we are  running in local mode on a single machine, it might be better to limit the number of partitions to 5. we can do this be chaging the configuration of the `SparkSession` object *spark*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81fd01b6-7b70-47c6-b7c5-9fb43d3c6f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ecefe0f-2fb1-4973-aa62-0d908d4e5dd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The explain plan confirms, that our configuration change has the desired effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b19bcd-7f83-4fdf-827f-08895492d365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfFlight.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d35d4fbe-e81f-43be-a5db-a94d1a113d14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The `explain()` function can helpusfiguring out how flexible we can chain up functions which define `DataFrame` to `DataFrame` transformations. For example, is it relevant for the query execution, whether we filter before selecting or the other way around? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fc18a84-91b9-439d-85c8-cb0e99b2aabf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# 如果你已经在 Notebook 中定义了 spark session，可以跳过上面两行\n",
    "\n",
    "# --- Volume 配置 (假设与前述保持一致) ---\n",
    "CATALOG_NAME = \"workspace\"\n",
    "SCHEMA_NAME = \"default\"\n",
    "VOLUME_NAME = \"course_data\"\n",
    "\n",
    "# 请根据你的实际部署情况修改 VOLUME_SUBFOLDER！\n",
    "# 假设 retail-data 位于 BDA2_Data 文件夹内\n",
    "VOLUME_SUBFOLDER = \"BDA2_Data\" \n",
    "\n",
    "# 构造完整的 Volume 路径到包含 CSV 文件的文件夹\n",
    "file_path_pattern = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}/{VOLUME_SUBFOLDER}/retail-data/by-day/*.csv\"\n",
    "\n",
    "print(f\"尝试读取的路径模式: {file_path_pattern}\")\n",
    "\n",
    "# 修正后的 Spark 读取代码\n",
    "dfRetail = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(file_path_pattern) # <--- 使用了构造好的绝对路径模式\n",
    "\n",
    "print(\"✅ DataFrame 已成功加载。\")\n",
    "dfRetail.printSchema()\n",
    "dfRetail.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20e44ee9-bb51-468c-85d8-4450f2ee76d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Can we get a performance benefit, when we filter very early?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344f1774-9908-4a9a-b827-578e7a060511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dfRetail.where(col(\"InvoiceNo\") != 536365)\\\n",
    "    .select(\"InvoiceNo\", \"Description\")\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea53f500-e86d-47f2-a6eb-7dcac3718d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Or can we stick to the well-known SQL pattern: SELECT ... FROM ... WHERE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5a09ea1-79e8-487e-bc7b-bd8190860b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfRetail.select(\"InvoiceNo\", \"Description\")\\\n",
    "    .where(col(\"InvoiceNo\") != 536365)\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30707e3f-f57a-4396-af98-1b1daa0584e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The execution plan is the same. Again Spark does the optimization in the background and performs the filter before the column projection. In this case, the functional PAI providesusmore flexibility than the strict SQL syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b6da23-da5c-4af3-8d97-601db79764b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark SQL, Tables and Views\n",
    "<a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html\">SQL Language Reference</a> provided by Databricks.\n",
    "\n",
    "[abc](https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html)\n",
    "\n",
    "we've been working with relational databases and SQL for many years. So we are  happy to notice, that Spark also speaks our languange. In fact, the Spark SQL API supports the ANSI SQL 2003 standard. we can turn a `Dataframe` into a table or view, which we can query with SQL. All we need to do is register a table/view on that `Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835e3b76-43de-4bf1-a04a-1e56740e4f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfFlight.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be7cca4-1703-4161-bbed-e93cb0f6bd89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we can write our Spark query as we did it for long times in classical databases, and we will get exactly the same result, as doing it the functional way.\n",
    "\n",
    "This example calculates the top 5 countries having the highest number of flight destinations in 2015. Obviously the most flights went to the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c521a9a-f6d2-4f19-8f8a-da92d6cacd4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# finding the top five destination countries by SQL\n",
    "from pyspark.sql.functions import max, desc\n",
    "# transformation\n",
    "maxSql = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\"\"\")\n",
    "# action\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca28cac2-9f5e-414d-8008-c093fd3c5455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The equivalent functional query looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a14b98d0-05d4-4a9d-bb08-35c96aba0b15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#transformation\n",
    "dfFlight\\\n",
    "   .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "   .sum(\"count\")\\\n",
    "   .withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    "   .sort(desc(\"destination_total\"))\\\n",
    "   .limit(5)\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02b9842e-9cc7-476a-841e-601139479543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The functional version looks touseven more self-explaing the **logical** transformations. The story reading it line-by-line is: first the data is grouped by the destination countries. Than, for each group (partition) the number of flights are summed up which generates a new, derived column which is than renamed. Afterwards the results are sorted is descending order by the calculated column and the output is limited by the top 5 rows.\n",
    "\n",
    "Fortunately the convenience of using Spark SQL API instead of functions does not have an negative performance impact. The **physical** execution explain plans of both versions are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ca3dc0d-00e6-491f-991d-19735f519962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# both transformartions compile to the same plan\n",
    "maxSql.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3126ed-22eb-431f-b352-a5ce09a4b774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfFlight\\\n",
    "   .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "   .sum(\"count\")\\\n",
    "   .withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    "   .sort(desc(\"destination_total\"))\\\n",
    "   .limit(5)\\\n",
    "   .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbee5e77-38b3-4d6e-bdf1-da37cef6600b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Interesting to note is that the `sum()` aggregation involves two *hashAggregate* steps. Because `sum()` is a commutative operation Spark first calculates partial sums partition by partition which is a narrow transformation. Afterwards the aggregted, i.e. already reduced data is shuffled (*Exchange hashpartitioning*) to calculate the overall sum across all partitions. This is another example how Spark optimizes the query execution by first analyzing  all transformations befor starting data processing.\n",
    "## Spark Datasets\n",
    "Datasets are a type-safe version of DataFrames. Since Python is a dynamically taped language, they are not available in pyspark but they can be used in the Java and Scala API. Good to keep in mind, but we skip it for now, since we prefer Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7b28b79-ed9d-463a-85b4-669b044ae07f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Structured Streaming\n",
    "So far we did all data processing in batch mode, i.e. all data get's processed at once. Batch mode forcesusto wait, until all data we want to analyse is available. Stream processing on the other hand enablesusto process data incrementally as it arrives, so we can get insights faster.\n",
    "\n",
    "Stream processing in Spark is very similar to data processing in batch mode. The following example will demonstrate this. As far as we can see so for now, this is because Spark stream processing is actually event-triggered mirco-batch-processing. \n",
    "\n",
    "Ok, let's have a closer look and start from scratch, i.e. creating a new `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55649b5e-02f6-4364-8f16-cbbd45a3f6d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, column, col, sum\n",
    "\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "# spark = SparkSession.builder.master(\"local\").config(\"spark.driver.memory\", \"8g\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09eaac51-2e7c-45d5-9108-0546f5acd13f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Batch processing\n",
    "This time, our data source is not a single file, instead the data is split into several files on a day-by-day basis. Nevertheless, infering the schema, the us ta data, works the same way. The only difference is the wildcard * in the filename to tell Spark, that we want to process all CSV files in the specified folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "becf8e90-4f2f-435c-9de8-a3d8633ac706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Volume 配置 (假设与前述保持一致) ---\n",
    "CATALOG_NAME = \"workspace\"\n",
    "SCHEMA_NAME = \"default\"\n",
    "VOLUME_NAME = \"course_data\"\n",
    "\n",
    "# 请根据你的实际部署情况修改 VOLUME_SUBFOLDER！\n",
    "# 假设 retail-data 位于 BDA2_Data 文件夹内\n",
    "VOLUME_SUBFOLDER = \"BDA2_Data\" \n",
    "\n",
    "# 构造完整的 Volume 路径到包含 CSV 文件的文件夹\n",
    "# 这里的路径是 Volume 路径 + 仓库内部的 data 路径 + 文件模式\n",
    "file_path_pattern = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}/{VOLUME_SUBFOLDER}/retail-data/by-day/*.csv\"\n",
    "\n",
    "print(f\"尝试读取的路径模式: {file_path_pattern}\")\n",
    "\n",
    "# 修正后的 Spark 读取代码\n",
    "staticDF = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(file_path_pattern) # <--- 使用了构造好的绝对路径模式\n",
    "\n",
    "print(\"✅ DataFrame 已成功加载。\")\n",
    "staticDF.printSchema()\n",
    "staticDF.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84c5799d-81a6-4842-9c11-97e3834d0335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This defines the transformation which  tells Spark how to create the source DataFrame.\n",
    "\n",
    "As a retailer, we want to analyse how much money each customer is pending in our shops per hour in each 1 day time window.\n",
    "So we add a further transformation on that `DataFrame`, which describes the business logic of our data analysis, and results to another `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc5fb8d-af9b-4c30-ad1c-2d4a75a143b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = staticDF\\\n",
    "   .selectExpr(\"CustomerId\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\")\\\n",
    "   .groupBy(\"CustomerId\", window(\"InvoiceDate\", \"1 day\"))\\\n",
    "   .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a37324f3-814f-4965-a6bb-0f4d256f7c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To take a look at the first 10 rows of the result, we have to call the action `show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb9f5ad-25f4-401f-82aa-4f8f8cabdc76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f25daa21-694b-40cc-a248-a9488f9c9a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In batch mode, we are  actually processing the entire data history, i.e. all files at once, which can take quite a long time for large data sets. To make this faster, we can switch to stream processing.\n",
    "### Stream processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed9ec27d-884f-4a6f-9947-3fda3c65a0d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are just two things, we have to do, to turn our batch processing into stream processing in Spark:\n",
    " - using `readStream()` instead of `read()`, and\n",
    " - defining a trigger refreshing the result after reading each input file\n",
    "\n",
    "In the given example, a trigger get's fired after reading each file (*maxFilesPerTrigger* = 1). Since all files are already on our harddrive, Spark will actually refresh the results every few (milli-)seconds, so finally we are  quite close to realtime-processing in this demonstration.\n",
    "\n",
    "The schema is the same, as for batch processing, so we are  re-using it from the *staticDF*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b92412c1-4552-45d4-a293-d727119efb59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"YourAppName\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70570d9d-4b93-47e7-9045-1bfdb5d0ac51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Volume 配置 (假设与前述保持一致) ---\n",
    "CATALOG_NAME = \"workspace\"\n",
    "SCHEMA_NAME = \"default\"\n",
    "VOLUME_NAME = \"course_data\"\n",
    "\n",
    "# 请根据你的实际部署情况修改 VOLUME_SUBFOLDER！\n",
    "# 假设 retail-data 位于 BDA2_Data 文件夹内\n",
    "VOLUME_SUBFOLDER = \"BDA2_Data\" \n",
    "\n",
    "# 构造完整的 Volume 路径到包含 CSV 文件的文件夹\n",
    "# 这里的路径是 Volume 路径 + 仓库内部的 data 路径 + 文件模式\n",
    "file_path_pattern = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}/{VOLUME_SUBFOLDER}/retail-data/by-day/*.csv\"\n",
    "\n",
    "print(f\"尝试读取流的路径模式: {file_path_pattern}\")\n",
    "\n",
    "# 修正后的 Spark Streaming 读取代码\n",
    "streamingDF = spark.readStream\\\n",
    "   .schema(staticDF.schema)\\\n",
    "   .option(\"maxFilesPerTrigger\", 1)\\\n",
    "   .format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .load(file_path_pattern) # <--- 使用了构造好的绝对路径模式\n",
    "\n",
    "print(\"✅ Streaming DataFrame 已成功定义。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79fc05d5-a257-48ec-8067-bf47b2b15347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's check, if the Stream creation was successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40b80fa2-d933-43e2-bf89-5031517596d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streamingDF.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71861094-36f7-4dc0-9917-c7433bd54d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The transformation is still the same, but now it is applied on a stream instead of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5787b160-418f-4864-879a-72b1b881925c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = streamingDF\\\n",
    "   .selectExpr(\"CustomerId\", \"(UnitPrice * Quantity) as total_cost\", \"InvoiceDate\")\\\n",
    "   .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "   .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c003f1-90db-419a-9d79-68d29ab72a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the checkpoint folder path in Unity Catalog Volume\n",
    "checkpoint_folder = \"/Volumes/workspace/default/course_data/checkpoints/customer_purchases\"\n",
    "\n",
    "# Create the checkpoint folder if it does not exist\n",
    "os.makedirs(checkpoint_folder, exist_ok=True)\n",
    "\n",
    "# Start the stream\n",
    "# Your stream starting code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f58a3907-960b-4a5f-a012-de55d2c210ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we've learnd so far, That Spark evaluates lazly and nothing happens untill we call an action to initiate the stream processing. The action `writeStream` generates a table, which gets updated after each trigger event. Important to note is, **streaming tables are mutable** whereas `DataFrame`objectss **are immutable.**\n",
    "\n",
    "Here we stream the results to our console using `format(\"console\")`, to make it visible how the result table gets updated regularly. Using`format(\"memory\")` would push the stream to an in-memory table so other stream processes could read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31a7eea4-0150-48c9-ae58-ef759dbe8d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"/Volumes/workspace/default/course_data/checkpoints/customer_purchases\"\n",
    "\n",
    "purchaseByCustomerPerHour\\\n",
    "   .writeStream\\\n",
    "   .format(\"console\")\\\n",
    "   .queryName(\"customer_purchases\")\\\n",
    "   .outputMode(\"complete\")\\\n",
    "   .trigger(once=True)\\\n",
    "   .option(\"checkpointLocation\", CHECKPOINT_PATH)\\\n",
    "   .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15196f53-527c-4c4a-a7a1-6458284f6d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_spark_applications_and_basic_concepts",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
