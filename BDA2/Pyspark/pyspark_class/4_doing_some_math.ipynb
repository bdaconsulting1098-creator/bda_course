{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Doing Some Math\n",
    "[pyspark Doc](https://spark.apache.org/docs/2.4.5/api/python/index.html)\n",
    "\n",
    "Doing some calculations is a common task for us when we are writing ETL jobs, e.g. when amounts need to be aligned to the domestic currency (for us mostly EUR) or when we need to unify the scaling of numeric values. \n",
    "\n",
    "Applying math functions becomes even more impartant to me, when it comes to analytical queries and Key Performance Indicator calculation. So today, we want to have a closer look at the following pyspark sub-modules and classes: \n",
    "* `pyspar.sql.functions`\n",
    "* `pyspark.sql.GroupedData`\n",
    "* `pyspark.sql.DataFrameStatFunctions`\n",
    "\n",
    "## Some Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark_start import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "   .format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .load(\"./data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the derived schema and a small data sample shows, that Spark interpretes the Customer ID as as decimal numbers (double). Actually they are integers and so we therefore we want to get rid of the decimals. \n",
    "\n",
    "Rounding to zero decimals is not a good option, because the result is still a *double* having a decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------+--------------------+--------------+\n",
      "|InvoiceNO|StockCode|Quantity|UnitPrice|round(CustomerID, 0)|       Country|\n",
      "+---------+---------+--------+---------+--------------------+--------------+\n",
      "|   580538|    23084|      48|     1.79|             14075.0|United Kingdom|\n",
      "|   580538|    23077|      20|     1.25|             14075.0|United Kingdom|\n",
      "|   580538|    22906|      24|     1.65|             14075.0|United Kingdom|\n",
      "|   580538|    21914|      24|     1.25|             14075.0|United Kingdom|\n",
      "|   580538|    22467|       6|     2.55|             14075.0|United Kingdom|\n",
      "|   580538|    21544|      48|     0.85|             14075.0|United Kingdom|\n",
      "|   580538|    23126|       8|     4.95|             14075.0|United Kingdom|\n",
      "|   580538|    21833|      24|     1.69|             14075.0|United Kingdom|\n",
      "|   580539|    21479|       4|     4.25|             18180.0|United Kingdom|\n",
      "|   580539|   84030E|       4|     4.25|             18180.0|United Kingdom|\n",
      "+---------+---------+--------+---------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "df.select(\"InvoiceNO\", \n",
    "          \"StockCode\", \n",
    "          \"Quantity\", \n",
    "          \"UnitPrice\", \n",
    "          round(\"CustomerID\",0), \n",
    "          \"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is more an data type issue, rather than a calculation problem, type casting is more appropriate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------+----------+--------------+\n",
      "|InvoiceNO|StockCode|Quantity|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------+---------+----------+--------------+\n",
      "|   580538|    23084|      48|     1.79|     14075|United Kingdom|\n",
      "|   580538|    23077|      20|     1.25|     14075|United Kingdom|\n",
      "|   580538|    22906|      24|     1.65|     14075|United Kingdom|\n",
      "|   580538|    21914|      24|     1.25|     14075|United Kingdom|\n",
      "|   580538|    22467|       6|     2.55|     14075|United Kingdom|\n",
      "|   580538|    21544|      48|     0.85|     14075|United Kingdom|\n",
      "|   580538|    23126|       8|     4.95|     14075|United Kingdom|\n",
      "|   580538|    21833|      24|     1.69|     14075|United Kingdom|\n",
      "|   580539|    21479|       4|     4.25|     18180|United Kingdom|\n",
      "|   580539|   84030E|       4|     4.25|     18180|United Kingdom|\n",
      "+---------+---------+--------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "df.select(\"InvoiceNO\", \n",
    "          \"StockCode\", \n",
    "          \"Quantity\", \n",
    "          \"UnitPrice\", \n",
    "          col(\"CustomerID\").cast(\"integer\"), \n",
    "          \"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to calculate the amount for each invoice position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------+------------------+----------+--------------+\n",
      "|InvoiceNO|StockCode|Quantity|UnitPrice|            Amount|CustomerID|       Country|\n",
      "+---------+---------+--------+---------+------------------+----------+--------------+\n",
      "|   580538|    23084|      48|     1.79|             85.92|     14075|United Kingdom|\n",
      "|   580538|    23077|      20|     1.25|              25.0|     14075|United Kingdom|\n",
      "|   580538|    22906|      24|     1.65|39.599999999999994|     14075|United Kingdom|\n",
      "|   580538|    21914|      24|     1.25|              30.0|     14075|United Kingdom|\n",
      "|   580538|    22467|       6|     2.55|15.299999999999999|     14075|United Kingdom|\n",
      "|   580538|    21544|      48|     0.85|              40.8|     14075|United Kingdom|\n",
      "|   580538|    23126|       8|     4.95|              39.6|     14075|United Kingdom|\n",
      "|   580538|    21833|      24|     1.69|             40.56|     14075|United Kingdom|\n",
      "|   580539|    21479|       4|     4.25|              17.0|     18180|United Kingdom|\n",
      "|   580539|   84030E|       4|     4.25|              17.0|     18180|United Kingdom|\n",
      "+---------+---------+--------+---------+------------------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "df.select(\"InvoiceNO\", \n",
    "          \"StockCode\", \n",
    "          \"Quantity\", \n",
    "          \"UnitPrice\",\n",
    "          (col(\"Quantity\") * col(\"UnitPrice\")).alias(\"Amount\"),\n",
    "          col(\"CustomerID\").cast(\"integer\"), \n",
    "          \"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we forgot to round the amount to two decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------+------+----------+--------------+\n",
      "|InvoiceNO|StockCode|Quantity|UnitPrice|Amount|CustomerID|       Country|\n",
      "+---------+---------+--------+---------+------+----------+--------------+\n",
      "|   580538|    23084|      48|     1.79| 85.92|     14075|United Kingdom|\n",
      "|   580538|    23077|      20|     1.25|  25.0|     14075|United Kingdom|\n",
      "|   580538|    22906|      24|     1.65|  39.6|     14075|United Kingdom|\n",
      "|   580538|    21914|      24|     1.25|  30.0|     14075|United Kingdom|\n",
      "|   580538|    22467|       6|     2.55|  15.3|     14075|United Kingdom|\n",
      "|   580538|    21544|      48|     0.85|  40.8|     14075|United Kingdom|\n",
      "|   580538|    23126|       8|     4.95|  39.6|     14075|United Kingdom|\n",
      "|   580538|    21833|      24|     1.69| 40.56|     14075|United Kingdom|\n",
      "|   580539|    21479|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "|   580539|   84030E|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "+---------+---------+--------+---------+------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round \n",
    "\n",
    "df.select(\"InvoiceNO\", \n",
    "          \"StockCode\", \n",
    "          \"Quantity\", \n",
    "          \"UnitPrice\",\n",
    "          round((col(\"Quantity\") * col(\"UnitPrice\")), 2).alias(\"Amount\"),\n",
    "          col(\"CustomerID\").cast(\"integer\"), \n",
    "          \"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the intermediate results of  our data preperation in a variable, to keep the further analytical queries more simple. By decomposing  our query into a preperation part and an analytical part, we get the option to check, that the intermediat results are correct and so they a the appropriate foundation of  our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "|InvoiceNO|        InvoiceDate|StockCode|Quantity|UnitPrice|Amount|CustomerID|       Country|\n",
      "+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "|   580538|2011-12-05 08:38:00|    23084|      48|     1.79| 85.92|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    23077|      20|     1.25|  25.0|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    22906|      24|     1.65|  39.6|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    21914|      24|     1.25|  30.0|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    22467|       6|     2.55|  15.3|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    21544|      48|     0.85|  40.8|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    23126|       8|     4.95|  39.6|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    21833|      24|     1.69| 40.56|     14075|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    21479|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|   84030E|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transformations -- DAG planning optimization\n",
    "preparedDf = df.select(\n",
    "    \"InvoiceNO\", \n",
    "    \"InvoiceDate\",\n",
    "    \"StockCode\", \n",
    "    \"Quantity\", \n",
    "    \"UnitPrice\",\n",
    "    round((col(\"Quantity\") * col(\"UnitPrice\")), 2).alias(\"Amount\"),\n",
    "    col(\"CustomerID\").cast(\"integer\"), \n",
    "    \"Country\")\n",
    "\n",
    "#action\n",
    "preparedDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations\n",
    "### Aggregating on DataFrames\n",
    "First we want to apply several aggregation functions on the entire dataset in the `DataFrame` to do some data profiling. To make the output more readable, we switch `show()` to vertical output to get many rows instead of many columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " count                 | 541909              \n",
      " countDistinct         | 5827                \n",
      " approx_count_distinct | 5417                \n",
      " sum                   | 9747747.929999981   \n",
      " sumDistinct           | 863586.9200000006   \n",
      " min                   | -168469.6           \n",
      " max                   | 168469.6            \n",
      " avg                   | 17.987794869618295  \n",
      " mean                  | 17.987794869618295  \n",
      " variance              | 143497.6400055403   \n",
      " var_samp              | 143497.6400055403   \n",
      " var_pop               | 143497.37520528786  \n",
      " stddev                | 378.8108235063253   \n",
      " kurtosis              | 151196.60137753293  \n",
      " skewness              | -0.9643865070855122 \n",
      " first                 | 3.26                \n",
      " last                  | 163.5               \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "preparedDf\\\n",
    "    .select(\n",
    "        count(\"Amount\").alias(\"count\"), \n",
    "        countDistinct(\"Amount\").alias(\"countDistinct\"),\n",
    "        approx_count_distinct(\"Amount\", rsd=0.1).alias(\"approx_count_distinct\"),\n",
    "        sum(\"Amount\").alias(\"sum\"), \n",
    "        sumDistinct(\"Amount\").alias(\"sumDistinct\"),\n",
    "        min(\"Amount\").alias(\"min\"), \n",
    "        max(\"Amount\").alias(\"max\"), \n",
    "        avg(\"Amount\").alias(\"avg\"), \n",
    "        mean(\"Amount\").alias(\"mean\"), \n",
    "        variance(\"Amount\").alias(\"variance\"), \n",
    "        var_samp(\"Amount\").alias(\"var_samp\"),\n",
    "        var_pop(\"Amount\").alias(\"var_pop\"),\n",
    "        stddev(\"Amount\").alias(\"stddev\"),\n",
    "        kurtosis(\"Amount\").alias(\"kurtosis\"),\n",
    "        skewness(\"Amount\").alias(\"skewness\"),\n",
    "        first(\"Amount\").alias(\"first\"),\n",
    "        last(\"Amount\").alias(\"last\")\n",
    "    )\\\n",
    "    .show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are further aggregating we don't have a use case in  our example here, functions like:\n",
    "* **corr(col1, col2)** - returns a new Column for the Pearson Correlation Coefficient for col1 and col2\n",
    "* **covar_pop(col1, col2)** - returns a new Column for the population covariance of col1 and col2\n",
    "* **covar_samp(col1, col2)** - Returns a new Column for the sample covariance of col1 and col2\n",
    "\n",
    "Most of the aggregation function names are self-explaining, so nothing to comment on. Just first() and last() are a bit special. In contrast to most of the other aggregation functions, `first()` and `last()` both refer to the value **position** in the dataset and not to the value **amount**, like `min()` and `max()` do. So `first()` and `last()` are the only aggregation functions, being affected by data sorting.\n",
    "\n",
    "Back to data profiling. The ratio between count and countDistinct is an important indicator to identify key candidate columns. For primary keys, the ratio must be 1, i.e. countDistinct must equal the total count of values so it's cardinality must be also 1 to ensure uniqueness. \n",
    "\n",
    "Even though not beeing unique, columns with low cardinality are still candidates for performant data acess patterns. The cardinality is a measure of the average number of rows we will get when filtering in such a column value.\n",
    "\n",
    "The reverse value of cardinality, the entropy, is an indicator how compressible a column is. `DataFrames` having many columns with low entropy benefit much from a columnar storage format. On the other extreme, primary key columns are not compressible at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " count         | 541909               \n",
      " countDistinct | 5827                 \n",
      " cardinality   | 92.99965677020765    \n",
      " entropy       | 0.010752727856522036 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .select(\n",
    "        count(\"Amount\").alias(\"count\"), \n",
    "        countDistinct(\"Amount\").alias(\"countDistinct\")\n",
    "    )\\\n",
    "    .withColumn(\"cardinality\", col(\"count\") / col(\"countDistinct\"))\\\n",
    "    .withColumn(\"entropy\", col(\"countdistinct\") / col(\"count\"))\\\n",
    "    .show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not suprisingly the amount column is not a key candidate but it is very interesting, that the cardinality is quite high. Maybe there are only a few standard unit prices and/or lot sizes we can put orders on. Let's compare it with the *InvoiceNO* column. The column name sounds like a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " count         | 541909               \n",
      " countDistinct | 25900                \n",
      " cardinality   | 20.923127413127414   \n",
      " entropy       | 0.047794002314041656 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .select(\n",
    "        count(\"InvoiceNo\").alias(\"count\"), \n",
    "        countDistinct(\"InvoiceNo\").alias(\"countDistinct\")\n",
    "    )\\\n",
    "    .withColumn(\"cardinality\", col(\"count\") / col(\"countDistinct\"))\\\n",
    "    .withColumn(\"entropy\", col(\"countdistinct\") / col(\"count\"))\\\n",
    "    .show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this column has a much lower cardinality but stii it is not a unique key. The reason is, that the retail dataset is denormalized and the granularity is not based on invoices but on stock items. Since each invoice can list multiple stock items, we need to combine InviceNo and StockCode to get a unique key. Let's check, if this solves  our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "|          Key|InvoiceNO|        InvoiceDate|StockCode|Quantity|UnitPrice|Amount|CustomerID|       Country|\n",
      "+-------------+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "| 580538-23084|   580538|2011-12-05 08:38:00|    23084|      48|     1.79| 85.92|     14075|United Kingdom|\n",
      "| 580538-23077|   580538|2011-12-05 08:38:00|    23077|      20|     1.25|  25.0|     14075|United Kingdom|\n",
      "| 580538-22906|   580538|2011-12-05 08:38:00|    22906|      24|     1.65|  39.6|     14075|United Kingdom|\n",
      "| 580538-21914|   580538|2011-12-05 08:38:00|    21914|      24|     1.25|  30.0|     14075|United Kingdom|\n",
      "| 580538-22467|   580538|2011-12-05 08:38:00|    22467|       6|     2.55|  15.3|     14075|United Kingdom|\n",
      "| 580538-21544|   580538|2011-12-05 08:38:00|    21544|      48|     0.85|  40.8|     14075|United Kingdom|\n",
      "| 580538-23126|   580538|2011-12-05 08:38:00|    23126|       8|     4.95|  39.6|     14075|United Kingdom|\n",
      "| 580538-21833|   580538|2011-12-05 08:38:00|    21833|      24|     1.69| 40.56|     14075|United Kingdom|\n",
      "| 580539-21479|   580539|2011-12-05 08:39:00|    21479|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "|580539-84030E|   580539|2011-12-05 08:39:00|   84030E|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "+-------------+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keyedDf = preparedDf.select(\n",
    "    concat_ws('-', \"InvoiceNO\",\"StockCode\").alias(\"Key\"),\n",
    "    \"InvoiceNO\", \n",
    "    \"InvoiceDate\",\n",
    "    \"StockCode\", \n",
    "    \"Quantity\", \n",
    "    \"UnitPrice\",\n",
    "    \"Amount\",\n",
    "    \"CustomerID\", \n",
    "    \"Country\")\n",
    "\n",
    "keyedDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------\n",
      " count         | 541909             \n",
      " countDistinct | 531225             \n",
      " cardinality   | 1.0201120052708363 \n",
      " entropy       | 0.9802845127133891 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "keyedDf\\\n",
    "    .select(\n",
    "        count(\"Key\").alias(\"count\"), \n",
    "        countDistinct(\"Key\").alias(\"countDistinct\")\n",
    "    )\\\n",
    "    .withColumn(\"cardinality\", col(\"count\") / col(\"countDistinct\"))\\\n",
    "    .withColumn(\"entropy\", col(\"countdistinct\") / col(\"count\"))\\\n",
    "    .show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mh, we are getting close but there are still some duplicates. Maybe there are some Null values in these columns. we would need ot investigate it furthr down, but we leave this for now because another phenomenon confuses me, there are two versions of counting in Spark:\n",
    "* `DataFrame.count()`\n",
    "* `pyspark.sql.functions.count()`\n",
    "\n",
    "The `DataFrame.count()` method is always applied on the entire `DataFrame` and counts the total number of physical rows in it. Additionally this method is an action and not a transformation, because the count number is directy determined and returned. On the other hand `pyspark.sql.functions.count()` is an aggregation function counting non-Null values which is applied on grouped data defined by a grouping key `DataFrame.groupBy()`or a window function. Aggregation functions define lazly evaluated transformations. \n",
    "## Aggregating on Grouped Data\n",
    "Aggregating on the entire DataFrame will show us just in row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+--------+------------------+------------------+\n",
      "|              sum|      min|     max|               avg|              mean|\n",
      "+-----------------+---------+--------+------------------+------------------+\n",
      "|9747747.929999461|-168469.6|168469.6|17.987794869617336|17.987794869617336|\n",
      "+-----------------+---------+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .select(\n",
    "        sum(\"Amount\").alias(\"sum\"), \n",
    "        min(\"Amount\").alias(\"min\"), \n",
    "        max(\"Amount\").alias(\"max\"), \n",
    "        avg(\"Amount\").alias(\"avg\"), \n",
    "        mean(\"Amount\").alias(\"mean\")\n",
    "    )\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[sum(Amount#2679), min(Amount#2679), max(Amount#2679), avg(Amount#2679)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=1467]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(Amount#2679), partial_min(Amount#2679), partial_max(Amount#2679), partial_avg(Amount#2679)])\n",
      "         +- Project [round((cast(Quantity#2460 as double) * UnitPrice#2462), 2) AS Amount#2679]\n",
      "            +- FileScan csv [Quantity#2460,UnitPrice#2462] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(305 paths)[file:/C:/Business_Data_Analysis/pyspark_class/data/retail-data/by-da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Quantity:int,UnitPrice:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .select(\n",
    "        sum(\"Amount\").alias(\"sum\"), \n",
    "        min(\"Amount\").alias(\"min\"), \n",
    "        max(\"Amount\").alias(\"max\"), \n",
    "        avg(\"Amount\").alias(\"avg\"), \n",
    "        mean(\"Amount\").alias(\"mean\")\n",
    "    ).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such highly aggregated data does not provide us much business insight, so we want to see the results for each country. So the first thing we have to do is to define a grouping key to arrange the data to get one group for each country. Than we can aggregate on each group seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|    Country|count|\n",
      "+-----------+-----+\n",
      "|     Sweden|  462|\n",
      "|    Germany| 9495|\n",
      "|     France| 8557|\n",
      "|     Greece|  146|\n",
      "|    Belgium| 2069|\n",
      "|    Finland|  695|\n",
      "|      Malta|  127|\n",
      "|Unspecified|  446|\n",
      "|      Italy|  803|\n",
      "|       EIRE| 8196|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf.groupBy(\"Country\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already know, rearranging data means, Spark is shuffling partitions around. The explain plan confirms this (*Exchange hashpartitioning*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[Country#2464], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(Country#2464, 200), ENSURE_REQUIREMENTS, [plan_id=1293]\n",
      "      +- HashAggregate(keys=[Country#2464], functions=[partial_count(1)])\n",
      "         +- FileScan csv [Country#2464] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(305 paths)[file:/C:/Business_Data_Analysis/pyspark_class/data/retail-data/by-da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Country:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf.groupBy(\"Country\").count().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "|InvoiceNO|        InvoiceDate|StockCode|Quantity|UnitPrice|Amount|CustomerID|       Country|\n",
      "+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "|   580538|2011-12-05 08:38:00|    23084|      48|     1.79| 85.92|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    23077|      20|     1.25|  25.0|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    22906|      24|     1.65|  39.6|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    21914|      24|     1.25|  30.0|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    22467|       6|     2.55|  15.3|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    21544|      48|     0.85|  40.8|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    23126|       8|     4.95|  39.6|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    21833|      24|     1.69| 40.56|     14075|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    21479|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|   84030E|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    23355|       4|     4.95|  19.8|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    22111|       3|     4.95| 14.85|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    21115|       8|     1.95|  15.6|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    21411|       8|     1.95|  15.6|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    23235|      12|     1.25|  15.0|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    23239|       6|     1.65|   9.9|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    22197|      36|     0.85|  30.6|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    22693|      24|     1.25|  30.0|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    22372|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    22375|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    22074|      24|     0.39|  9.36|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    22075|      24|     0.39|  9.36|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    22076|      24|     0.39|  9.36|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    22389|      12|     0.39|  4.68|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    22391|      12|     0.39|  4.68|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    22393|      12|     0.39|  4.68|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    22395|      12|     0.39|  4.68|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    22481|      12|     0.39|  4.68|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    23320|      12|     1.25|  15.0|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    23543|       6|     4.15|  24.9|     18180|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    23497|      12|     1.45|  17.4|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    23498|      12|     1.45|  17.4|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    23552|       6|     2.08| 12.48|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    22109|       4|     3.75|  15.0|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    23556|       3|     12.5|  37.5|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    23555|       3|     12.5|  37.5|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    23554|       3|     12.5|  37.5|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    22466|      12|     1.95|  23.4|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    23480|       4|     3.75|  15.0|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    23357|       4|     4.95|  19.8|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    23358|       6|     3.75|  22.5|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    22423|       1|    12.75| 12.75|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    79321|       4|     5.75|  23.0|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    48194|       4|     8.25|  33.0|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    21523|       2|     8.25|  16.5|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    23283|       2|     8.25|  16.5|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    23472|       2|    14.95|  29.9|     13417|United Kingdom|\n",
      "|   580540|2011-12-05 08:49:00|    22457|       6|     2.95|  17.7|     13417|United Kingdom|\n",
      "|   580541|2011-12-05 09:03:00|    22436|      20|     0.65|  13.0|     15358|United Kingdom|\n",
      "|   580541|2011-12-05 09:03:00|    22439|      24|     0.65|  15.6|     15358|United Kingdom|\n",
      "+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+--------+-------+------------------+------------------+\n",
      "|    Country|               sum|     min|    max|               avg|              mean|\n",
      "+-----------+------------------+--------+-------+------------------+------------------+\n",
      "|     Sweden|          36595.91| -1188.0| 1188.0| 79.21192640692641| 79.21192640692641|\n",
      "|    Germany|         221698.21|  -599.5|  876.0|23.348942601369142|23.348942601369142|\n",
      "|     France|          197403.9|-8322.12|4161.06| 23.06928830197499| 23.06928830197499|\n",
      "|     Greece|           4710.52|   -50.0|  175.2| 32.26383561643836| 32.26383561643836|\n",
      "|    Belgium|          40910.96|  -19.95|  165.0|19.773301111648138|19.773301111648138|\n",
      "|    Finland|          22326.74|   -80.0|  551.2|32.124805755395684|32.124805755395684|\n",
      "|      Malta|           2505.47|  -130.0|  455.0| 19.72811023622047| 19.72811023622047|\n",
      "|Unspecified|           4749.79|    0.19|  69.36|  10.6497533632287|  10.6497533632287|\n",
      "|      Italy|          16890.51|  -89.55|  300.0|21.034259028642587|21.034259028642587|\n",
      "|       EIRE|263276.82000000024| -1917.0| 2365.2| 32.12259882869695| 32.12259882869695|\n",
      "+-----------+------------------+--------+-------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .groupBy(\"Country\")\\\n",
    "    .agg(\n",
    "        sum(\"Amount\").alias(\"sum\"), \n",
    "        min(\"Amount\").alias(\"min\"), \n",
    "        max(\"Amount\").alias(\"max\"), \n",
    "        avg(\"Amount\").alias(\"avg\"), \n",
    "        mean(\"Amount\").alias(\"mean\")\n",
    "    )\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to remind myself: we could do this all using  our good-old SQL. (ref. <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/index.html\">SQL Language Reference</a> provided by Databricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+--------+-------+------------------+------------------+\n",
      "|    Country|               sum|     min|    max|               avg|              mean|\n",
      "+-----------+------------------+--------+-------+------------------+------------------+\n",
      "|     Sweden|          36595.91| -1188.0| 1188.0| 79.21192640692641| 79.21192640692641|\n",
      "|    Germany|         221698.21|  -599.5|  876.0|23.348942601369142|23.348942601369142|\n",
      "|     France|          197403.9|-8322.12|4161.06| 23.06928830197499| 23.06928830197499|\n",
      "|     Greece|           4710.52|   -50.0|  175.2| 32.26383561643836| 32.26383561643836|\n",
      "|    Belgium|          40910.96|  -19.95|  165.0|19.773301111648138|19.773301111648138|\n",
      "|    Finland|          22326.74|   -80.0|  551.2|32.124805755395684|32.124805755395684|\n",
      "|      Malta|           2505.47|  -130.0|  455.0| 19.72811023622047| 19.72811023622047|\n",
      "|Unspecified|           4749.79|    0.19|  69.36|  10.6497533632287|  10.6497533632287|\n",
      "|      Italy|          16890.51|  -89.55|  300.0|21.034259028642587|21.034259028642587|\n",
      "|       EIRE|263276.82000000024| -1917.0| 2365.2| 32.12259882869695| 32.12259882869695|\n",
      "+-----------+------------------+--------+-------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf.createOrReplaceTempView(\"retailTable\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Country, sum(Amount) as sum, min(Amount) as min, max(Amount) as max, \n",
    "        avg(Amount) as avg, mean(Amount) as mean \n",
    "    FROM retailTable\n",
    "    GROUP BY Country\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating on Floating Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the average amount per country over the entire data set history of aprox. one year is still very high-level. we would like to make a time-series analysis on the invoice mounts. So the first thing we have to do is to include the *InvoiceDate* column into the grouping key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------------+\n",
      "|  Country|        InvoiceDate|               avg|\n",
      "+---------+-------------------+------------------+\n",
      "|Australia|2010-12-01 10:03:00|25.589285714285715|\n",
      "|Australia|2011-10-05 12:44:00|193.13296296296298|\n",
      "|Australia|2010-12-17 14:10:00|             41.57|\n",
      "|Australia|2011-11-24 12:30:00|35.800000000000004|\n",
      "|Australia|2011-11-02 12:03:00|            624.24|\n",
      "|Australia|2011-10-06 09:31:00|21.813333333333325|\n",
      "|Australia|2011-10-06 09:32:00|17.959999999999997|\n",
      "|Australia|2011-09-28 15:41:00|             20.86|\n",
      "|Australia|2011-11-15 10:32:00|189.45384615384617|\n",
      "|Australia|2011-11-03 11:26:00| 75.04799999999999|\n",
      "+---------+-------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .groupBy(\"Country\", \"InvoiceDate\")\\\n",
    "    .agg( \n",
    "        avg(\"Amount\").alias(\"avg\")\n",
    "    )\\\n",
    "    .orderBy(\"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *InvoiceDate* is actually a timestamp, now we have to much details because approximaly each row is representig one invoice. To reduce the noice in  our time series due to volatile invoice amounts, we would like to calculate rolling 7-day average amounts. Grouping the data does not help us here because this would assign each row to ecactly one group. Now each row should be member of seven overlapping time windows each spanning over seven days.\n",
    "\n",
    "So first, we have to define how to generate 7-day time windows. Than we can aggregate, averaging in particular, the amounts in each window. \n",
    "\n",
    "Ok, how can we define sliding windows? Maybe the sub-module `pyspark.sql.window` can help here?\n",
    "To get sliding time windows for each country, we have to partition the data by that column. Next we have to order the data along the date because the window boundaries are based on it. Finally we define the bounderies based on the date **values** relative to the date of the current row. Since there can be more than one row per day we cannot use rowsBetween() which is **position** based. Instead `rangeBetween()` seems more suitable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "sevenDayWindows = Window\\\n",
    "    .partitionBy(\"Country\")\\\n",
    "    .orderBy(\"InvoiceDate\")\\\n",
    "    .rangeBetween(-7, Window.currentRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply  our `avg()` aggregation on each timewindow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '(PARTITION BY Country ORDER BY InvoiceDate ASC NULLS FIRST RANGE BETWEEN -7L FOLLOWING AND CURRENT ROW)' due to data type mismatch: The data type 'timestamp' used in the order specification does not match the data type 'bigint' which is used in the range frame.;\n'Project [Country#2464, InvoiceDate#2461, avg(Amount#2679) windowspecdefinition(Country#2464, InvoiceDate#2461 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -7, currentrow$())) AS 7-day-avg#4879]\n+- Project [InvoiceNO#2457, InvoiceDate#2461, StockCode#2458, Quantity#2460, UnitPrice#2462, round((cast(Quantity#2460 as double) * UnitPrice#2462), 2) AS Amount#2679, cast(CustomerID#2463 as int) AS CustomerID#2680, Country#2464]\n   +- Relation [InvoiceNo#2457,StockCode#2458,Description#2459,Quantity#2460,InvoiceDate#2461,UnitPrice#2462,CustomerID#2463,Country#2464] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2492/4122871225.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m preparedDf.select(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"Country\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m\"InvoiceDate\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mavg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Amount\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msevenDayWindows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"7-day-avg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m ).show(10)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2021\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2022\u001b[0m         \"\"\"\n\u001b[1;32m-> 2023\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2024\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '(PARTITION BY Country ORDER BY InvoiceDate ASC NULLS FIRST RANGE BETWEEN -7L FOLLOWING AND CURRENT ROW)' due to data type mismatch: The data type 'timestamp' used in the order specification does not match the data type 'bigint' which is used in the range frame.;\n'Project [Country#2464, InvoiceDate#2461, avg(Amount#2679) windowspecdefinition(Country#2464, InvoiceDate#2461 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -7, currentrow$())) AS 7-day-avg#4879]\n+- Project [InvoiceNO#2457, InvoiceDate#2461, StockCode#2458, Quantity#2460, UnitPrice#2462, round((cast(Quantity#2460 as double) * UnitPrice#2462), 2) AS Amount#2679, cast(CustomerID#2463 as int) AS CustomerID#2680, Country#2464]\n   +- Relation [InvoiceNo#2457,StockCode#2458,Description#2459,Quantity#2460,InvoiceDate#2461,UnitPrice#2462,CustomerID#2463,Country#2464] csv\n"
     ]
    }
   ],
   "source": [
    "preparedDf.select(\n",
    "    \"Country\",\n",
    "    \"InvoiceDate\",\n",
    "    avg(\"Amount\").over(sevenDayWindows).alias(\"7-day-avg\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ups, what' wrong now? The trace strack states:\n",
    "> data type mismatch: The data type 'date' used in the order specification does not match the data type 'bigint' which is used in the range frame\n",
    "\n",
    "Ok, `Window.rangeBetween()` is obviously only suitable for integer ranges but not for date ranges. Fortunately there is another `window()` function available from sub-module `pyspark.sql.function`. The pyspark  <a href=\"https://spark.apache.org/docs/2.4.5/api/python/pyspark.sql.html#module-pyspark.sql.functions\">documentation</a> says:\n",
    "> pyspark.sql.functions.window(*timeColumn, windowDuration, slideDuration=None, startTime=None*)\n",
    "\n",
    "> Bucketize rows into one or more time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive\n",
    "\n",
    "we want to have 7-day windows sliding every day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+------------------+\n",
      "|Country  |7-day windows                             |avg               |\n",
      "+---------+------------------------------------------+------------------+\n",
      "|Australia|{2010-11-24 19:00:00, 2010-12-01 19:00:00}|25.589285714285715|\n",
      "|Australia|{2010-11-25 19:00:00, 2010-12-02 19:00:00}|25.589285714285715|\n",
      "|Australia|{2010-11-26 19:00:00, 2010-12-03 19:00:00}|25.589285714285715|\n",
      "|Australia|{2010-11-27 19:00:00, 2010-12-04 19:00:00}|25.589285714285715|\n",
      "|Australia|{2010-11-28 19:00:00, 2010-12-05 19:00:00}|25.589285714285715|\n",
      "|Australia|{2010-11-29 19:00:00, 2010-12-06 19:00:00}|25.589285714285715|\n",
      "|Australia|{2010-11-30 19:00:00, 2010-12-07 19:00:00}|25.589285714285715|\n",
      "|Australia|{2010-12-01 19:00:00, 2010-12-08 19:00:00}|32.362500000000004|\n",
      "|Australia|{2010-12-02 19:00:00, 2010-12-09 19:00:00}|32.362500000000004|\n",
      "|Australia|{2010-12-03 19:00:00, 2010-12-10 19:00:00}|32.362500000000004|\n",
      "|Australia|{2010-12-04 19:00:00, 2010-12-11 19:00:00}|32.362500000000004|\n",
      "|Australia|{2010-12-05 19:00:00, 2010-12-12 19:00:00}|32.362500000000004|\n",
      "|Australia|{2010-12-06 19:00:00, 2010-12-13 19:00:00}|32.362500000000004|\n",
      "|Australia|{2010-12-07 19:00:00, 2010-12-14 19:00:00}|21.013636363636365|\n",
      "|Australia|{2010-12-08 19:00:00, 2010-12-15 19:00:00}|-9.25             |\n",
      "|Australia|{2010-12-09 19:00:00, 2010-12-16 19:00:00}|-9.25             |\n",
      "|Australia|{2010-12-10 19:00:00, 2010-12-17 19:00:00}|29.842307692307692|\n",
      "|Australia|{2010-12-11 19:00:00, 2010-12-18 19:00:00}|29.842307692307692|\n",
      "|Australia|{2010-12-12 19:00:00, 2010-12-19 19:00:00}|29.842307692307692|\n",
      "|Australia|{2010-12-13 19:00:00, 2010-12-20 19:00:00}|29.842307692307692|\n",
      "+---------+------------------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .groupBy(\n",
    "        \"Country\", \n",
    "        window(timeColumn=\"InvoiceDate\", windowDuration=\"7 days\", slideDuration=\"1 day\").alias(\"7-day windows\")\n",
    "    )\\\n",
    "    .agg(avg(\"Amount\").alias(\"avg\"))\\\n",
    "    .orderBy(\"Country\", \"7-day windows\")\\\n",
    "    .show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating on Group Sets\n",
    "Now we want to drill down and analyse the biggest amounts on levels starting on country level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "|        Country|Total Amount|\n",
      "+---------------+------------+\n",
      "|      Australia|      2090.3|\n",
      "|        Austria|        51.0|\n",
      "|        Bahrain|        25.5|\n",
      "|        Belgium|      599.25|\n",
      "|         Brazil|       175.2|\n",
      "|         Canada|       12.75|\n",
      "|Channel Islands|       624.0|\n",
      "|         Cyprus|       382.5|\n",
      "|           EIRE|     8524.35|\n",
      "|        Finland|       38.25|\n",
      "+---------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .where(\"StockCode = 22423 or StockCode = 22086\")\\\n",
    "    .groupBy(\"Country\")\\\n",
    "    .agg(sum(\"Amount\").alias(\"sum_amount\"))\\\n",
    "    .select(\"Country\", round(\"sum_amount\", 2).alias(\"Total Amount\"))\\\n",
    "    .orderBy(\"Country\", desc(\"Total Amount\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go further deeper on StockCode level, we just need to  add this column to the grouping key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+------------+\n",
      "|        Country|StockCode|Total Amount|\n",
      "+---------------+---------+------------+\n",
      "|      Australia|    22423|      1978.2|\n",
      "|      Australia|    22086|       112.1|\n",
      "|        Austria|    22423|        51.0|\n",
      "|        Bahrain|    22423|        25.5|\n",
      "|        Belgium|    22423|      599.25|\n",
      "|         Brazil|    22423|       175.2|\n",
      "|         Canada|    22423|       12.75|\n",
      "|Channel Islands|    22423|       517.8|\n",
      "|Channel Islands|    22086|       106.2|\n",
      "|         Cyprus|    22423|       382.5|\n",
      "+---------------+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .where(\"StockCode = 22423 or StockCode = 22086\")\\\n",
    "    .groupBy(\"Country\", \"StockCode\")\\\n",
    "    .agg(sum(\"Amount\").alias(\"sum_amount\"))\\\n",
    "    .select(\"Country\", \"StockCode\", round(\"sum_amount\", 2).alias(\"Total Amount\"))\\\n",
    "    .orderBy(\"Country\", desc(\"Total Amount\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding more and more columns, I'll get more granular figures, but we will see only the figures on the most detailed level. What should we do to get the sums on all higher levels as well?\n",
    "### Rollup\n",
    "All we need to do is just rolling up the detaild sums up to the top level. Actually we just need to change one word in the code, replacing `groupBy()` by `rollup()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+\n",
      "|  Country|StockCode|Total Amount|\n",
      "+---------+---------+------------+\n",
      "|     null|     null|   228554.13|\n",
      "|Australia|     null|      2090.3|\n",
      "|Australia|    22423|      1978.2|\n",
      "|Australia|    22086|       112.1|\n",
      "|  Austria|    22423|        51.0|\n",
      "|  Austria|     null|        51.0|\n",
      "|  Bahrain|     null|        25.5|\n",
      "|  Bahrain|    22423|        25.5|\n",
      "|  Belgium|     null|      599.25|\n",
      "|  Belgium|    22423|      599.25|\n",
      "+---------+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .where(\"StockCode = 22423 or StockCode = 22086\")\\\n",
    "    .rollup(\"Country\", \"StockCode\")\\\n",
    "    .agg(sum(\"Amount\").alias(\"sum_amount\"))\\\n",
    "    .select(\"Country\", \"StockCode\", round(\"sum_amount\", 2).alias(\"Total Amount\"))\\\n",
    "    .orderBy(\"Country\", desc(\"Total Amount\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cube\n",
    "Rolling up aggregations is strongly hierarchical. Now we have the total by country and the sub-total per StockCode per country. But we need the total per StockCode across all countries as well. More generla, I( want to slice and dice the data along each dimension independently from other dimensions, like we can do in relational star schemas.\n",
    "\n",
    "Again, there is only one little piece to tbe changed in the code: using `cube()` instead of `rollup()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+\n",
      "|  Country|StockCode|Total Amount|\n",
      "+---------+---------+------------+\n",
      "|     null|     null|   228554.13|\n",
      "|     null|    22423|   164762.19|\n",
      "|     null|    22086|    63791.94|\n",
      "|Australia|     null|      2090.3|\n",
      "|Australia|    22423|      1978.2|\n",
      "|Australia|    22086|       112.1|\n",
      "|  Austria|    22423|        51.0|\n",
      "|  Austria|     null|        51.0|\n",
      "|  Bahrain|     null|        25.5|\n",
      "|  Bahrain|    22423|        25.5|\n",
      "+---------+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .where(\"StockCode = 22423 or StockCode = 22086\")\\\n",
    "    .cube(\"Country\", \"StockCode\")\\\n",
    "    .agg(sum(\"Amount\").alias(\"sum_amount\"))\\\n",
    "    .select(\"Country\", \"StockCode\", round(\"sum_amount\", 2).alias(\"Total Amount\"))\\\n",
    "    .orderBy(\"Country\", desc(\"Total Amount\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the second row shows the total for StockCode 22423 across all countries (Country is null ) whereas the fourth row shows the total across StockCodes (StockCode is null) for Australia.\n",
    "\n",
    "Another nice feature is `grouping_id()` which is a special aggregation function. Starting with 0 at the lowest level, the group_id provides the aggregation level in ascending order so we could easily filter on specific detail levels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+-----+\n",
      "|  Country|StockCode|Total Amount|Level|\n",
      "+---------+---------+------------+-----+\n",
      "|     null|     null|   228554.13|    3|\n",
      "|     null|    22423|   164762.19|    2|\n",
      "|     null|    22086|    63791.94|    2|\n",
      "|Australia|     null|      2090.3|    1|\n",
      "|Australia|    22423|      1978.2|    0|\n",
      "|Australia|    22086|       112.1|    0|\n",
      "|  Austria|    22423|        51.0|    0|\n",
      "|  Austria|     null|        51.0|    1|\n",
      "|  Bahrain|     null|        25.5|    1|\n",
      "|  Bahrain|    22423|        25.5|    0|\n",
      "+---------+---------+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .where(\"StockCode = 22423 or StockCode = 22086\")\\\n",
    "    .cube(\"Country\", \"StockCode\")\\\n",
    "    .agg(sum(\"Amount\").alias(\"sum_amount\"), grouping_id().alias(\"Level\"))\\\n",
    "    .select(\"Country\", \"StockCode\", round(\"sum_amount\", 2).alias(\"Total Amount\"), \"Level\")\\\n",
    "    .orderBy(\"Country\", desc(\"Total Amount\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In  our example the aggregation hierarchy looks like this:\n",
    "\n",
    "3: overall total\n",
    "\n",
    "2: StockCode total\n",
    "\n",
    "1: Country total\n",
    "\n",
    "0: StockCode & Country total\n",
    "\n",
    "Ok, why has StockCode a higher aggregation level than Country? The answer is, they are numbered according to their order in `cube(\"Country\", \"StockCode\")`, so Country comes first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we want to analyse the data for two countries, Germany and Australia, across all StockCode, which can be quite a lot, and we want to compare the German with the Australian figures. Therefore we want to have the totaly by country lined-up in columns. By calling the `pivot()` function, we can flip the country totals from rows to columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+\n",
      "|StockCode|Australia|           Germany|\n",
      "+---------+---------+------------------+\n",
      "|    10002|     null|              0.85|\n",
      "|    10125|     null|              84.8|\n",
      "|    10135|     null|             212.0|\n",
      "|    11001|     null|             54.08|\n",
      "|    15034|     null|              3.36|\n",
      "|    15036|    432.0| 853.3200000000002|\n",
      "|    15039|     null|              0.85|\n",
      "|   15044A|     null|              88.5|\n",
      "|   15044B|     null|              35.4|\n",
      "|   15044D|     null|53.099999999999994|\n",
      "|  15056BL|    17.85|           1100.25|\n",
      "|   15056N|     null|            695.65|\n",
      "|   15056P|     null| 564.7500000000001|\n",
      "|   15058A|     null|              15.9|\n",
      "|   15058B|     null|              15.9|\n",
      "|   15058C|     null|              39.5|\n",
      "|   15060B|     null|              90.0|\n",
      "|    16008|     null|              8.64|\n",
      "|    16011|     null|             20.16|\n",
      "+---------+---------+------------------+\n",
      "only showing top 19 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .where(\"Country = 'Australia' or Country = 'Germany'\")\\\n",
    "    .groupBy(\"StockCode\")\\\n",
    "    .pivot(\"Country\")\\\n",
    "    .sum(\"Amount\")\\\n",
    "    .orderBy(\"StockCode\")\\\n",
    "    .show(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Statistical Functions on DataFrames\n",
    "`DataFrame` objects have also some statistical methods. In contrast to the aggregation functions, they always apply on the entire DataFrame data set and cannot be applied on data groups whithin a DataFrame.\n",
    "* **approxQuantile(col, probabilities, relativeError)**\n",
    "* **corr()**\n",
    "* **count()**\n",
    "* **cov()**\n",
    "* **crosstab()**\n",
    "* **describe()** alias **summary()**\n",
    "\n",
    "The `describe()` method is quite interesting because it computes basic statistics (count, mean, stddev, min, and max) for numeric and string columns eventhough ean, stddev, min, and max are not available as `DataFrame` object methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            Amount|\n",
      "+-------+------------------+\n",
      "|  count|            541909|\n",
      "|   mean|17.987794869617336|\n",
      "| stddev| 378.8108235063262|\n",
      "|    min|         -168469.6|\n",
      "|    max|          168469.6|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf.describe(\"Amount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameStatFunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `DataFrame` object has an `na` property referencing a `DataFrameStatFunctions` object which provides statisical object methods. Some of them are just aliases of `DataFrame` object methods, e.g.:\n",
    "\n",
    "* df.**corr()** alias df.**na.corr()** - calculates the Pearson Correlation Coefficient of two columns of a DataFrame as a double value\n",
    "* df.**cov()** alias df.**na.cov()** - calculate the sample covariance for the given columns, specified by their names, as a double value\n",
    "* df.**crosstab()** alias df.**na.crosstab()** - computes a pair-wise frequency table of the given columns. Also known as a contingency table\n",
    "\n",
    "we've no clue about the reason for this duplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNO',\n",
       " 'InvoiceDate',\n",
       " 'StockCode',\n",
       " 'Quantity',\n",
       " 'UnitPrice',\n",
       " 'Amount',\n",
       " 'CustomerID',\n",
       " 'Country']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preparedDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0012349245448702957"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preparedDf.corr(\"Quantity\",\"UnitPrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+-------+-------+------+------+---------------+------+--------------+-------+----+------------------+-------+------+-------+------+---------+-------+------+-----+-----+-------+---------+-----+-----------+------+------+--------+---+------------+---------+-----+------+-----------+---+--------------------+--------------+-----------+\n",
      "|StockCode_Country|Australia|Austria|Bahrain|Belgium|Brazil|Canada|Channel Islands|Cyprus|Czech Republic|Denmark|EIRE|European Community|Finland|France|Germany|Greece|Hong Kong|Iceland|Israel|Italy|Japan|Lebanon|Lithuania|Malta|Netherlands|Norway|Poland|Portugal|RSA|Saudi Arabia|Singapore|Spain|Sweden|Switzerland|USA|United Arab Emirates|United Kingdom|Unspecified|\n",
      "+-----------------+---------+-------+-------+-------+------+------+---------------+------+--------------+-------+----+------------------+-------+------+-------+------+---------+-------+------+-----+-----+-------+---------+-----+-----------+------+------+--------+---+------------+---------+-----+------+-----------+---+--------------------+--------------+-----------+\n",
      "|            20778|        0|      0|      0|      0|     0|     0|              0|     0|             0|      0|   0|                 0|      0|     0|      0|     0|        0|      0|     0|    0|    0|      0|        0|    0|          0|     0|     0|       0|  0|           0|        0|    0|     0|          0|  0|                   0|            10|          0|\n",
      "|           90011E|        0|      0|      0|      0|     0|     0|              0|     0|             0|      0|   0|                 0|      0|     0|      0|     0|        0|      0|     0|    0|    0|      0|        0|    0|          0|     0|     0|       0|  0|           0|        0|    0|     0|          0|  0|                   0|             3|          0|\n",
      "|            22578|        0|      0|      0|      0|     0|     0|              0|     0|             1|      0|  15|                 0|      1|     6|      3|     0|        0|      0|     0|    0|    0|      0|        0|    0|          0|     0|     0|       2|  0|           0|        1|    4|     1|          0|  0|                   0|           502|          0|\n",
      "|            21476|        0|      0|      0|      0|     0|     0|              0|     0|             0|      0|   0|                 0|      0|     0|      0|     0|        0|      0|     0|    0|    0|      0|        0|    0|          0|     0|     0|       0|  0|           0|        0|    0|     0|          0|  0|                   0|            24|          0|\n",
      "|            21327|        0|      0|      0|      1|     0|     0|              0|     0|             0|      0|   0|                 0|      0|     0|      0|     0|        0|      0|     0|    0|    0|      0|        0|    0|          0|     0|     0|       0|  0|           0|        0|    0|     0|          0|  0|                   0|           255|          0|\n",
      "|            22064|        0|      0|      0|      1|     0|     1|              0|     0|             0|      0|   1|                 0|      1|     2|      7|     0|        0|      0|     0|    1|    0|      0|        0|    1|          0|     1|     0|       0|  0|           0|        0|    0|     0|          1|  0|                   0|           249|          0|\n",
      "|            21080|        3|      1|      0|      8|     0|     0|              5|     0|             0|      0|  15|                 0|      2|    52|     27|     0|        0|      0|     1|    1|    3|      0|        0|    0|          6|     3|     0|       6|  0|           0|        0|    9|     0|          5|  0|                   0|           868|          0|\n",
      "|           90179A|        0|      0|      0|      0|     0|     0|              0|     0|             0|      0|   0|                 0|      0|     0|      0|     0|        0|      0|     0|    0|    0|      0|        0|    0|          0|     0|     0|       0|  0|           0|        0|    0|     0|          0|  0|                   0|             5|          0|\n",
      "|            22219|        1|      0|      0|      0|     0|     0|              0|     0|             0|      0|   1|                 0|      0|     3|      0|     0|        0|      0|     0|    0|    0|      0|        0|    0|          1|     0|     0|       0|  0|           0|        0|    1|     0|          0|  0|                   0|           554|          0|\n",
      "|            84452|        0|      0|      0|      0|     0|     0|              0|     0|             0|      0|   1|                 0|      0|     0|      0|     0|        0|      0|     0|    0|    0|      0|        0|    0|          0|     0|     0|       0|  0|           0|        0|    0|     0|          0|  0|                   0|            17|          0|\n",
      "|            37447|        0|      1|      0|      1|     0|     0|              0|     0|             0|      0|   1|                 0|      1|     0|      9|     0|        0|      0|     0|    0|    0|      0|        0|    1|          0|     0|     3|       2|  0|           0|        0|    2|     0|          1|  0|                   0|            99|          0|\n",
      "|            21908|        0|      0|      0|      2|     0|     0|              1|     0|             0|      0|   5|                 0|      0|     4|      2|     0|        0|      0|     0|    0|    0|      0|        0|    0|          0|     4|     0|       1|  0|           0|        0|    2|     0|          0|  0|                   1|           455|          1|\n",
      "|           90067B|        0|      0|      0|      0|     0|     0|              0|     0|             0|      0|   0|                 0|      0|     0|      0|     0|        0|      0|     0|    0|    0|      0|        0|    0|          0|     0|     0|       0|  0|           0|        0|    0|     0|          0|  0|                   0|             2|          0|\n",
      "|            23398|        0|      0|      0|      1|     0|     0|              0|     0|             0|      0|   2|                 0|      0|     1|      0|     0|        0|      0|     0|    0|    0|      0|        0|    0|          1|     0|     0|       1|  0|           0|        0|    0|     0|          0|  0|                   0|            73|          0|\n",
      "|            22818|        2|      2|      0|      0|     0|     0|              0|     1|             0|      0|   1|                 0|      0|     6|      5|     0|        0|      0|     0|    0|    0|      0|        0|    1|          0|     0|     0|       0|  0|           0|        0|    0|     0|          0|  2|                   0|           229|          0|\n",
      "|            22285|        0|      0|      0|      0|     0|     0|              0|     0|             0|      0|   1|                 0|      0|     1|      0|     0|        0|      0|     0|    0|    0|      0|        0|    0|          0|     0|     0|       1|  0|           0|        0|    0|     0|          0|  0|                   0|           121|          0|\n",
      "|            23268|        1|      0|      0|      0|     0|     0|              1|     0|             0|      0|   2|                 1|      0|     3|      3|     0|        0|      0|     0|    0|    0|      0|        0|    1|          3|     0|     0|       0|  0|           0|        0|    0|     0|          1|  0|                   0|            80|          1|\n",
      "|          15056BL|        1|      2|      0|      3|     1|     0|              0|     0|             0|      0|  14|                 0|      1|    18|     16|     0|        0|      0|     0|    0|    0|      0|        1|    0|          0|     1|     3|       0|  0|           0|        0|    0|     0|          2|  0|                   0|           263|          0|\n",
      "|            72817|        0|      1|      0|      0|     0|     0|              0|     0|             0|      0|   6|                 0|      1|     0|      1|     0|        0|      0|     0|    0|    0|      0|        0|    0|          0|     0|     0|       0|  0|           0|        0|    1|     0|          1|  0|                   0|           157|          0|\n",
      "|            22545|        3|      0|      0|      1|     0|     0|              0|     0|             0|      0|   1|                 0|      0|     0|      0|     0|        0|      0|     0|    0|    0|      0|        0|    0|          1|     0|     0|       0|  0|           0|        1|    0|     0|          1|  0|                   0|            69|          0|\n",
      "+-----------------+---------+-------+-------+-------+------+------+---------------+------+--------------+-------+----+------------------+-------+------+-------+------+---------+-------+------+-----+-----+-------+---------+-----+-----------+------+------+--------+---+------------+---------+-----+------+-----------+---+--------------------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf.crosstab('StockCode','Country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
