{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Date and Time Challenges\n",
    "Working with date and time data is always a struggle, because there are many different notations and formats as well as the timezone issue. So todaywewant to check, how the `pysparl.sql.functions` module can help us here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark_start import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are just two kind of objects regarding date and time known to Spark\n",
    "1. `DateType`: which refers to a calender\n",
    "1. `TimeStampType`: which is a date extended by a time releated to a timezone\n",
    "\n",
    "By default, Spark derives the current date and timestamp from the local machine settings, unless it get's overruled by `SparkConf` settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n",
      "|id |Today     |Now                    |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2023-01-20|2023-01-20 18:23:32.583|\n",
      "+---+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = spark\\\n",
    "    .range(1)\\\n",
    "    .withColumn(\"Today\", current_date())\\\n",
    "    .withColumn(\"Now\", current_timestamp())\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposing Date and Timestamp Objects\n",
    "Ifweneed to check for a particular part of a date/timestap, maybe to get all data records created in Apr-2020,wecan decompose these objects by one of the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------\n",
      " now          | 2023-01-20 18:23:37.073 \n",
      " year         | 2023                    \n",
      " quarter      | 1                       \n",
      " month        | 1                       \n",
      " day of month | 20                      \n",
      " hours        | 18                      \n",
      " minutes      | 23                      \n",
      " seconds      | 23                      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"now\",\n",
    "   year(\"Now\").alias(\"year\"),\n",
    "   quarter(\"Now\").alias(\"quarter\"),\n",
    "   month(\"now\").alias(\"month\"),\n",
    "   dayofmonth(\"now\").alias(\"day of month\"),\n",
    "   hour(\"now\").alias(\"hours\"),\n",
    "   minute(\"now\").alias(\"minutes\"),\n",
    "   minute(\"now\").alias(\"seconds\")\n",
    ").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even derive some further attributes from a given date. Especially the `last_day()` function can help us implementing month-end reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " now          | 2023-01-20 18:23:37.86 \n",
      " week of year | 3                      \n",
      " day of week  | 6                      \n",
      " month end    | 2023-01-31             \n",
      " next Sunday  | 2023-01-22             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"now\",\n",
    "    weekofyear(\"now\").alias(\"week of year\"),\n",
    "    dayofweek(\"now\").alias(\"day of week\"),\n",
    "    last_day(\"now\").alias(\"month end\"),\n",
    "    next_day(\"today\", \"Sun\").alias(\"next Sunday\")\n",
    ").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Shifts and Date Period Calculation\n",
    "\n",
    "Spark provides native support for calulations of time periodes on daily or monthly basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------\n",
      " today                                    | 2023-01-20              \n",
      " day after tomorrow                       | 2023-01-22              \n",
      " yesterday                                | 2023-01-19              \n",
      " yesterday                                | 2023-01-19              \n",
      " same day next month                      | 2023-02-20              \n",
      " same day prev. month                     | 2022-12-20              \n",
      " now                                      | 2023-01-20 18:23:38.456 \n",
      " tomorrow at same time                    | 2023-01-21              \n",
      " yesterday at same time                   | 2023-01-19              \n",
      " next month                               | 2023-02-20              \n",
      " # of days between yesterday and tomorrow | 2                       \n",
      " # of months between two dates            | -2.58064516             \n",
      " # of months between two dates            | 2.51612903              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"today\",\n",
    "    date_add(\"today\", 2).alias(\"day after tomorrow\"),\n",
    "    date_add(\"today\", -1).alias(\"yesterday\"),\n",
    "    date_sub(\"today\", 1).alias(\"yesterday\"),\n",
    "    add_months(\"today\", 1).alias(\"same day next month\"),\n",
    "    add_months(\"today\", -1).alias(\"same day prev. month\"),\n",
    "    \"now\",\n",
    "    date_add(\"now\", 1).alias(\"tomorrow at same time\"),\n",
    "    date_sub(\"now\", 1).alias(\"yesterday at same time\"),\n",
    "    add_months(\"now\", 1).alias(\"next month\"),\n",
    "    datediff(date_add(\"today\", 1), date_sub(\"today\", 1)).alias(\"# of days between yesterday and tomorrow\"),\n",
    "    months_between(\"today\", date_add(\"today\", 77)).alias(\"# of months between two dates\"),\n",
    "    months_between(\"today\", date_sub(\"today\", 77)).alias(\"# of months between two dates\")\n",
    ").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite counter-intuitive to us that the month diff is negative when the second date is later than the first date and vice versa. The calculation is simply date1 - date2 and since earlier dates are smaller than later ones,  the negative result obvious is absolutely intuitive for mathematicians.\n",
    "\n",
    "We should keep also in mind, that when applying these funtions to timestamps, they get truncated to dates and timestamps by myself.\n",
    "\n",
    "The `date_sub()` function is redundant because `date_add()` accepts also negative day shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " today   | 2023-01-20              \n",
      " yyyy    | 2023-01-01 00:00:00     \n",
      " year    | 2023-01-01 00:00:00     \n",
      " yy      | 2023-01-01 00:00:00     \n",
      " quarter | 2023-01-01 00:00:00     \n",
      " month   | 2023-01-01 00:00:00     \n",
      " mon     | 2023-01-01 00:00:00     \n",
      " week    | 2023-01-16 00:00:00     \n",
      " mm      | 2023-01-01 00:00:00     \n",
      " now     | 2023-01-20 18:23:39.272 \n",
      " yyyy    | 2023-01-01 00:00:00     \n",
      " year    | 2023-01-01 00:00:00     \n",
      " yy      | 2023-01-01 00:00:00     \n",
      " quarter | 2023-01-01 00:00:00     \n",
      " month   | 2023-01-01 00:00:00     \n",
      " mon     | 2023-01-01 00:00:00     \n",
      " week    | 2023-01-16 00:00:00     \n",
      " mm      | 2023-01-01 00:00:00     \n",
      " day     | 2023-01-20 00:00:00     \n",
      " dd      | 2023-01-20 00:00:00     \n",
      " hour    | 2023-01-20 18:00:00     \n",
      " minute  | 2023-01-20 18:23:00     \n",
      " second  | 2023-01-20 18:23:39     \n",
      " quarter | 2023-01-01 00:00:00     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"today\",\n",
    "    date_trunc(\"yyyy\", \"today\").alias(\"yyyy\"),\n",
    "    date_trunc(\"year\", \"today\").alias(\"year\"),\n",
    "    date_trunc(\"year\", \"today\").alias(\"yy\"),\n",
    "    date_trunc(\"quarter\", \"today\").alias(\"quarter\"),\n",
    "    date_trunc(\"month\", \"today\").alias(\"month\"),\n",
    "    date_trunc(\"mon\", \"today\").alias(\"mon\"),\n",
    "    date_trunc(\"week\", \"today\").alias(\"week\"),\n",
    "    date_trunc(\"mm\", \"today\").alias(\"mm\"),\n",
    "    \"now\",\n",
    "    date_trunc(\"yyyy\", \"now\").alias(\"yyyy\"),\n",
    "    date_trunc(\"year\", \"now\").alias(\"year\"),\n",
    "    date_trunc(\"year\", \"now\").alias(\"yy\"),\n",
    "    date_trunc(\"quarter\", \"now\").alias(\"quarter\"),\n",
    "    date_trunc(\"month\", \"now\").alias(\"month\"),\n",
    "    date_trunc(\"mon\", \"now\").alias(\"mon\"),\n",
    "    date_trunc(\"week\", \"now\").alias(\"week\"),\n",
    "    date_trunc(\"mm\", \"now\").alias(\"mm\"),\n",
    "    date_trunc(\"day\", \"now\").alias(\"day\"),\n",
    "    date_trunc(\"dd\", \"now\").alias(\"dd\"),\n",
    "    date_trunc(\"hour\", \"now\").alias(\"hour\"),\n",
    "    date_trunc(\"minute\", \"now\").alias(\"minute\"),\n",
    "    date_trunc(\"second\", \"now\").alias(\"second\"),\n",
    "    date_trunc(\"quarter\", \"today\").alias(\"quarter\"),\n",
    ").show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Conversion: String vs. Date/Timstamps\n",
    "Especially during data import or export of dates/timestamps,weoften have to read from strings or write to strings. \n",
    "\n",
    "### Reading Strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "|id |ts string          |\n",
      "+---+-------------------+\n",
      "|0  |2020-04-16 14:16:23|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringDF = spark.range(1).withColumn(\"ts string\", lit(\"2020-04-16 14:16:23\"))\n",
    "stringDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- ts string: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenweread from file with `option(\"inferSchema\", \"true\")` Spark can identifiy most of the common date and datetime notations and derive the correspondig column type. If the notation is unknon to Spark, it will set the column to DataTypeString. In that casewecan convert these strings to dates or timestamps during a subsequent transformation. This requires a string format according to the Java <a href=\"https://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html\">SimpleDateFormats</a>\n",
    "\n",
    "**Note:** Internally Spark works with Java dates and timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------------+\n",
      "|ts string          |converted to ts    |converted to date|\n",
      "+-------------------+-------------------+-----------------+\n",
      "|2020-04-16 14:16:23|2020-04-16 14:16:23|2020-04-16       |\n",
      "+-------------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringDF.select(\n",
    "        \"ts string\",\n",
    "        to_timestamp(\"ts string\", \"yyyy-MM-dd HH:mm:ss\").alias(\"converted to ts\"),\n",
    "        to_date(\"ts string\", \"yyyy-MM-dd HH:mm:ss\").alias(\"converted to date\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_date in module pyspark.sql.functions:\n",
      "\n",
      "to_date(col: 'ColumnOrName', format: Optional[str] = None) -> pyspark.sql.column.Column\n",
      "    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n",
      "    using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "    is omitted. Equivalent to ``col.cast(\"date\")``.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 2.2.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(to_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing as Strings\n",
    "To convert either a date or a timestamp into a string, againwehave to specify the format according to the Java [SimpleDateFormats](https://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- Today: date (nullable = false)\n",
      " |-- Now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|date string|ts string          |\n",
      "+-----------+-------------------+\n",
      "|20.01.2023 |20.01:2023 19:21.25|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    date_format(\"today\", \"dd.MM.yyyy\").alias(\"date string\"),\n",
    "    date_format(\"now\", \"dd.MM:yyyy HH:mm.ss\").alias(\"ts string\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timezone Conversion\n",
    "Processing timestamp gets even more complicated, whenweneed to load data from source systems, which are located in different timezones. Whenwewant to analyse flight durations,weshould consider that the timezone offsets between daparture time and arrival time. Otherwise in  our dataresult a flight from Hamburg to London might take just 5 minutes.\n",
    "\n",
    "So to ensure proper time period calculationsweshould normalize all timestamps to a common timezone which is the *Unified Time Coordinated (UTC)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+-----------------------+-----------------------+\n",
      "|now                    |CET ts in UTC          |CEST ts ts in UTC      |Europe/Berlin ts in UTC|\n",
      "+-----------------------+-----------------------+-----------------------+-----------------------+\n",
      "|2023-01-20 19:23:05.322|2023-01-20 18:23:05.322|2023-01-20 18:23:05.322|2023-01-20 18:23:05.322|\n",
      "+-----------------------+-----------------------+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utcDF = df.select(\n",
    "    \"now\",\n",
    "    to_utc_timestamp(\"now\", \"CET\").alias(\"CET ts in UTC\"),\n",
    "    to_utc_timestamp(\"now\", \"CET\").alias(\"CEST ts ts in UTC\"),\n",
    "    to_utc_timestamp(\"now\", \"Europe/Berlin\").alias(\"Europe/Berlin ts in UTC\"),\n",
    ")\n",
    "\n",
    "utcDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, Spark is smart enough to identify, that the given timestamp is in the day-light-saving period where CET = UTC+1 get's shifted to CEST = UTC+2, so in all cases Spark applies the 'Europe/Berlin' rules regardless whetherwedefine this rule rexplicitly orwepass the CET or CEST timezone.\n",
    "\n",
    "Ifweneed to localize UTC timestamps,wejust need to apply the reverse function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|local ts               |\n",
      "+-----------------------+\n",
      "|2023-01-20 19:23:39.474|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utcDF.select(\n",
    "    from_utc_timestamp(\"Europe/Berlin ts in UTC\", \"Europe/Berlin\").alias(\"local ts\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unix timestamp is another alternative for timestamp harmonization:\n",
    "\n",
    "**from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')**\n",
    "Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format.\n",
    "\n",
    "\n",
    "**unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')**\n",
    "Convert time string with given pattern (‘yyyy-MM-dd HH:mm:ss’, by default) to Unix time stamp (in seconds), using the default timezone and the default locale, return null if fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "String Manipulation Functions\n",
    "\n",
    "Case Conversion - lower, upper\n",
    "\n",
    "Getting Length - length\n",
    "\n",
    "Extracting substrings - substring, split\n",
    "\n",
    "Trimming - trim, ltrim, rtrim\n",
    "\n",
    "Padding - lpad, rpad\n",
    "\n",
    "Concatenating string - concat, concat_ws"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
