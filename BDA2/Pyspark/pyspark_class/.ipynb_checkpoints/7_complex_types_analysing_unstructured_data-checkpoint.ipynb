{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Using Complex Types to Analyse Unstructured or JSON Data\n",
    "Our challenge of today is to go beyond processing well structured data, which complies to a schema and where all values are clearly seperated into typed columns. Today we want to analyse the stock descriptions in the retail data set, which come as unstructured text. This is our use case to investigate Sparks complex datataypes like arrays and maps. Next to that, we want to get familiar with the processing of semi-structured data like JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark_start import *\n",
    "\n",
    "retailDF = spark.read\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .format(\"csv\")\\\n",
    "   .load(\"./data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two questions, we want to investigate regarding the description data:\n",
    "* What is the average number of words in the Description per StockCode?\n",
    "* Which are the most frequently used words?\n",
    "\n",
    "## Data Preparation\n",
    "The granularity of our analysis is StockCode and not individual invoice items. So to prevent StockCode duplicates, we tailor the data set to get a DataFrame containing distinct StockCodes and their description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctDF = retailDF.select(\n",
    "        \"StockCode\",\n",
    "        \"Description\").distinct()\n",
    "\n",
    "distinctDF.orderBy(\"StockCode\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the null value problem, we investigated yesterday, occures again. Rows having null values in any column are uselesss for our analysis, so we want to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedDF = distinctDF.dropna(how=\"any\")\n",
    "\n",
    "cleanedDF.orderBy(\"StockCode\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays\n",
    "Next we've  to do is to split up the text strings into arrays of words. The words in the descriptions are seperated by blanks, so we define this as split seperator. The result looks like Python lists but in contrast to lists, all array elements must have the same data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "splittedDF = cleanedDF.select(\n",
    "        \"StockCode\",\n",
    "        split(\"Description\", \" \").alias(\"word_list\")\n",
    ")\n",
    "\n",
    "splittedDF.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with normal Python lists we can grab specific elements, i.e. words from our word lists, by referencing their index starting with 0 for the first element. So to get the second word in each description, we need to refer to index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "splittedDF.select(\"StockCode\", col(\"word_list\")[1]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to note that InvoiceNo 21249 seems to have a double blank after the first word. Maybe a typo in a free-text field? Anyway, we dont to count words, not blanks, so we have to removing them later. First, we want to double check, if this is a more general or single-case issue. \n",
    "\n",
    "We can easily check wether or not a word list contains specific key words by using the `array_contains()` function. For our analysis, we want to identify rows having empty words in the list, which I'dont want to count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "splittedDF.select(\n",
    "    \"StockCode\", \n",
    "    \"word_list\", \n",
    "    array_contains(\"word_list\", \"\").alias(\"empty strings inside\")\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now lets let's clean up the word lists and remove any empty words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_remove\n",
    "\n",
    "cleanedWordListDF = splittedDF.select(\n",
    "    \"StockCode\", \n",
    "    array_remove(\"word_list\", \"\").alias(\"word_list\")\n",
    ")\n",
    "\n",
    "cleanedWordListDF.show(10, truncate=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedWordListDF.select(\n",
    "    \"StockCode\", \n",
    "    \"word_list\", \n",
    "    array_contains(\"word_list\", \"\").alias(\"empty strings inside\")\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yes, it did.\n",
    "\n",
    "Back to our questions. Now, after having cleaned up the data the number of words per stock description is simply the array length which is provided by the `size()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "cleanedWordListDF.select(\n",
    "    \"StockCode\", \n",
    "    size(\"word_list\").alias(\"num_of_words\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "avgDF = cleanedWordListDF.select(\n",
    "    avg(\n",
    "        size(\"word_list\")\n",
    "    ).alias(\"avg_num_of_words\")\n",
    ")\n",
    "\n",
    "avgDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the answer to our first question is that stock descriptions are quite short, just about four words in average.\n",
    "\n",
    "Pyspark module [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) provides further array related functions, which we just list here for later reference:\n",
    "\n",
    "* **array()** - creates a new array column from a list of columns or column expressions that have the **same data type**\n",
    "* **array_distinct(col)** - Collection function: removes duplicate values from the array \n",
    "* **array_except(col1, col2)** - Collection function: returns an array of the elements in col1 but not in col2, without duplicates\n",
    "* **array_intersect(col1, col2)** - Collection function: returns an array of the elements in the intersection of col1 and col2, without duplicates \n",
    "* **array_join()** \n",
    "* **array_max()** - Collection function: returns the maximum value of the array\n",
    "* **array_min()** - Collection function: returns the maximum value of the array\n",
    "* **array_position()** - Collection function: Locates the position of the first occurrence of the given value in the given array\n",
    "* **array_repeat(col, count)** - Collection function: creates an array containing a column repeated count times\n",
    "* **array_sort(col)** - Collection function: sorts the input array in ascending order\n",
    "* **array_union(col1, col2)** - Collection function: returns an array of the elements in the union of col1 and col2, without duplicates\n",
    "* **arrays_overlap(a1, a2)** - Collection function: returns true if the arrays contain any common non-null element\n",
    "* **arrays_zip()** - Collection function: Returns a merged array of structs in which the N-th struct contains all N-th values of input arrays\n",
    "\n",
    "## Explode\n",
    "Two answer our secondf question, it would be easier for us having all words in in column instead of spread across many lists. To turn array elements into rows, we need to apply the `explode()` function. As the name of the function indicates, this can heavily increase the number of rows and the values of all remaining columns get duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "explodedDF = cleanedWordListDF.select(\n",
    "    \"StockCode\",\n",
    "    explode(\"word_list\").alias(\"words\")\n",
    ")\n",
    "\n",
    "explodedDF.orderBy(\"StockCode\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anser to our second question is simply a count of rows per word sorted in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, count, lit\n",
    "\n",
    "explodedDF\\\n",
    "    .groupBy(\"words\")\\\n",
    "    .agg(count(lit(1)).alias(\"word_count\"))\\\n",
    "    .orderBy(desc(\"word_count\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pink stocks seems to be quite popular.\n",
    "\n",
    "## Maps\n",
    "For handling data in key:value structure, Spark provides another complex datatype: *maps*.\n",
    "\n",
    "Our testdata does not provide key:value structured data. So first, we will transform our existing data into maps and second, we can investigate, how to handle key:value source data as an input to our ETL dataprocessing.\n",
    "\n",
    "### Creating Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFlight = spark.read\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .csv(\"./data/flight-data/2015-summary.csv\")\n",
    "\n",
    "from pyspark.sql.functions import lit, struct, array, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "arrDF = dfFlight.select(\n",
    "    array(\n",
    "        lit(\"destination\"),\n",
    "        lit(\"origin\"),\n",
    "        lit(\"count\")\n",
    "    ).alias(\"key\"),\n",
    "    array(\n",
    "        \"DEST_COUNTRY_NAME\",\n",
    "        \"ORIGIN_COUNTRY_NAME\",\n",
    "        col(\"count\").cast(StringType())\n",
    "    ).alias(\"value\")\n",
    ")\n",
    "\n",
    "arrDF.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import map_from_arrays\n",
    "\n",
    "mapDF = arrDF.select(\n",
    "    map_from_arrays(\"key\", \"value\").alias(\"data_map\")\n",
    ")\n",
    "\n",
    "mapDF.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDF.select(col(\"data_map\")[\"destination\"]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDF.select(col(\"data_map\")[\"origin\"]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "\n",
    "mapDF.select(\n",
    "        map_keys(\"data_map\")\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "\n",
    "mapDF.select(\n",
    "        map_values(\"data_map\")\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we've  processed so far looks at least semi-structured because the keys and values all appear in identical order. So there is still an implicit schema because all rows match to the same pattern:\n",
    "\n",
    "destination -> descVal, origin -> origValue, count -> cntVal\n",
    "\n",
    "What would happen, if rows have keys and values in different order? Because our testdata does not provide examples for this, we create a DataFrame manuall with synthetic data in multiple orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unstructuredDF = spark.createDataFrame(\n",
    "        [\n",
    "            ([\"destination\", \"origin\", \"count\"], [\"United States\", \"Germany\", \"10\"],), \n",
    "            ([\"count\", \"origin\", \"destination\"], [\"25\", \"France\", \"Spain\"],),\n",
    "            ([\"count\", \"destination\", \"origin\"], [\"75\", \"Italy\", \"Spain\"],)\n",
    "        ], \n",
    "        [\"key\", \"value\"]\n",
    ")\n",
    "\n",
    "unstructuredDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDF2 = unstructuredDF.select(\n",
    "    map_from_arrays(\"key\", \"value\").alias(\"data_map\")\n",
    ")\n",
    "\n",
    "mapDF2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDF2.select(col(\"data_map\")[\"origin\"]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily the odering doesn't matter because we reference the values by keys and not by positions. Maps are more like dictionaries than lists or arrays.\n",
    "\n",
    "### Turning Maps into DataFrames\n",
    "\n",
    "So with our self-created map we can now investigate how to handle such data as input for our ETL process which finally will write data in tabular form into a file or database table. So as intermediate step, we will have to align more or less ordered *key:value* pairs with the schema of a `DataFrame`. \n",
    "\n",
    "Can the `explode()` function help again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDF2.select(explode(\"data_map\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, yes and no. Yes, `explode()` accepts both arrays as well as maps as an argument. No, because now we've  lost the information, which three rows belong together. Additionally our intention was to gain three columns, one for each key value, and not just two. For maps referencing by key is always a better approach than referencing by position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDF2.select(\n",
    "    col(\"data_map\")[\"destination\"].alias(\"destination\"),\n",
    "    col(\"data_map\")[\"origin\"].alias(\"origin\"),\n",
    "    col(\"data_map\")[\"count\"].alias(\"count\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with Spark handling nearly unstructured data records of key:value pairs in different orders is not a big problem.\n",
    "\n",
    "Pyspark module <a href=https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions>pyspark.sql.functions</a> provides further map related functions, which we also just list here for later reference:\n",
    "\n",
    "* **map_concat()** - Returns the union of all the given maps\n",
    "* **map_from_entries()** - Collection function: Returns a map created from the given array of entries\n",
    "\n",
    "### Turning Arrays or Maps into JSON\n",
    "A nice Spark feature is the `to_json()` function which converts StructType, ArrayType or MapType data into JSON. This can be relevant for us if we have to call a REST API which expects JSON documents as paylod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "mapDF2.select(to_json(\"data_map\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapDF2.select(to_json(\"data_map\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Semi-structured JSON data\n",
    "As we've  learned on day 3, reading data from JSON file and transforming it into a DataFreame is quite simple. Just for repetition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF = spark.read\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .format(\"json\")\\\n",
    "   .load(\"./data/flight-data/2015-summary.json\")\\\n",
    "\n",
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what have we to to in case of having tabular data where only one column contains JSON strings? To check this out first we create same testdata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "        [\n",
    "            (123, \"DUS\", '{\"destinations\" : [\"FRA\", \"MUC\", \"TXL\"], \"airlines\" : [\"LH\", \"EW\", \"RY\"]}'), \n",
    "            (456, \"FRA\", '{\"destinations\" : [\"CDG\", \"MUC\", \"JFK\"], \"airlines\" : [\"AF\", \"LH\", \"DL\"]}'),\n",
    "            (789, \"MUC\", '{\"destinations\" : [\"FRA\", \"ZUC\", \"DUS\"], \"airlines\" : [\"EW\", \"LH\", \"EW\"]}')\n",
    "        ], \n",
    "        [\"key\", \"airport\", \"dest\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation along JSON Paths\n",
    "Each row in the \"dest\" column contains a valid JSON document. Now we can use the `get_json_object()` function to access the values inside of the JSON documents by specifiying the path from the root element (represented by `$`) down the nesting hierarchie to the specific JSON obect we want to extract. \n",
    "\n",
    "path: `$.key_level1.key_level_2....key_level_n`\n",
    "\n",
    "Since in our DataFrame the objects \"destinations\", and \"airlines\" have value lists, we have to specify the list index to get one singular value per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "\n",
    "df.select(\n",
    "        \"key\", \n",
    "        \"airport\",\n",
    "        get_json_object(\"dest\", '$.destinations[2]').alias(\"destination\"),\n",
    "        get_json_object(\"dest\", '$.airlines[1]').alias(\"airline\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we omitt the list index, I'll get the entire value list in our result DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "        \"key\", \n",
    "        \"airport\",\n",
    "        get_json_object(\"dest\", '$.destinations').alias(\"destination\"),\n",
    "        get_json_object(\"dest\", '$.airlines').alias(\"airline\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a similar function `json_tuple()` but we are not sure if it provides any benefits to me, because:\n",
    "1. we cannot use it if the JSON document has more than one level of nesting, and\n",
    "1. we cannot refer to single list elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "df.select(\"key\", \n",
    "          \"airport\",\n",
    "          json_tuple(\"dest\", \"destinations\", \"airlines\").alias(\"destination\", \"airline\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning JSON to Map based on Schema\n",
    "Finally, like we can read from JSON files using an explicit schema definition, we can also apply `from_json()` on DataFrame columns containing JSON by using a schema. Depending on the schema definition `from_json()` will return StructType, ArrayType or MapType. Actually we could perform a conversion round-trip  from StructType, ArrayType or MapType -> `to_json()` -> {Json} -> `from_json()` ->  StructType, ArrayType or MapType.\n",
    "\n",
    "convert the Json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "jsonSchema = MapType(\n",
    "    StringType(), \n",
    "    ArrayType(StringType(), True),\n",
    "    True\n",
    ")\n",
    "\n",
    "mappedDF = df.select(\"key\", \n",
    "          \"airport\",\n",
    "         from_json(\"dest\", jsonSchema).alias(\"json_data\")\n",
    ")\n",
    "\n",
    "mappedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can navigate on the Map structure to extract single values similar to navigating the JSON path using `get_json_object()`, e.g. grabbing the third element of the destinations lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappedDF.select(\n",
    "    \"key\", \n",
    "    \"airport\",\n",
    "    col(\"json_data\")[\"destinations\"][2]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is: what is the benefit of taking these extra effort, defining a schema and converting JSON to Map? In our opinion this leads to cleaner code and a better design, because:\n",
    "1. now the JSON structure, a mexpecting is explicitly documented in the code by the schema instead of implicitly assumed \n",
    "1. the Map structure is a unifying abstraction of any key:value data, regardles of the source format, e.g. CSV file, JSON documents or key-value database tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
