{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Joins\n",
    "On of the strength of Spark is, that we can combine data from many different sources, regardless of their source format or system. To do this, we need to join data records sharing at least one common key column. So the topic of today is: how does Spark joins data together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark_start import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create two DataFrames with students and university data, which we will use to checkout Spark's join operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import NullType\n",
    "leftDF = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Peter Pan\", \"1999-04-23\", 90, 3),\n",
    "        (2, \"Alice Smith\", \"2000-05-15\", 100, 2),\n",
    "        (3, \"Bob Miller\", \"2000-09-01\", 80, 3),\n",
    "        (4, \"April May\", \"1999-09-30\", 110, 1),\n",
    "        (5, \"Billie Jean\", \"1990-07-12\", 360, None)\n",
    "    ],\n",
    "    [\"leftid\", \"name\", \"date-of-birth\", \"creditscore\", \"universityid\"]\n",
    ")\n",
    "leftDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rightDF = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Harvard\", \"Massachusetts\", 1636),\n",
    "        (2, \"MIT\", \"Massachusetts\", 1861),\n",
    "        (3, \"Stanford\", \"California\", 1891),\n",
    "        (4, \"UC Berkeley\", \"California\", 1868),\n",
    "        (5, \"Princeton\", \"New Jersey\", 1746)\n",
    "    ],\n",
    "    [\"rightid\", \"name\", \"state\", \"founded\"]\n",
    ")\n",
    "rightDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Types\n",
    "To perform a join we need to define two things, a join expression and the join type. The join expression can be any complex expressions that evaluates to True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpression = leftDF[\"universityid\"] == rightDF[\"rightid\"]\n",
    "joinType = \"inner\"\n",
    "\n",
    "leftDF.join(rightDF, on=joinExpression, how=joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"left_outer\"\n",
    "leftDF.join(rightDF, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"right_outer\"\n",
    "leftDF.join(rightDF, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"full_outer\"\n",
    "leftDF.join(rightDF, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join types:\n",
    "* inner (is the default)\n",
    "* cross \n",
    "* outer, full, full_outer (all are equivalent) \n",
    "* left, left_outer (both are equivalent) \n",
    "* right, right_outer (both are equivalent) \n",
    "* left_semi\n",
    "* left_anti\n",
    "\n",
    "Especially the last to join types are interesting because they implement a do-exist respectively do-not-exist logic. The columns of the right side do not appear in the result set. The right side is just used for the existence check. Using these join types savesusfrom removing many columns from the result DataFrame if we are not interested in the column values of the right side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"left_semi\"\n",
    "leftDF.join(rightDF, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"left_anti\"\n",
    "leftDF.join(rightDF, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are wondering why there is an additiona `crossJoin()` function, even though we can use `join()` with join type `cross`. So we want to double check if both functions will provide trhe same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftDF.join(rightDF, joinExpression, how=\"cross\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks to me, that `join(..., how=\"cross\")` doesn't work because it provides the same result as an inner join. What about `crossJoin()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftDF.crossJoin(rightDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, `crossJoin()` works properly.\n",
    "\n",
    "Just to remind me: we can do all this joining stuff also with our well-known SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftDF.createOrReplaceTempView(\"leftTable\")\n",
    "rightDF.createOrReplaceTempView(\"rightTable\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM leftTable INNER JOIN rightTable ON universityid = rightid\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Joins vs. Broadcast Joins\n",
    "To handle very large data sets, DataFrames get partitioned and these partitions are spread accross the Spark cluster.  Therefore join operations are like to be wide transformations where Spark has to shuffle the data between all nodes to match corresponding rows of the joined DataFrames. As we learned on day 3, shuffle operations are wide transformations which cannot be perfomred entirely in-memory. Additionally, this all-to-all communication between the participating nodes increases heavily with the more nodes get involved. Such *shuffle joins* can cause performance issues in the data pipeline so we would like to avoid them whenever possible.\n",
    "\n",
    "There is actually a chance to avoid a shuffle join when at least one of the joined DataFrames is small enough to fit into each nodes' memory. In that case it can be broadcasted to all nodes so that the join operations can be applied locally and in-memory without further shuffling. To initiate a broadcast join, we hust have to appley the `broadcast()` function on the small table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joinExpression = leftDF[\"universityid\"] == rightDF[\"rightid\"]\n",
    "joinType = \"inner\"\n",
    "\n",
    "leftDF.join(broadcast(rightDF), on=joinExpression, how=joinType).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explain plan shows that two steps of the join transformation: one `BroadcastExchange` step followed by one -> `BroadcastHashJoin` step.\n",
    "    \n",
    "Without using the `broadcast()` function the explain plan shows the shuffle join comprising two parallel `Exchange hashpartitioning` stepts and one final `SortMergeJoin` step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftDF.join(rightDF, on=joinExpression, how=joinType).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
