{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd807928-2655-4eed-93c3-85cc2d662ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-30 01:50:16,982 - INFO - Converting C:\\Users\\user1\\Documents\\BDA2\\data\\Promotion_data.xlsx to CSV...\n",
      "2025-09-30 01:50:17,032 - INFO - Successfully saved CSV to C:\\Users\\user1\\Documents\\BDA2\\data\\csv\\Promotion_data.csv\n",
      "2025-09-30 01:50:17,035 - INFO - Converting C:\\Users\\user1\\Documents\\BDA2\\data\\Promotion_new_data.xlsx to CSV...\n",
      "2025-09-30 01:50:17,054 - INFO - Successfully saved CSV to C:\\Users\\user1\\Documents\\BDA2\\data\\csv\\Promotion_new_data.csv\n",
      "2025-09-30 01:50:17,209 - INFO - Spark session created successfully.\n",
      "2025-09-30 01:50:17,234 - INFO - Loading CSV from: C:\\Users\\user1\\Documents\\BDA2\\data\\csv\\Promotion_data.csv\n",
      "2025-09-30 01:50:17,487 - INFO - Loaded 119 rows and 10 columns.\n",
      "2025-09-30 01:50:17,488 - INFO - Starting data validation for 'Promotion_data.csv'...\n",
      "2025-09-30 01:50:18,130 - ERROR - [NEGATIVE CHECK] Column 'Gross Margin $' has 9 negative values in 'Promotion_data.csv'.\n",
      "2025-09-30 01:50:18,641 - WARNING - [SALES CHECK] Found 70 sales consistency mismatches in 'Promotion_data.csv'.\n",
      "2025-09-30 01:50:18,642 - INFO - Data validation finished for 'Promotion_data.csv'.\n",
      "2025-09-30 01:50:18,642 - INFO - Loading CSV from: C:\\Users\\user1\\Documents\\BDA2\\data\\csv\\Promotion_new_data.csv\n",
      "2025-09-30 01:50:18,814 - INFO - Loaded 15 rows and 10 columns.\n",
      "2025-09-30 01:50:18,814 - INFO - Starting data validation for 'Promotion_new_data.csv'...\n",
      "2025-09-30 01:50:19,726 - WARNING - [SALES CHECK] Found 7 sales consistency mismatches in 'Promotion_new_data.csv'.\n",
      "2025-09-30 01:50:19,727 - INFO - Data validation finished for 'Promotion_new_data.csv'.\n",
      "2025-09-30 01:50:19,739 - INFO - Temporary SQL view 'promotion_data_old' created.\n",
      "2025-09-30 01:50:19,759 - INFO - Transformation query executed successfully for 'promotion_data_old'.\n",
      "2025-09-30 01:50:19,773 - INFO - Temporary SQL view 'promotion_data_new' created.\n",
      "2025-09-30 01:50:19,789 - INFO - Transformation query executed successfully for 'promotion_data_new'.\n",
      "2025-09-30 01:50:19,895 - INFO - Combined dataframes. Total rows: 134\n",
      "+----+-----------+-------+-----+--------+----------------+----------------+-----+------------------+--------------------+--------------------+-------------+------------------+--------------+---------------+\n",
      "|Year|week_number|Product|Price|Discount|Discount_Percent|Final_Unit_Price|Units|Sales_dollars     |Gross_Margin_dollars|Gross_Margin_Percent|On_Flyer_Flag|High_Discount_Flag|Sales_Category|Week_Start_Date|\n",
      "+----+-----------+-------+-----+--------+----------------+----------------+-----+------------------+--------------------+--------------------+-------------+------------------+--------------+---------------+\n",
      "|2021|1          |PANTENE|7.99 |0.0     |0.0             |7.99            |8630 |68953.7           |33570.7             |48.69               |0            |0                 |High          |2020-12-27     |\n",
      "|2021|2          |PANTENE|7.29 |0.1     |10.0            |6.56            |10183|74234.07          |32483.77000000001   |43.76               |1            |0                 |High          |2021-01-03     |\n",
      "|2021|3          |PANTENE|5.49 |0.3     |30.0            |3.84            |21568|118408.32         |29979.52000000001   |25.32               |1            |0                 |High          |2021-01-10     |\n",
      "|2021|4          |PANTENE|7.49 |0.05    |5.0             |7.12            |9309 |69724.41          |31557.510000000013  |45.26               |1            |0                 |High          |2021-01-17     |\n",
      "|2021|5          |PANTENE|7.99 |0.0     |0.0             |7.99            |8462 |67611.38          |32917.180000000015  |48.69               |0            |0                 |High          |2021-01-24     |\n",
      "|2021|6          |PANTENE|3.99 |0.5     |50.0            |2.0             |57742|230390.58000000002|-6351.619999999967  |-2.76               |1            |0                 |High          |2021-01-31     |\n",
      "|2021|7          |PANTENE|7.99 |0.0     |0.0             |7.99            |8884 |70983.16          |34558.76            |48.69               |0            |0                 |High          |2021-02-07     |\n",
      "|2021|8          |PANTENE|7.99 |0.0     |0.0             |7.99            |8463 |67619.37          |32921.07            |48.69               |0            |0                 |High          |2021-02-14     |\n",
      "|2021|9          |PANTENE|4.79 |0.4     |40.0            |2.87            |33108|158587.32         |22844.52000000001   |14.41               |1            |0                 |High          |2021-02-21     |\n",
      "|2021|10         |PANTENE|7.99 |0.0     |0.0             |7.99            |8463 |67619.37          |32921.07            |48.69               |0            |0                 |High          |2021-02-28     |\n",
      "+----+-----------+-------+-----+--------+----------------+----------------+-----+------------------+--------------------+--------------------+-------------+------------------+--------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "2025-09-30 01:50:20,137 - INFO - Writing 134 rows to JDBC table: dbo.PromotionTable\n",
      "2025-09-30 01:50:20,374 - INFO - Successfully wrote data to database.\n",
      "2025-09-30 01:50:20,374 - INFO - Promotion pipeline finished successfully.\n",
      "2025-09-30 01:50:20,396 - INFO - Attached log file: logs\\promotionpipeline_20250930_015016.log\n",
      "2025-09-30 01:50:21,967 - INFO - Status email sent successfully.\n",
      "2025-09-30 01:50:22,294 - INFO - Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# PySpark Promotion Pipeline (Refactored to read secrets from config.ini)\n",
    "\n",
    "import configparser  ### NEW ###\n",
    "import logging\n",
    "import os\n",
    "import smtplib\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from email.mime.application import MIMEApplication\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (abs as spark_abs, col, concat_ws, count,\n",
    "                                   isnan, lit, to_date, trim, upper, when)\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "# ===================== LOAD CONFIGURATION FROM FILE ===================== ### NEW ###\n",
    "config = configparser.ConfigParser()\n",
    "# Assumes config.ini is in the same directory as the script\n",
    "config.read('config.ini')\n",
    "\n",
    "db_password = config.get('database', 'password')\n",
    "email_user = config.get('email', 'user')\n",
    "email_password = config.get('email', 'password')\n",
    "\n",
    "# ===================== MAIN CONFIG DICTIONARY =====================\n",
    "# Secrets are now loaded from the config.ini file, not hardcoded here.\n",
    "CONFIG = {\n",
    "    \"app_name\": \"PromotionPipeline\",\n",
    "    \"log_dir\": \"logs\",\n",
    "    \"data_dir\": os.path.join(os.environ[\"USERPROFILE\"], \"Documents\", \"BDA2\", \"data\"),\n",
    "    \"old_data_xls\": \"Promotion_data.xlsx\",\n",
    "    \"new_data_xls\": \"Promotion_new_data.xlsx\",\n",
    "    \"jdbc\": {\n",
    "        \"url\": \"jdbc:sqlserver://localhost:1433;databaseName=datahub;encrypt=true;trustServerCertificate=true\",\n",
    "        \"user\": \"sa\",\n",
    "        \"password\": db_password,  ### MODIFIED ###\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "        \"table\": \"dbo.PromotionTable\",\n",
    "        \"mode\": \"overwrite\"\n",
    "    },\n",
    "    \"email\": {\n",
    "        \"host\": \"smtp.gmail.com\",\n",
    "        \"port\": 587,\n",
    "        \"user\": email_user,      ### MODIFIED ###\n",
    "        \"password\": email_password, ### MODIFIED ###\n",
    "        \"to\": email_user # You can also move this to the config file if you want\n",
    "    }\n",
    "}\n",
    "\n",
    "# ===================== HELPER FUNCTIONS =====================\n",
    "\n",
    "def setup_logging(app_name: str, log_dir: str) -> (logging.Logger, str):\n",
    "    \"\"\"Configures and returns a logger and the log file path.\"\"\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_filename = datetime.now().strftime(os.path.join(log_dir, f\"{app_name.lower()}_%Y%m%d_%H%M%S.log\"))\n",
    "\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        handlers=[logging.FileHandler(log_filename, encoding=\"utf-8\"), logging.StreamHandler(sys.stdout)],\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    return logging.getLogger(app_name), log_filename\n",
    "\n",
    "def convert_excel_to_csv(xls_path: str, out_dir: str) -> str:\n",
    "    \"\"\"Converts the first sheet of an Excel file to a CSV file.\"\"\"\n",
    "    logger = logging.getLogger(CONFIG[\"app_name\"])\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    base_name = os.path.splitext(os.path.basename(xls_path))[0]\n",
    "    out_file = os.path.join(out_dir, f\"{base_name}.csv\")\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Converting {xls_path} to CSV...\")\n",
    "        xls = pd.ExcelFile(xls_path)\n",
    "        df = pd.read_excel(xls, sheet_name=xls.sheet_names[0])\n",
    "        df.to_csv(out_file, index=False)\n",
    "        logger.info(f\"Successfully saved CSV to {out_file}\")\n",
    "        return out_file\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to convert Excel file {xls_path}: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def create_spark_session(app_name: str) -> SparkSession:\n",
    "    \"\"\"Creates and returns a Spark session.\"\"\"\n",
    "    spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "    logging.getLogger(app_name).info(\"Spark session created successfully.\")\n",
    "    return spark\n",
    "\n",
    "def load_data(spark: SparkSession, csv_path: str) -> DataFrame:\n",
    "    \"\"\"Loads a CSV file into a Spark DataFrame.\"\"\"\n",
    "    logger = logging.getLogger(CONFIG[\"app_name\"])\n",
    "    logger.info(f\"Loading CSV from: {csv_path}\")\n",
    "    df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(csv_path)\n",
    "    )\n",
    "    logger.info(f\"Loaded {df.count()} rows and {len(df.columns)} columns.\")\n",
    "    return df\n",
    "\n",
    "def validate_dataframe(df: DataFrame, df_name: str):\n",
    "    \"\"\"Performs all data validation checks from the original script.\"\"\"\n",
    "    logger = logging.getLogger(CONFIG[\"app_name\"])\n",
    "    logger.info(f\"Starting data validation for '{df_name}'...\")\n",
    "\n",
    "    # 1. Null counts per column\n",
    "    null_counts = df.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in df.columns]).first().asDict()\n",
    "    for c, n in null_counts.items():\n",
    "        if n > 0:\n",
    "            logger.warning(f\"[NULL CHECK] Column '{c}' has {n} nulls in '{df_name}'.\")\n",
    "\n",
    "    # 2. Negative values check for specific numeric columns\n",
    "    numeric_cols_to_check = [\"Price\", \"Discount\", \"Units\", \"Sales $\", \"Gross Margin $\", \"# Transactions that contained the product\"]\n",
    "    for colname in numeric_cols_to_check:\n",
    "        if colname in df.columns:\n",
    "            negatives = df.filter(col(colname) < 0).count()\n",
    "            if negatives > 0:\n",
    "                logger.error(f\"[NEGATIVE CHECK] Column '{colname}' has {negatives} negative values in '{df_name}'.\")\n",
    "\n",
    "    # 3. Discount between 0 and 1\n",
    "    invalid_discounts = df.filter((col(\"Discount\") < 0) | (col(\"Discount\") > 1)).count()\n",
    "    if invalid_discounts > 0:\n",
    "        logger.error(f\"[DISCOUNT CHECK] Found {invalid_discounts} invalid discount values (not between 0 and 1) in '{df_name}'.\")\n",
    "\n",
    "    # 4. 'On Flyer?' column contains only Yes/No\n",
    "    invalid_on_flyer = df.filter(~col(\"On Flyer?\").isin(\"Yes\", \"No\")).count()\n",
    "    if invalid_on_flyer > 0:\n",
    "        logger.error(f\"[ON FLYER CHECK] Found {invalid_on_flyer} invalid values in 'On Flyer?' column in '{df_name}'.\")\n",
    "\n",
    "    # 5. Year range check (2000-2030)\n",
    "    invalid_years = df.filter((col(\"Year\") < 2000) | (col(\"Year\") > 2030)).count()\n",
    "    if invalid_years > 0:\n",
    "        logger.error(f\"[YEAR CHECK] Found {invalid_years} invalid year values in '{df_name}'.\")\n",
    "\n",
    "    # 6. Week number between 1 and 53\n",
    "    invalid_weeks = df.filter((col(\"week number\") < 1) | (col(\"week number\") > 53)).count()\n",
    "    if invalid_weeks > 0:\n",
    "        logger.error(f\"[WEEK CHECK] Found {invalid_weeks} invalid week numbers in '{df_name}'.\")\n",
    "\n",
    "    # 7. Sales consistency check: Sales $ â‰ˆ Units * Price * (1 - Discount)\n",
    "    expected_sales = (col(\"Units\") * col(\"Price\") * (lit(1) - col(\"Discount\")))\n",
    "    sales_mismatch = df.filter(spark_abs(col(\"Sales $\") - expected_sales) > 1e-2).count()\n",
    "    if sales_mismatch > 0:\n",
    "        logger.warning(f\"[SALES CHECK] Found {sales_mismatch} sales consistency mismatches in '{df_name}'.\")\n",
    "\n",
    "    logger.info(f\"Data validation finished for '{df_name}'.\")\n",
    "\n",
    "def transform_data(spark: SparkSession, df: DataFrame, view_name: str) -> DataFrame:\n",
    "    \"\"\"Transforms raw promotion data using Spark SQL.\"\"\"\n",
    "    logger = logging.getLogger(CONFIG[\"app_name\"])\n",
    "    \n",
    "    # Clean column names for SQL compatibility\n",
    "    clean_cols_df = df.toDF(*[c.strip().replace(\" \", \"_\").replace(\"?\", \"\").replace(\"$\", \"dollars\").replace(\"#\", \"num\") for c in df.columns])\n",
    "    clean_cols_df.createOrReplaceTempView(view_name)\n",
    "    logger.info(f\"Temporary SQL view '{view_name}' created.\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        Year,\n",
    "        week_number,\n",
    "        UPPER(TRIM(Product)) AS Product,\n",
    "        Price,\n",
    "        Discount,\n",
    "        ROUND(Discount * 100, 2) AS Discount_Percent,\n",
    "        ROUND(Price * (1 - Discount), 2) AS Final_Unit_Price,\n",
    "        Units,\n",
    "        Sales_dollars,\n",
    "        Gross_Margin_dollars,\n",
    "        ROUND((Gross_Margin_dollars / NULLIF(Sales_dollars, 0)) * 100, 2) AS Gross_Margin_Percent,\n",
    "        CASE WHEN On_Flyer = 'Yes' THEN 1 ELSE 0 END AS On_Flyer_Flag,\n",
    "        CASE WHEN Discount > 0.5 THEN 1 ELSE 0 END AS High_Discount_Flag,\n",
    "        CASE \n",
    "            WHEN Units >= 100 THEN 'High'\n",
    "            WHEN Units >= 50 THEN 'Medium'\n",
    "            ELSE 'Low'\n",
    "        END AS Sales_Category,\n",
    "        -- Corrected date format pattern for Spark 3.0+\n",
    "        to_date(concat_ws('-', Year, week_number), 'YYYY-ww') AS Week_Start_Date\n",
    "    FROM {view_name}\n",
    "    \"\"\"\n",
    "    transformed_df = spark.sql(query)\n",
    "    logger.info(f\"Transformation query executed successfully for '{view_name}'.\")\n",
    "    return transformed_df\n",
    "\n",
    "def write_to_jdbc(df: DataFrame, jdbc_config: dict):\n",
    "    \"\"\"Writes a DataFrame to a JDBC data source.\"\"\"\n",
    "    logger = logging.getLogger(CONFIG[\"app_name\"])\n",
    "    logger.info(f\"Writing {df.count()} rows to JDBC table: {jdbc_config['table']}\")\n",
    "    \n",
    "    props = {\"user\": jdbc_config[\"user\"], \"password\": jdbc_config[\"password\"], \"driver\": jdbc_config[\"driver\"]}\n",
    "    \n",
    "    try:\n",
    "        df.write.jdbc(\n",
    "            url=jdbc_config[\"url\"], \n",
    "            table=jdbc_config[\"table\"], \n",
    "            mode=jdbc_config[\"mode\"], \n",
    "            properties=props\n",
    "        )\n",
    "        logger.info(\"Successfully wrote data to database.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"JDBC write failed: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def send_status_email(log_file: str, email_config: dict, success: bool):\n",
    "    \"\"\"Sends a pipeline status email with the log file attached.\"\"\"\n",
    "    logger = logging.getLogger(CONFIG[\"app_name\"])\n",
    "\n",
    "    if not all([email_config[\"user\"], email_config[\"password\"], email_config[\"to\"]]):\n",
    "        logger.warning(\"Email configuration is incomplete. Skipping email notification.\")\n",
    "        return\n",
    "\n",
    "    status = \"SUCCESS\" if success else \"FAILURE\"\n",
    "    subject = f\"Promotion Pipeline Status: {status}\"\n",
    "    body = f\"The Promotion Pipeline completed with status: {status}.\\n\\nPlease see the attached log file for details.\"\n",
    "\n",
    "    msg = MIMEMultipart()\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = email_config[\"user\"]\n",
    "    msg[\"To\"] = email_config[\"to\"]\n",
    "    msg.attach(MIMEText(body, \"plain\", _charset=\"utf-8\"))\n",
    "\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, \"rb\") as f:\n",
    "            part = MIMEApplication(f.read(), Name=os.path.basename(log_file))\n",
    "            part.add_header(\"Content-Disposition\", f'attachment; filename=\"{os.path.basename(log_file)}\"')\n",
    "            msg.attach(part)\n",
    "        logger.info(f\"Attached log file: {log_file}\")\n",
    "\n",
    "    try:\n",
    "        with smtplib.SMTP(email_config[\"host\"], email_config[\"port\"]) as server:\n",
    "            server.starttls()\n",
    "            server.login(email_config[\"user\"], email_config[\"password\"])\n",
    "            server.send_message(msg)\n",
    "        logger.info(\"Status email sent successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to send status email: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# ===================== MAIN EXECUTION =====================\n",
    "def main():\n",
    "    \"\"\"Main ETL pipeline orchestration function.\"\"\"\n",
    "    logger, log_filename = setup_logging(CONFIG[\"app_name\"], CONFIG[\"log_dir\"])\n",
    "    pipeline_success = False\n",
    "    spark = None\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Convert Excel files to CSV\n",
    "        csv_dir = os.path.join(CONFIG[\"data_dir\"], \"csv\")\n",
    "        old_xls_path = os.path.join(CONFIG[\"data_dir\"], CONFIG[\"old_data_xls\"])\n",
    "        new_xls_path = os.path.join(CONFIG[\"data_dir\"], CONFIG[\"new_data_xls\"])\n",
    "        \n",
    "        old_csv_path = convert_excel_to_csv(old_xls_path, csv_dir)\n",
    "        new_csv_path = convert_excel_to_csv(new_xls_path, csv_dir)\n",
    "\n",
    "        # Step 2: Initialize Spark\n",
    "        spark = create_spark_session(CONFIG[\"app_name\"])\n",
    "        spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "        # Step 3: Load and Validate Data\n",
    "        df_old = load_data(spark, old_csv_path)\n",
    "        validate_dataframe(df_old, \"Promotion_data.csv\")\n",
    "\n",
    "        df_new = load_data(spark, new_csv_path)\n",
    "        validate_dataframe(df_new, \"Promotion_new_data.csv\")\n",
    "\n",
    "        # Step 4: Transform Data\n",
    "        df_transformed_old = transform_data(spark, df_old, \"promotion_data_old\")\n",
    "        df_transformed_new = transform_data(spark, df_new, \"promotion_data_new\")\n",
    "\n",
    "        # Step 5: Combine Data\n",
    "        df_combined = df_transformed_old.unionByName(df_transformed_new)\n",
    "        logger.info(f\"Combined dataframes. Total rows: {df_combined.count()}\")\n",
    "        df_combined.show(10, truncate=False)\n",
    "\n",
    "        # Step 6: Load to Database\n",
    "        write_to_jdbc(df_combined, CONFIG[\"jdbc\"])\n",
    "\n",
    "        pipeline_success = True\n",
    "        logger.info(\"Promotion pipeline finished successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Promotion pipeline failed: {e}\", exc_info=True)\n",
    "        pipeline_success = False\n",
    "\n",
    "    finally:\n",
    "        # Step 7: Send Notification\n",
    "        send_status_email(log_filename, CONFIG[\"email\"], pipeline_success)\n",
    "        \n",
    "        # Stop Spark Session\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "            logger.info(\"Spark session stopped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6f909-5573-4789-a47a-537064cce750",
   "metadata": {},
   "source": [
    "@echo off\n",
    "REM =================================================================\n",
    "REM BATCH FILE TO RUN PYSPARK SCRIPT WITH python.exe\n",
    "REM (Corrected and using %USERPROFILE%)\n",
    "REM =================================================================\n",
    "\n",
    "REM --- SET YOUR VARIABLES HERE ---\n",
    "\n",
    "REM ### FIX 1: JAVA_HOME IS REQUIRED FOR PYSPARK TO WORK ###\n",
    "REM 1. Set the path to your Java installation\n",
    "SET JAVA_HOME=C:\\Program Files\\Java\\jdk1.8.0_331\n",
    "\n",
    "REM ### FIX 2: USE THE FULL, SPECIFIC PATH TO YOUR SPARK INSTALLATION ###\n",
    "REM 2. Set the path to your Spark installation\n",
    "SET SPARK_HOME=C:\\spark\\\n",
    "\n",
    "REM ### FIX 3: USE THE FULL, ABSOLUTE PATH TO python.exe ###\n",
    "REM 3. Set the path to your python.exe\n",
    "SET PYTHON_EXECUTABLE=C:\\ProgramData\\Anaconda3\\python.exe\n",
    "\n",
    "REM ### YOUR REQUEST: Using %USERPROFILE% for the script directory ###\n",
    "REM 4. Set the path to the directory containing your script\n",
    "SET SCRIPT_DIR=%USERPROFILE%\\Downloads\n",
    "\n",
    "REM 5. Set the name of your python script\n",
    "SET PYTHON_SCRIPT=data_pipeline_prod.py\n",
    "\n",
    "REM 6. Define a log directory\n",
    "SET LOG_DIR=%USERPROFILE%\\Downloads\\pipeline_logs\n",
    "REM ---------------------------------\n",
    "\n",
    "REM Automatically create the log directory if it doesn't exist\n",
    "if not exist \"%LOG_DIR%\" mkdir \"%LOG_DIR%\"\n",
    "\n",
    "REM Change the current directory to the script's directory\n",
    "cd /d \"%SCRIPT_DIR%\"\n",
    "\n",
    "echo =================================================================\n",
    "echo Starting Promotion Pipeline at %date% %time%\n",
    "echo Script Directory: %SCRIPT_DIR%\n",
    "echo Python Executable: %PYTHON_EXECUTABLE%\n",
    "echo =================================================================\n",
    "\n",
    "REM Run the python script and redirect output to a log file\n",
    "\"%PYTHON_EXECUTABLE%\" \"%PYTHON_SCRIPT%\" >> \"%LOG_DIR%\\pipeline_run_log.txt\" 2>&1\n",
    "\n",
    "echo.\n",
    "echo Pipeline execution finished at %date% %time%\n",
    "echo See log file in %LOG_DIR%\n",
    "echo =================================================================\n",
    "echo.\n",
    "\n",
    "REM Keep the window open\n",
    "PAUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc6b63-2936-4814-9b18-8ce2256edbad",
   "metadata": {},
   "source": [
    "@echo off\n",
    "REM =================================================================\n",
    "REM BATCH FILE TO RUN PYSPARK - FINAL VERSION WITH CALL\n",
    "REM =================================================================\n",
    "\n",
    "REM --- SET YOUR VARIABLES HERE (USE FULL PATHS!) ---\n",
    "\n",
    "REM 1. Set the path to your Java installation\n",
    "SET JAVA_HOME=C:\\Program Files\\Java\\jdk1.8.0_331\n",
    "\n",
    "REM 2. Set the path to your Spark installation\n",
    "SET SPARK_HOME=C:\\spark\n",
    "\n",
    "REM 3. Set the full, absolute path to your python.exe\n",
    "SET PYTHON_EXECUTABLE=C:\\ProgramData\\Anaconda3\\python.exe\n",
    "\n",
    "REM 4. Set the path to the directory containing your script\n",
    "SET SCRIPT_DIR=%USERPROFILE%\\Downloads\n",
    "\n",
    "REM 5. Set the name of your python script\n",
    "SET PYTHON_SCRIPT=data_pipeline_prod.py\n",
    "\n",
    "REM 6. Set the full path to your SQL Server JDBC driver JAR file\n",
    "SET JDBC_DRIVER_PATH=C:\\spark\\jars\\mssql-jdbc-12.10.0.jre8.jar\n",
    "\n",
    "REM 7. Set the path to the directory where you want to save the batch script's logs\n",
    "SET LOG_DIR=%USERPROFILE%\\Downloads\\pipeline_logs\n",
    "REM ---------------------------------\n",
    "\n",
    "REM --- SCRIPT EXECUTION ---\n",
    "\n",
    "REM Automatically create the log directory if it does not exist\n",
    "if not exist \"%LOG_DIR%\" mkdir \"%LOG_DIR%\"\n",
    "\n",
    "REM Change the current directory to the script's directory\n",
    "cd /d \"%SCRIPT_DIR%\"\n",
    "\n",
    "echo =================================================================\n",
    "echo Starting Promotion Pipeline via spark-submit at %date% %time%\n",
    "echo.\n",
    "echo Using Spark from: \"%SPARK_HOME%\"\n",
    "echo Using Python executable: \"%PYTHON_EXECUTABLE%\"\n",
    "echo.\n",
    "echo =================================================================\n",
    "\n",
    "REM ### FINAL FIX - Use CALL to ensure control returns to this script ###\n",
    "CALL \"%SPARK_HOME%\\bin\\spark-submit.cmd\" ^\n",
    "    --master local[*] ^\n",
    "    --driver-memory 1g ^\n",
    "    --conf spark.pyspark.python=\"%PYTHON_EXECUTABLE%\" ^\n",
    "    --driver-class-path \"%JDBC_DRIVER_PATH%\" ^\n",
    "    \"%PYTHON_SCRIPT%\" >> \"%LOG_DIR%\\pipeline_run_log.txt\" 2>&1\n",
    "\n",
    "echo.\n",
    "echo Pipeline execution finished at %date% %time%\n",
    "echo See the log file in: %LOG_DIR%\n",
    "echo =================================================================\n",
    "echo.\n",
    "\n",
    "REM This command will now be reached\n",
    "PAUSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
