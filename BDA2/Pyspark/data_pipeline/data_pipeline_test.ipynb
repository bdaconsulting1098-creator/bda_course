{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af18e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# PySpark Promotion Pipeline (completely flat: no def, no env)\n",
    "\n",
    "import os, sys, logging, smtplib\n",
    "from datetime import datetime\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.application import MIMEApplication\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, count, when, trim, upper, to_date, lit\n",
    "import pandas as pd\n",
    "\n",
    "# ===================== LOGGING =====================\n",
    "APP_NAME = \"PromotionPipeline\"\n",
    "LOG_DIR = \"logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "log_filename = datetime.now().strftime(os.path.join(LOG_DIR, \"promotion_pipeline_%Y%m%d_%H%M%S.log\"))\n",
    "\n",
    "for h in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(h)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    handlers=[logging.FileHandler(log_filename,encoding=\"utf-8\"), logging.StreamHandler(sys.stdout)],\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(APP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fabae4-3ced-4c99-828f-67d602efcfdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#export excel to csv        \n",
    "import os\n",
    "\n",
    "xls_path = os.path.join(os.environ[\"USERPROFILE\"], \"Documents\", \"BDA2\", \"data\", \"Promotion_data.xlsx\")\n",
    "out_dir  = os.path.join(os.environ[\"USERPROFILE\"], \"Documents\", \"BDA2\", \"data\", \"csv\")\n",
    "\n",
    "print(xls_path)\n",
    "print(out_dir)\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "base_name = os.path.splitext(os.path.basename(xls_path))[0]\n",
    "xls = pd.ExcelFile(xls_path)\n",
    "for sheet in xls.sheet_names:\n",
    "    df = pd.read_excel(xls, sheet_name=sheet)\n",
    "    # save with Excel filename (Promotion_data.csv), not sheet name\n",
    "    out_file = os.path.join(out_dir, f\"{base_name}.csv\")\n",
    "    df.to_csv(out_file, index=False)\n",
    "    print(f\"Saved {out_file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee87bb-8375-4095-bb95-a1c02a4e1254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xls_path = os.path.join(os.environ[\"USERPROFILE\"], \"Documents\", \"BDA2\", \"data\", \"Promotion_new_data.xlsx\")\n",
    "out_dir  = os.path.join(os.environ[\"USERPROFILE\"], \"Documents\", \"BDA2\", \"data\", \"csv\")\n",
    "\n",
    "print(xls_path)\n",
    "print(out_dir)\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "base_name = os.path.splitext(os.path.basename(xls_path))[0]\n",
    "xls = pd.ExcelFile(xls_path)\n",
    "for sheet in xls.sheet_names:\n",
    "    df = pd.read_excel(xls, sheet_name=sheet)\n",
    "    # save with Excel filename (Promotion_data.csv), not sheet name\n",
    "    out_file = os.path.join(out_dir, f\"{base_name}.csv\")\n",
    "    df.to_csv(out_file, index=False)\n",
    "    print(f\"Saved {out_file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ac9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CREATE SPARK SESSION =====================\n",
    "# Build or get an existing Spark session. \n",
    "# - .builder: starts a new session builder\n",
    "# - .appName(APP_NAME): names the Spark application (shows in Spark UI/logs)\n",
    "# - .getOrCreate(): reuses an existing session if one is active, otherwise creates a new one\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(APP_NAME)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Log confirmation that Spark has started successfully\n",
    "logger.info(\"Spark session created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88006e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== LOAD CSV: OLD (each CSV -> separate DataFrame) =====================\n",
    "csv_path = os.path.join(os.environ[\"USERPROFILE\"], \"Documents\", \"BDA2\", \"data\", \"csv\", \"Promotion_data.csv\")\n",
    "\n",
    "print(csv_path)\n",
    "\n",
    "\n",
    "logger.info(f\"[OLD] Starting to load CSV: {csv_path}\")\n",
    "\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")       # first row as header\n",
    "    .option(\"inferSchema\", \"true\")  # try to detect column types\n",
    "    .load(csv_path)\n",
    ")\n",
    "\n",
    "row_count = df.count()\n",
    "col_count = len(df.columns)\n",
    "\n",
    "logger.info(f\"[OLD] Loaded CSV {csv_path} -> rows={row_count}, cols={col_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== LOAD CSV: NEW (each CSV -> separate DataFrame) =====================\n",
    "csv_path_new = os.path.join(os.environ[\"USERPROFILE\"], \"Documents\", \"BDA2\", \"data\", \"csv\", \"Promotion_new_data.csv\")\n",
    "\n",
    "logger.info(f\"[NEW] Starting to load CSV: {csv_path_new}\")\n",
    "\n",
    "df_new = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")       # first row as header\n",
    "    .option(\"inferSchema\", \"true\")  # try to detect column types\n",
    "    .load(csv_path_new)\n",
    ")\n",
    "\n",
    "row_count_new = df_new.count()\n",
    "col_count_new = len(df_new.columns)\n",
    "\n",
    "logger.info(f\"[NEW] Loaded CSV {csv_path_new} -> rows={row_count_new}, cols={col_count_new}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f461f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc6ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================DATA  VALIDATION =====================\n",
    "from pyspark.sql.functions import col, isnan, count, lit\n",
    "from pyspark.sql.functions import abs as spark_abs\n",
    "logger.info(\"Starting data validation for Promotion_data.csv (Spark DataFrame)\")\n",
    "\n",
    "# 1. Null counts per column\n",
    "null_counts = df.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "for c, n in null_counts.items():\n",
    "    if n > 0:\n",
    "        logger.warning(f\"[NULL] Column '{c}' has {n} nulls\")\n",
    "    else:\n",
    "        logger.info(f\"[NULL] Column '{c}' has no nulls\")\n",
    "\n",
    "# 2. Negative values check for numeric columns\n",
    "numeric_cols = [\"Price\", \"Discount\", \"Units\", \"Sales $\", \"Gross Margin $\", \"# Transactions that contained the product\"]\n",
    "for colname in numeric_cols:\n",
    "    negatives = df.filter(col(colname) < 0).count()\n",
    "    if negatives > 0:\n",
    "        logger.error(f\"[NEGATIVE] Column '{colname}' has {negatives} negative values\")\n",
    "    else:\n",
    "        logger.info(f\"[NEGATIVE] Column '{colname}' all values non-negative\")\n",
    "\n",
    "# 3. Discount between 0 and 1\n",
    "invalid_discounts = df.filter((col(\"Discount\") < 0) | (col(\"Discount\") > 1)).count()\n",
    "if invalid_discounts > 0:\n",
    "    logger.error(f\"[DISCOUNT] Found {invalid_discounts} invalid discount values\")\n",
    "else:\n",
    "    logger.info(\"[DISCOUNT] All discount values between 0 and 1\")\n",
    "\n",
    "# 4. On Flyer? only Yes/No\n",
    "invalid_on_flyer = df.filter(~col(\"On Flyer?\").isin(\"Yes\", \"No\")).count()\n",
    "if invalid_on_flyer > 0:\n",
    "    logger.error(f\"[ON FLYER] Found {invalid_on_flyer} invalid values in 'On Flyer?' column\")\n",
    "else:\n",
    "    logger.info(\"[ON FLYER] All values are Yes/No\")\n",
    "\n",
    "# 5. Year range check\n",
    "invalid_years = df.filter((col(\"Year\") < 2000) | (col(\"Year\") > 2030)).count()\n",
    "if invalid_years > 0:\n",
    "    logger.error(f\"[YEAR] Found {invalid_years} invalid year values\")\n",
    "else:\n",
    "    logger.info(\"[YEAR] All years are within 2000‚Äì2030\")\n",
    "\n",
    "# 6. Week number between 1 and 53\n",
    "invalid_weeks = df.filter((col(\"week number\") < 1) | (col(\"week number\") > 53)).count()\n",
    "if invalid_weeks > 0:\n",
    "    logger.error(f\"[WEEK] Found {invalid_weeks} invalid week numbers\")\n",
    "else:\n",
    "    logger.info(\"[WEEK] All week numbers between 1‚Äì53\")\n",
    "\n",
    "\n",
    "# 7. Sales consistency check: Sales $ ‚âà Units * Price * (1 - Discount)\n",
    "expected_sales = (col(\"Units\") * col(\"Price\") * (lit(1) - col(\"Discount\")))\n",
    "sales_mismatch = df.filter(spark_abs(col(\"Sales $\") - expected_sales) > 1e-2).count()\n",
    "if sales_mismatch > 0:\n",
    "    logger.warning(f\"[SALES] {sales_mismatch} rows where Sales $ != Units*Price*(1-Discount)\")\n",
    "else:\n",
    "    logger.info(\"[SALES] All rows pass sales consistency check\")\n",
    "\n",
    "logger.info(\"Data validation finished for Promotion_data.csv (Spark DataFrame)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================DATA  TRANSFORM OPTION 1 USING SPARK =====================\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, upper, round, when,\n",
    "    to_date, concat_ws, lit\n",
    ")\n",
    "\n",
    "logger.info(\"Starting data transformations on Promotion_data.csv\")\n",
    "\n",
    "# 1. Clean column names (replace spaces with underscores)\n",
    "df_clean = df.toDF(*[c.strip().replace(\" \", \"_\") for c in df.columns])\n",
    "logger.info(\"Step 1: Column names cleaned (spaces replaced with underscores)\")\n",
    "\n",
    "# 2. Standardize text columns\n",
    "df_clean = df_clean.withColumn(\"Product\", trim(upper(col(\"Product\"))))\n",
    "df_clean = df_clean.withColumn(\"On_Flyer\", when(col(\"On_Flyer?\") == \"Yes\", lit(1)).otherwise(lit(0)))\n",
    "logger.info(\"Step 2: Standardized text columns (Product uppercased, On_Flyer flag set)\")\n",
    "\n",
    "# 3. Create proper date from Year + week number\n",
    "df_clean = df_clean.withColumn(\"Year\", col(\"Year\").cast(\"int\"))\n",
    "df_clean = df_clean.withColumn(\"week_number\", col(\"week_number\").cast(\"int\"))\n",
    "df_clean = df_clean.withColumn(\"Week_Start_Date\", to_date(concat_ws(\"-W\", col(\"Year\"), col(\"week_number\")), \"yyyy-ww\"))\n",
    "logger.info(\"Step 3: Derived Week_Start_Date from Year and week_number\")\n",
    "\n",
    "# 4. Calculate normalized discount percentage\n",
    "df_clean = df_clean.withColumn(\"Discount_Percent\", round(col(\"Discount\") * 100, 2))\n",
    "logger.info(\"Step 4: Discount_Percent column created\")\n",
    "\n",
    "# 5. Calculate unit price after discount\n",
    "df_clean = df_clean.withColumn(\"Final_Unit_Price\", round(col(\"Price\") * (1 - col(\"Discount\")), 2))\n",
    "logger.info(\"Step 5: Final_Unit_Price column created\")\n",
    "\n",
    "# 6. Calculate Gross Margin % relative to Sales\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"Gross_Margin_Percent\",\n",
    "    round((col(\"Gross_Margin_$\") / col(\"Sales_$\")) * 100, 2)\n",
    ")\n",
    "logger.info(\"Step 6: Gross_Margin_Percent column created\")\n",
    "\n",
    "# 7. Flag transactions with unusually high discounts (>50%)\n",
    "df_clean = df_clean.withColumn(\"High_Discount_Flag\", when(col(\"Discount\") > 0.5, lit(1)).otherwise(lit(0)))\n",
    "logger.info(\"Step 7: High_Discount_Flag column created\")\n",
    "\n",
    "# 8. Categorize sales volume\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"Sales_Category\",\n",
    "    when(col(\"Units\") >= 100, lit(\"High\"))\n",
    "    .when(col(\"Units\") >= 50, lit(\"Medium\"))\n",
    "    .otherwise(lit(\"Low\"))\n",
    ")\n",
    "logger.info(\"Step 8: Sales_Category column created (Low/Medium/High)\")\n",
    "\n",
    "logger.info(\"All data transformations completed successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================DATA  TRANSFORM OPTION 2 USING SPARK SQL=====================\n",
    "# ==================== REGISTER TEMP VIEW ====================\n",
    "# Clean column names (replace spaces, remove special chars)\n",
    "df_sql = df.toDF(*[c.strip().replace(\" \", \"_\").replace(\"?\", \"\") for c in df.columns])\n",
    "df_sql.createOrReplaceTempView(\"promotion_data\")\n",
    "logger.info(\"Temporary SQL view 'promotion_data' created\")\n",
    "\n",
    "# ==================== TRANSFORMATION WITH SPARK SQL ====================\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    Year,\n",
    "    week_number,\n",
    "    UPPER(TRIM(Product)) AS Product,               -- normalize product names\n",
    "    Price,\n",
    "    Discount,\n",
    "    ROUND(Discount * 100, 2) AS Discount_Percent,  -- discount in %\n",
    "    ROUND(Price * (1 - Discount), 2) AS Final_Unit_Price,\n",
    "    Units,\n",
    "    `Sales_$`,\n",
    "    `Gross_Margin_$`,\n",
    "    ROUND((`Gross_Margin_$` / NULLIF(`Sales_$`,0)) * 100, 2) AS Gross_Margin_Percent,\n",
    "    CASE WHEN On_Flyer = 'Yes' THEN 1 ELSE 0 END AS On_Flyer_Flag,\n",
    "    CASE WHEN Discount > 0.5 THEN 1 ELSE 0 END AS High_Discount_Flag,\n",
    "    CASE \n",
    "        WHEN Units >= 100 THEN 'High'\n",
    "        WHEN Units >= 50 THEN 'Medium'\n",
    "        ELSE 'Low'\n",
    "    END AS Sales_Category\n",
    "FROM promotion_data\n",
    "\"\"\"\n",
    "\n",
    "df_transformed = spark.sql(query)\n",
    "logger.info(\"Transformation query executed successfully\")\n",
    "\n",
    "# ==================== SHOW RESULTS ====================\n",
    "df_transformed.show(20, truncate=False)\n",
    "logger.info(\"Sample transformed rows displayed\")\n",
    "\n",
    "# =====================DATA  TRANSFORM OPTION 2 USING SPARK SQL=====================\n",
    "# ==================== REGISTER TEMP VIEW ====================\n",
    "# Clean column names (replace spaces, remove special chars)\n",
    "df_sql = df_new.toDF(*[c.strip().replace(\" \", \"_\").replace(\"?\", \"\") for c in df.columns])\n",
    "df_sql.createOrReplaceTempView(\"promotion_data_new\")\n",
    "logger.info(\"Temporary SQL view 'promotion_data_new' created\")\n",
    "\n",
    "# ==================== TRANSFORMATION WITH SPARK SQL ====================\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    Year,\n",
    "    week_number,\n",
    "    UPPER(TRIM(Product)) AS Product,               -- normalize product names\n",
    "    Price,\n",
    "    Discount,\n",
    "    ROUND(Discount * 100, 2) AS Discount_Percent,  -- discount in %\n",
    "    ROUND(Price * (1 - Discount), 2) AS Final_Unit_Price,\n",
    "    Units,\n",
    "    `Sales_$`,\n",
    "    `Gross_Margin_$`,\n",
    "    ROUND((`Gross_Margin_$` / NULLIF(`Sales_$`,0)) * 100, 2) AS Gross_Margin_Percent,\n",
    "    CASE WHEN On_Flyer = 'Yes' THEN 1 ELSE 0 END AS On_Flyer_Flag,\n",
    "    CASE WHEN Discount > 0.5 THEN 1 ELSE 0 END AS High_Discount_Flag,\n",
    "    CASE \n",
    "        WHEN Units >= 100 THEN 'High'\n",
    "        WHEN Units >= 50 THEN 'Medium'\n",
    "        ELSE 'Low'\n",
    "    END AS Sales_Category\n",
    "FROM promotion_data_new\n",
    "\"\"\"\n",
    "\n",
    "df_transformed_new = spark.sql(query)\n",
    "logger.info(\"Transformation query executed successfully\")\n",
    "\n",
    "# ==================== SHOW RESULTS ====================\n",
    "df_transformed_new.show(20, truncate=False)\n",
    "logger.info(\"Sample transformed rows displayed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b97ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==================== APPEND OLD + NEW ====================\n",
    "df_combined = df_transformed.unionByName(df_transformed_new)\n",
    "logger.info(\"Combined DataFrame created by appending old and new data\")\n",
    "\n",
    "# ==================== SHOW RESULTS ====================\n",
    "df_combined.show(20, truncate=False)\n",
    "logger.info(\"Sample combined rows displayed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd90a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== JDBC WRITE =====================\n",
    "JDBC_URL    = \"jdbc:sqlserver://localhost:1433;databaseName=datahub;encrypt=true;trustServerCertificate=true\"\n",
    "JDBC_USER   = \"sa\"\n",
    "JDBC_PWD    = \"user1\"\n",
    "JDBC_DRIVER = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "JDBC_TABLE  = \"dbo.PromotionTable\"\n",
    "JDBC_MODE   = \"overwrite\"\n",
    "\n",
    "props = {\"user\": JDBC_USER, \"password\": JDBC_PWD, \"driver\": JDBC_DRIVER}\n",
    "try:\n",
    "    df_combined.write.jdbc(url=JDBC_URL, table=JDBC_TABLE, mode=JDBC_MODE, properties=props)\n",
    "    logger.info(f\"Wrote {row_count} rows to {JDBC_TABLE}\")\n",
    "    write_ok = True\n",
    "except Exception as e:\n",
    "    logger.error(f\"JDBC write failed: {e}\", exc_info=True)\n",
    "    write_ok = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b298cc4",
   "metadata": {},
   "source": [
    "üõ† Step-by-step: Enable 2FA + Create App Password\n",
    "\n",
    "Here‚Äôs what you do:\n",
    "\n",
    "Enable 2-Step Verification (if you haven‚Äôt already)\n",
    "\n",
    "Go to your Google Account > Security\n",
    "\n",
    "Under ‚ÄúHow you sign in to Google,‚Äù find 2-Step Verification and turn it on\n",
    "\n",
    "Follow the prompts (enter phone number, etc.)\n",
    "\n",
    "After this, your account requires a second factor to login. \n",
    "CCTV Camera World\n",
    "+2\n",
    "Saleshandy\n",
    "+2\n",
    "\n",
    "Go to App Passwords page\n",
    "\n",
    "In Google Account > Security > ‚ÄúApp passwords‚Äù\n",
    "\n",
    "Or navigate directly: https://myaccount.google.com/apppasswords\n",
    "\n",
    "You may be asked to sign in again for security. \n",
    "Reolink Support\n",
    "+2\n",
    "Google Help\n",
    "+2\n",
    "\n",
    "Generate a new App Password\n",
    "\n",
    "Under ‚ÄúSelect app,‚Äù choose ‚ÄúOther (Custom name)‚Äù\n",
    "\n",
    "Give it a label (e.g. PromotionPipelineEmail)\n",
    "\n",
    "Click Generate\n",
    "\n",
    "Google shows a 16-character password (no spaces). Copy it. \n",
    "Reolink Support\n",
    "+2\n",
    "Saleshandy\n",
    "+2\n",
    "\n",
    "Use the App Password in your SMTP config\n",
    "\n",
    "In your Python / email-sending code, replace your normal Gmail password with this App Password\n",
    "\n",
    "Example: MAIL_PWD = \"abcd wxyz pqrs uv12\" (without spaces)\n",
    "\n",
    "Test sending email\n",
    "\n",
    "Try sending a simple test email using SMTP with TLS\n",
    "\n",
    "If it works, you're good. If it fails: check port, host, firewall, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbfe358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== EMAIL WITH LOG ATTACHED =====================\n",
    "import smtplib, os, logging\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.application import MIMEApplication\n",
    "\n",
    "# --- Config (edit these) ---\n",
    "MAIL_HOST = \"smtp.gmail.com\"         # Gmail: smtp.gmail.com | Office 365: smtp.office365.com\n",
    "MAIL_PORT = 587\n",
    "MAIL_USER = \"abc@gmail.com\"        # sender address (must be the auth user on many providers)\n",
    "MAIL_PWD  = \"kfah osuo oxzp aide\"    # app password / SMTP password (not your normal login)\n",
    "MAIL_TO   = \"abc@gmail.com\"        # comma-separated for multiple: \"a@x.com,b@y.com\"\n",
    "\n",
    "SUBJECT   = \"Promotion Pipeline Status\"\n",
    "BODY_TEXT = \"The pipeline completed. See attached log.\"\n",
    "\n",
    "# --- Log file to attach (uses your earlier variable 'log_filename') ---\n",
    "# If you used a different variable name, set it here:\n",
    "# log_filename = r\"C:\\path\\to\\promotion_pipeline_YYYYMMDD_HHMMSS.log\"\n",
    "\n",
    "logger.info(\"Preparing email with log attachment\")\n",
    "\n",
    "if not (MAIL_USER and MAIL_PWD and MAIL_TO):\n",
    "    logger.warning(\"Email config missing (MAIL_USER/MAIL_PWD/MAIL_TO). Skipping email.\")\n",
    "else:\n",
    "    msg = MIMEMultipart()\n",
    "    msg[\"Subject\"] = SUBJECT\n",
    "    msg[\"From\"] = MAIL_USER\n",
    "    msg[\"To\"] = MAIL_TO\n",
    "\n",
    "    # body\n",
    "    msg.attach(MIMEText(BODY_TEXT, \"plain\", _charset=\"utf-8\"))\n",
    "\n",
    "    # attach log if present\n",
    "    if os.path.exists(log_filename):\n",
    "        try:\n",
    "            with open(log_filename, \"rb\") as f:\n",
    "                part = MIMEApplication(f.read(), Name=os.path.basename(log_filename))\n",
    "                part.add_header(\"Content-Disposition\", f'attachment; filename=\"{os.path.basename(log_filename)}\"')\n",
    "                msg.attach(part)\n",
    "            logger.info(f\"Attached log file: {log_filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to attach log file: {e}\", exc_info=True)\n",
    "    else:\n",
    "        logger.warning(f\"Log file not found, skipping attachment: {log_filename}\")\n",
    "\n",
    "    # send\n",
    "    try:\n",
    "        with smtplib.SMTP(MAIL_HOST, MAIL_PORT) as server:\n",
    "            server.starttls()\n",
    "            server.login(MAIL_USER, MAIL_PWD)\n",
    "            server.send_message(msg)\n",
    "        logger.info(\"Email sent successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to send email: {e}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22685d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
