{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c10ffe-f241-4562-ad19-177c2eb18dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# ---------------------------------------------\n",
    "# CONFIG DEFAULTS\n",
    "# ---------------------------------------------\n",
    "# RANGE MODE DEFAULTS (if last_n_days == 0)\n",
    "DEFAULT_START_DATE = \"2025-03-01\"  # yyyy-MM-dd\n",
    "DEFAULT_END_DATE   = \"2025-10-31\"  # yyyy-MM-dd\n",
    "\n",
    "# LAST-N-DAYS MODE\n",
    "# If LAST_N_DAYS > 0, we ignore the explicit start/end and use:\n",
    "#   [today - LAST_N_DAYS + 1  ...  today]\n",
    "DEFAULT_LAST_N_DAYS = 0  # 0 = disabled\n",
    "\n",
    "COUNTRY = \"canada\"\n",
    "STATE = \"on\"\n",
    "CITY = \"toronto\"\n",
    "\n",
    "# Where to store snapshots (Unity Catalog Volume)\n",
    "# Final layout:\n",
    "#   /Volumes/workspace/default/course_data/airbnb_toronto/<snapshot_date>/*.csv\n",
    "BASE_ROOT = \"/Volumes/workspace/default/course_data/airbnb_toronto\"\n",
    "\n",
    "REQUIRED_FILES = [\"listings.csv\", \"calendar.csv\", \"reviews.csv\", \"neighbourhoods.csv\"]\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Resolve parameters from widgets (if available)\n",
    "# ---------------------------------------------\n",
    "start_date_str = DEFAULT_START_DATE\n",
    "end_date_str = DEFAULT_END_DATE\n",
    "last_n_days = DEFAULT_LAST_N_DAYS\n",
    "\n",
    "# In Databricks notebook you *can* add:\n",
    "# dbutils.widgets.text(\"start_date\", DEFAULT_START_DATE, \"Start date (yyyy-MM-dd)\")\n",
    "# dbutils.widgets.text(\"end_date\",   DEFAULT_END_DATE,   \"End date (yyyy-MM-dd)\")\n",
    "# dbutils.widgets.text(\"last_n_days\", str(DEFAULT_LAST_N_DAYS), \"Last N days (0 = off)\")\n",
    "try:\n",
    "    sd = dbutils.widgets.get(\"start_date\")   # type: ignore[name-defined]\n",
    "    ed = dbutils.widgets.get(\"end_date\")     # type: ignore[name-defined]\n",
    "    ln = dbutils.widgets.get(\"last_n_days\")  # type: ignore[name-defined]\n",
    "\n",
    "    if sd:\n",
    "        start_date_str = sd.strip()\n",
    "    if ed:\n",
    "        end_date_str = ed.strip()\n",
    "    if ln:\n",
    "        try:\n",
    "            last_n_days = int(ln)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid last_n_days '{ln}', falling back to {DEFAULT_LAST_N_DAYS}\")\n",
    "            last_n_days = DEFAULT_LAST_N_DAYS\n",
    "except Exception:\n",
    "    # Not in Databricks or widgets not defined → use defaults\n",
    "    pass\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Build list of snapshot_dates to try\n",
    "# ---------------------------------------------\n",
    "snapshot_dates = []\n",
    "today = date.today()\n",
    "\n",
    "if last_n_days and last_n_days > 0:\n",
    "    # Use last N days up to today\n",
    "    print(f\"Using last_n_days mode: last {last_n_days} days (including today)\")\n",
    "\n",
    "    start = today - timedelta(days=last_n_days - 1)\n",
    "    end = today\n",
    "else:\n",
    "    # Use explicit start/end range\n",
    "    print(f\"Using explicit range mode: {start_date_str} -> {end_date_str}\")\n",
    "    start = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "    end = datetime.strptime(end_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "    if end < start:\n",
    "        raise ValueError(\"END_DATE must be >= START_DATE\")\n",
    "\n",
    "current = start\n",
    "while current <= end:\n",
    "    snapshot_dates.append(current.strftime(\"%Y-%m-%d\"))\n",
    "    current += timedelta(days=1)\n",
    "\n",
    "if not snapshot_dates:\n",
    "    raise ValueError(\"No snapshot dates resolved from inputs.\")\n",
    "\n",
    "print(f\"Snapshot dates to attempt: {snapshot_dates}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Helper functions\n",
    "# ---------------------------------------------\n",
    "def download(url: str, path: str):\n",
    "    print(f\"  Downloading {url} -> {path}\")\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    # Treat 403 and 404 as \"no snapshot for this date\"\n",
    "    if resp.status_code in (403, 404):\n",
    "        raise FileNotFoundError(f\"URL not accessible ({resp.status_code}): {url}\")\n",
    "\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(resp.content)\n",
    "\n",
    "def download_and_unzip_gz(url: str, out_csv_path: str):\n",
    "    gz_path = out_csv_path + \".gz\"\n",
    "    download(url, gz_path)\n",
    "    print(f\"  Unzipping {gz_path} -> {out_csv_path}\")\n",
    "    with gzip.open(gz_path, \"rb\") as f_in, open(out_csv_path, \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    os.remove(gz_path)\n",
    "\n",
    "def folder_has_all_files(folder: str) -> bool:\n",
    "    \"\"\"Return True if folder already has all expected CSVs.\"\"\"\n",
    "    for fname in REQUIRED_FILES:\n",
    "        if not os.path.exists(os.path.join(folder, fname)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Ensure base root exists\n",
    "# ---------------------------------------------\n",
    "os.makedirs(BASE_ROOT, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Main loop: download for each snapshot date\n",
    "# ---------------------------------------------\n",
    "for snapshot_date in snapshot_dates:\n",
    "    print(f\"\\n=== Processing snapshot date: {snapshot_date} ===\")\n",
    "\n",
    "    target_dir = os.path.join(BASE_ROOT, snapshot_date)\n",
    "\n",
    "    # If this snapshot already fully exists, skip downloading\n",
    "    if os.path.isdir(target_dir) and folder_has_all_files(target_dir):\n",
    "        print(f\"  Snapshot {snapshot_date} already fully downloaded. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    base_url = f\"https://data.insideairbnb.com/{COUNTRY}/{STATE}/{CITY}/{snapshot_date}\"\n",
    "\n",
    "    urls = {\n",
    "        \"listings\":       f\"{base_url}/data/listings.csv.gz\",\n",
    "        \"calendar\":       f\"{base_url}/data/calendar.csv.gz\",\n",
    "        \"reviews\":        f\"{base_url}/data/reviews.csv.gz\",\n",
    "        \"neighbourhoods\": f\"{base_url}/visualisations/neighbourhoods.csv\",  # not gzipped\n",
    "    }\n",
    "\n",
    "    # (Re)create target_dir if needed (we'll wipe it on failure)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    print(f\"  Saving files under: {target_dir}\")\n",
    "\n",
    "    success = False  # track if this date actually completed downloads\n",
    "\n",
    "    try:\n",
    "        # Listings, calendar, reviews (gzipped)\n",
    "        download_and_unzip_gz(urls[\"listings\"],  os.path.join(target_dir, \"listings.csv\"))\n",
    "        download_and_unzip_gz(urls[\"calendar\"],  os.path.join(target_dir, \"calendar.csv\"))\n",
    "        download_and_unzip_gz(urls[\"reviews\"],   os.path.join(target_dir, \"reviews.csv\"))\n",
    "\n",
    "        # Neighbourhoods (already CSV)\n",
    "        download(urls[\"neighbourhoods\"], os.path.join(target_dir, \"neighbourhoods.csv\"))\n",
    "\n",
    "        # Final sanity check\n",
    "        if not folder_has_all_files(target_dir):\n",
    "            raise RuntimeError(f\"Missing one or more files after download for {snapshot_date}\")\n",
    "\n",
    "        success = True\n",
    "        print(f\"✓ Finished snapshot {snapshot_date}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"! Snapshot {snapshot_date} appears not to exist / be accessible on InsideAirbnb: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"! Error while processing {snapshot_date}: {e}\")\n",
    "    finally:\n",
    "        # If we didn't successfully download everything, delete the folder\n",
    "        if not success:\n",
    "            print(f\"  Deleting folder with incomplete or no data: {target_dir}\")\n",
    "            try:\n",
    "                shutil.rmtree(target_dir, ignore_errors=True)\n",
    "            except Exception as cleanup_err:\n",
    "                print(f\"  Warning: failed to remove {target_dir}: {cleanup_err}\")\n",
    "\n",
    "print(\"\\nAll requested snapshots processed.\")\n",
    "print(\"Base folder pattern for snapshots with data:\")\n",
    "print(\"  /Volumes/workspace/default/course_data/airbnb_toronto/<snapshot_date>/\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6003115410886125,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "00_download_toronto",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
