{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdfea837-8612-44e9-8b54-6329f5c42770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, DateType, TimestampType\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Spark & schema\n",
    "# ---------------------------------------------\n",
    "\n",
    "spark.sql(\"CREATE schema IF NOT EXISTS airbnb_project\")\n",
    "spark.catalog.setCurrentDatabase(\"airbnb_project\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Config: where 00 put the files\n",
    "# ---------------------------------------------\n",
    "BASE_ROOT = \"/Volumes/workspace/default/course_data/airbnb_toronto\"\n",
    "REQUIRED_FILES = [\"listings.csv\", \"calendar.csv\", \"reviews.csv\", \"neighbourhoods.csv\"]\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Explicit schemas for all tables (fixes merge error)\n",
    "# ---------------------------------------------\n",
    "listings_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"listing_url\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"host_id\", IntegerType(), True),\n",
    "    StructField(\"host_name\", StringType(), True),\n",
    "    StructField(\"neighbourhood_cleansed\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"room_type\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    # ... add all other columns as needed ...\n",
    "])\n",
    "\n",
    "calendar_schema = StructType([\n",
    "    StructField(\"listing_id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"available\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    # ... add all other columns as needed ...\n",
    "])\n",
    "\n",
    "reviews_schema = StructType([\n",
    "    StructField(\"listing_id\", IntegerType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"reviewer_id\", IntegerType(), True),\n",
    "    StructField(\"reviewer_name\", StringType(), True),\n",
    "    StructField(\"comments\", StringType(), True),\n",
    "    # ... add all other columns as needed ...\n",
    "])\n",
    "\n",
    "neighbourhoods_schema = StructType([\n",
    "    StructField(\"neighbourhood\", StringType(), True),\n",
    "    StructField(\"neighbourhood_group\", StringType(), True),\n",
    "    # ... add all other columns as needed ...\n",
    "])\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Helper: check if folder has all required CSVs\n",
    "# ---------------------------------------------\n",
    "def folder_has_all_files(folder: str) -> bool:\n",
    "    for fname in REQUIRED_FILES:\n",
    "        if not os.path.exists(os.path.join(folder, fname)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Discover available snapshot folders\n",
    "# ---------------------------------------------\n",
    "if not os.path.exists(BASE_ROOT):\n",
    "    raise RuntimeError(f\"Base root does not exist: {BASE_ROOT}\")\n",
    "\n",
    "all_entries = [\n",
    "    name for name in os.listdir(BASE_ROOT)\n",
    "    if os.path.isdir(os.path.join(BASE_ROOT, name))\n",
    "]\n",
    "\n",
    "# Only keep folders that look like yyyy-MM-dd AND have all files\n",
    "snapshot_dates = []\n",
    "for name in sorted(all_entries):\n",
    "    full_dir = os.path.join(BASE_ROOT, name)\n",
    "    if not folder_has_all_files(full_dir):\n",
    "        print(f\"Skipping folder {full_dir} (missing required files).\")\n",
    "        continue\n",
    "    snapshot_dates.append(name)\n",
    "\n",
    "if not snapshot_dates:\n",
    "    raise RuntimeError(f\"No valid snapshot folders found under {BASE_ROOT}\")\n",
    "\n",
    "print(f\"Discovered snapshot_dates with full data: {snapshot_dates}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Helper: append rows for a given snapshot_date if not already loaded\n",
    "# ---------------------------------------------\n",
    "def append_if_new_snapshot(table_name: str, df, snapshot_date_str: str):\n",
    "    snap_date_lit = F.to_date(F.lit(snapshot_date_str))\n",
    "    try:\n",
    "        existing = spark.table(table_name)\n",
    "        already = (\n",
    "            existing\n",
    "            .filter(F.col(\"snapshot_date\") == snap_date_lit)\n",
    "            .limit(1)\n",
    "            .count()\n",
    "        )\n",
    "        if already > 0:\n",
    "            print(f\"{table_name}: snapshot_date {snapshot_date_str} already loaded, skipping.\")\n",
    "            return\n",
    "\n",
    "        (\n",
    "            df.write\n",
    "            .format(\"delta\")\n",
    "            .option(\"mergeSchema\", \"true\")  # Allow schema evolution\n",
    "            .mode(\"append\")\n",
    "            .saveAsTable(table_name)\n",
    "        )\n",
    "        print(f\"{table_name}: appended snapshot_date {snapshot_date_str}.\")\n",
    "    except AnalysisException:\n",
    "        # Table does not exist yet → initial load\n",
    "        (\n",
    "            df.write\n",
    "            .format(\"delta\")\n",
    "            .option(\"mergeSchema\", \"true\")  # Allow schema evolution\n",
    "            .mode(\"overwrite\")\n",
    "            .saveAsTable(table_name)\n",
    "        )\n",
    "        print(f\"{table_name}: created and loaded snapshot_date {snapshot_date_str} (initial load).\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Main loop: process each snapshot_date\n",
    "# ---------------------------------------------\n",
    "for snapshot_date in snapshot_dates:\n",
    "    print(f\"\\n=== Processing snapshot_date: {snapshot_date} ===\")\n",
    "    base_path = os.path.join(BASE_ROOT, snapshot_date)\n",
    "\n",
    "    # Tag with snapshot_date column\n",
    "    snap_date_lit = F.to_date(F.lit(snapshot_date))\n",
    "\n",
    "    listings_df = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .option(\"escape\", \"\\\"\")\n",
    "        .option(\"quote\", \"\\\"\")\n",
    "        .schema(listings_schema)\n",
    "        .csv(os.path.join(base_path, \"listings.csv\"))\n",
    "        .withColumn(\"snapshot_date\", snap_date_lit)\n",
    "    )\n",
    "\n",
    "    calendar_df = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .schema(calendar_schema)\n",
    "        .csv(os.path.join(base_path, \"calendar.csv\"))\n",
    "        .withColumn(\"snapshot_date\", snap_date_lit)\n",
    "    )\n",
    "\n",
    "    reviews_df = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .schema(reviews_schema)\n",
    "        .csv(os.path.join(base_path, \"reviews.csv\"))\n",
    "        .withColumn(\"snapshot_date\", snap_date_lit)\n",
    "    )\n",
    "\n",
    "    neighbourhoods_df = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .schema(neighbourhoods_schema)\n",
    "        .csv(os.path.join(base_path, \"neighbourhoods.csv\"))\n",
    "        .withColumn(\"snapshot_date\", snap_date_lit)\n",
    "    )\n",
    "\n",
    "    append_if_new_snapshot(\"airbnb_project.airbnb_raw_listings\", listings_df, snapshot_date)\n",
    "    append_if_new_snapshot(\"airbnb_project.airbnb_raw_calendar\", calendar_df, snapshot_date)\n",
    "    append_if_new_snapshot(\"airbnb_project.airbnb_raw_reviews\", reviews_df, snapshot_date)\n",
    "    append_if_new_snapshot(\"airbnb_project.airbnb_raw_neighbourhoods\", neighbourhoods_df, snapshot_date)\n",
    "\n",
    "print(\"\\nBronze raw tables are up to date with all discovered snapshots.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01 – Bronze Incremental Loader (Raw Delta Tables)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
