{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0881ba-1d4b-4454-bba5-5332fd169b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import Window\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Spark & schema\n",
    "# ---------------------------------------------\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS airbnb_project\")\n",
    "spark.catalog.setCurrentDatabase(\"airbnb_project\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load Bronze raw tables\n",
    "# ---------------------------------------------\n",
    "raw_listings_df = spark.table(\"airbnb_project.airbnb_raw_listings\")\n",
    "raw_calendar_df = spark.table(\"airbnb_project.airbnb_raw_calendar\")\n",
    "raw_reviews_df = spark.table(\"airbnb_project.airbnb_raw_reviews\")\n",
    "raw_neigh_df = spark.table(\"airbnb_project.airbnb_raw_neighbourhoods\")\n",
    "\n",
    "# ============================================================\n",
    "# 1) stg_listings: incremental upsert (Type 1) on listing_id\n",
    "# ============================================================\n",
    "# Normalize / clean from Bronze\n",
    "window_latest = Window.partitionBy(\"id\").orderBy(F.col(\"snapshot_date\").desc())\n",
    "raw_listings_dedup = (\n",
    "    raw_listings_df\n",
    "    .withColumn(\"row_num\", F.row_number().over(window_latest))\n",
    "    .filter(F.col(\"row_num\") == 1)\n",
    ")\n",
    "stg_listings_current = (\n",
    "    raw_listings_dedup\n",
    "    .select(\n",
    "        F.col(\"id\").cast(\"bigint\").alias(\"listing_id\"),\n",
    "        F.col(\"host_id\").cast(\"bigint\").alias(\"host_id\"),\n",
    "        F.col(\"name\").alias(\"name\"),\n",
    "        F.col(\"host_name\").alias(\"host_name\"),\n",
    "        F.col(\"neighbourhood_cleansed\").alias(\"neighbourhood\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"neighbourhood_group\"),\n",
    "        F.col(\"latitude\").cast(\"double\").alias(\"latitude\"),\n",
    "        F.col(\"longitude\").cast(\"double\").alias(\"longitude\"),\n",
    "        F.col(\"room_type\").alias(\"room_type\"),\n",
    "        F.regexp_replace(F.col(\"price\").cast(\"string\"), \"[$,]\", \"\").cast(\"double\").alias(\"price\"),\n",
    "        F.col(\"snapshot_date\").cast(\"date\").alias(\"snapshot_date\")\n",
    "    )\n",
    "    .where(F.col(\"listing_id\").isNotNull())\n",
    ")\n",
    "\n",
    "stg_listings_target = \"airbnb_project.airbnb_stg_listings\"\n",
    "\n",
    "try:\n",
    "    # If target exists, MERGE (Type 1) on listing_id\n",
    "    spark.table(stg_listings_target)\n",
    "    delta_target = DeltaTable.forName(spark, stg_listings_target)\n",
    "\n",
    "    (\n",
    "        delta_target.alias(\"t\")\n",
    "        .merge(\n",
    "            stg_listings_current.alias(\"s\"),\n",
    "            \"t.listing_id = s.listing_id\"\n",
    "        )\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    print(\"Upserted airbnb_stg_listings incrementally (Type 1 on listing_id).\")\n",
    "\n",
    "except AnalysisException:\n",
    "    # First run – create the table\n",
    "    (\n",
    "        stg_listings_current\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(stg_listings_target)\n",
    "    )\n",
    "    print(\"Created airbnb_stg_listings (initial load).\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) stg_calendar: incremental append by date watermark\n",
    "# ============================================================\n",
    "stg_calendar_current = (\n",
    "    raw_calendar_df\n",
    "    .select(\n",
    "        F.col(\"listing_id\").cast(\"bigint\").alias(\"listing_id\"),\n",
    "        F.col(\"date\").cast(\"date\").alias(\"date\"),\n",
    "        F.when(F.col(\"available\") == \"t\", F.lit(1))\n",
    "         .when(F.col(\"available\") == \"f\", F.lit(0))\n",
    "         .otherwise(F.lit(None)).alias(\"is_available\"),\n",
    "        F.regexp_replace(F.col(\"price\").cast(\"string\"), \"[$,]\", \"\").cast(\"double\").alias(\"price\"),\n",
    "        F.col(\"snapshot_date\").cast(\"date\").alias(\"snapshot_date\")\n",
    "    )\n",
    "    .where(F.col(\"listing_id\").isNotNull() & F.col(\"date\").isNotNull())\n",
    ")\n",
    "\n",
    "stg_calendar_target = \"airbnb_project.airbnb_stg_calendar\"\n",
    "\n",
    "try:\n",
    "    existing_cal = spark.table(stg_calendar_target)\n",
    "    max_date = existing_cal.agg(F.max(\"date\")).collect()[0][0]\n",
    "\n",
    "    if max_date is None:\n",
    "        new_cal_rows = stg_calendar_current\n",
    "    else:\n",
    "        new_cal_rows = stg_calendar_current.filter(F.col(\"date\") > F.lit(max_date))\n",
    "\n",
    "    if new_cal_rows.count() == 0:\n",
    "        print(\"No new rows to append to airbnb_stg_calendar.\")\n",
    "    else:\n",
    "        (\n",
    "            new_cal_rows\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .saveAsTable(stg_calendar_target)\n",
    "        )\n",
    "        print(f\"Appended {new_cal_rows.count()} new rows to airbnb_stg_calendar after {max_date}.\")\n",
    "\n",
    "except AnalysisException:\n",
    "    (\n",
    "        stg_calendar_current\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(stg_calendar_target)\n",
    "    )\n",
    "    print(\"Created airbnb_stg_calendar (initial load).\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) stg_reviews: incremental append by review_date watermark\n",
    "# ============================================================\n",
    "stg_reviews_current = (\n",
    "    raw_reviews_df\n",
    "    .select(\n",
    "        F.col(\"listing_id\").cast(\"bigint\").alias(\"listing_id\"),\n",
    "        F.col(\"id\").cast(\"bigint\").alias(\"review_id\"),\n",
    "        F.col(\"date\").cast(\"date\").alias(\"review_date\"),\n",
    "        F.col(\"reviewer_id\").cast(\"bigint\").alias(\"reviewer_id\"),\n",
    "        F.col(\"reviewer_name\").cast(\"string\").alias(\"reviewer_name\"),\n",
    "        F.col(\"comments\").alias(\"string\").alias(\"comments\"),\n",
    "        F.col(\"snapshot_date\").cast(\"date\").alias(\"snapshot_date\")\n",
    "    )\n",
    "    .where(F.col(\"listing_id\").isNotNull() & F.col(\"review_id\").isNotNull())\n",
    ")\n",
    "\n",
    "stg_reviews_target = \"airbnb_project.airbnb_stg_reviews\"\n",
    "\n",
    "try:\n",
    "    existing_rev = spark.table(stg_reviews_target)\n",
    "    max_rdate = existing_rev.agg(F.max(\"review_date\")).collect()[0][0]\n",
    "\n",
    "    if max_rdate is None:\n",
    "        new_rev_rows = stg_reviews_current\n",
    "    else:\n",
    "        new_rev_rows = stg_reviews_current.filter(F.col(\"review_date\") > F.lit(max_rdate))\n",
    "\n",
    "    if new_rev_rows.count() == 0:\n",
    "        print(\"No new rows to append to airbnb_stg_reviews.\")\n",
    "    else:\n",
    "        (\n",
    "            new_rev_rows\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .saveAsTable(stg_reviews_target)\n",
    "        )\n",
    "        print(f\"Appended {new_rev_rows.count()} new rows to airbnb_stg_reviews after {max_rdate}.\")\n",
    "\n",
    "except AnalysisException:\n",
    "    (\n",
    "        stg_reviews_current\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(stg_reviews_target)\n",
    "    )\n",
    "    print(\"Created airbnb_stg_reviews (initial load).\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) stg_neighbourhoods: small dim – overwrite is fine\n",
    "# ============================================================\n",
    "stg_neigh_current = (\n",
    "    raw_neigh_df\n",
    "    .select(\n",
    "        F.col(\"neighbourhood\").alias(\"neighbourhood\"),\n",
    "        F.col(\"neighbourhood_group\").alias(\"neighbourhood_group\"),\n",
    "        F.col(\"snapshot_date\").cast(\"date\").alias(\"snapshot_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    stg_neigh_current\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"airbnb_project.airbnb_stg_neighbourhoods\")\n",
    ")\n",
    "\n",
    "print(\"Updated airbnb_stg_neighbourhoods (overwrite).\")\n",
    "print(\"Silver layer updated.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02 – Silver Transformations (Incremental MERGE _Watermark)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
