{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672c03ff-7923-4297-9793-6f70cd36133c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83E\uDDF9 正在执行终极环境清理...\n------------------------------------------\n1. 正在删除目标数据库 workspace.course_db...\n   ✅ 目标数据库已清除。\n2. 正在彻底删除 Volume 对象 workspace.default.course_data...\n   ✅ Volume 对象已彻底删除。\n3. 正在删除工作区文件夹 /Workspace/Users/bdaconsulting1098@gmail.com/Data_Course_101...\n   ✅ 工作区文件夹已清除。\n------------------------------------------\n\uD83C\uDF89 终极清理完成。Volume 已被彻底删除，准备重新创建。\n"
     ]
    }
   ],
   "source": [
    "# ================= CELL 0: 环境清理脚本 (终极重置) =================\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 定义配置 (必须和 CELL 1 & 2 保持一致)\n",
    "COURSE_NAME = \"Data_Course_101\"\n",
    "CATALOG_NAME = \"workspace\"\n",
    "SCHEMA_NAME = \"default\" # Volume 所在的 Schema\n",
    "VOLUME_NAME = \"course_data\"\n",
    "TARGET_CATALOG = \"workspace\"\n",
    "TARGET_SCHEMA = \"course_db\"\n",
    "\n",
    "# 动态获取当前用户身份\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "\n",
    "user_workspace_dir = f\"/Workspace/Users/{current_user}/{COURSE_NAME}\"\n",
    "\n",
    "print(\"\uD83E\uDDF9 正在执行终极环境清理...\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# 1. \uD83D\uDDD1️ 删除 Unity Catalog 表格和数据库\n",
    "print(f\"1. 正在删除目标数据库 {TARGET_CATALOG}.{TARGET_SCHEMA}...\")\n",
    "try:\n",
    "    spark.sql(f\"DROP SCHEMA IF EXISTS {TARGET_CATALOG}.{TARGET_SCHEMA} CASCADE\")\n",
    "    print(\"   ✅ 目标数据库已清除。\")\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️ 数据库清理失败 (可能是权限问题)。错误: {e}\")\n",
    "\n",
    "# 2. \uD83D\uDDD1️ 彻底删除 Volume 对象 (解决了底层残留问题)\n",
    "print(f\"2. 正在彻底删除 Volume 对象 {CATALOG_NAME}.{SCHEMA_NAME}.{VOLUME_NAME}...\")\n",
    "try:\n",
    "    # DROP VOLUME 彻底删除 Volume 及其所有底层数据\n",
    "    spark.sql(f\"DROP VOLUME IF EXISTS {CATALOG_NAME}.{SCHEMA_NAME}.{VOLUME_NAME}\")\n",
    "    print(\"   ✅ Volume 对象已彻底删除。\")\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️ Volume 删除失败 (可能是权限问题)。错误: {e}\")\n",
    "\n",
    "# 3. \uD83D\uDDD1️ 删除用户工作区文件夹\n",
    "print(f\"3. 正在删除工作区文件夹 {user_workspace_dir}...\")\n",
    "if os.path.exists(user_workspace_dir):\n",
    "    shutil.rmtree(user_workspace_dir, ignore_errors=True)\n",
    "    print(\"   ✅ 工作区文件夹已清除。\")\n",
    "else:\n",
    "    print(\"   ℹ️ 工作区文件夹不存在，跳过清理。\")\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "print(\"\uD83C\uDF89 终极清理完成。Volume 已被彻底删除，准备重新创建。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8814701485042684,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b798f6fb-3665-4b0f-bc81-d476af675caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDC4B 欢迎, bdaconsulting1098@gmail.com! 正在为您准备课程环境...\n\uD83D\uDEE0️  正在检查 Volume 环境...\n   ✅ Volume 已准备就绪: workspace.default.course_data\n⬇️  正在从 GitHub 克隆资料...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/Data_Course_101_temp'...\nUpdating files:  11% (190/1698)\rUpdating files:  12% (204/1698)\rUpdating files:  13% (221/1698)\rUpdating files:  14% (238/1698)\rUpdating files:  15% (255/1698)\rUpdating files:  16% (272/1698)\rUpdating files:  17% (289/1698)\rUpdating files:  18% (306/1698)\rUpdating files:  19% (323/1698)\rUpdating files:  20% (340/1698)\rUpdating files:  21% (357/1698)\rUpdating files:  22% (374/1698)\rUpdating files:  23% (391/1698)\rUpdating files:  24% (408/1698)\rUpdating files:  25% (425/1698)\rUpdating files:  26% (442/1698)\rUpdating files:  27% (459/1698)\rUpdating files:  28% (476/1698)\rUpdating files:  29% (493/1698)\rUpdating files:  30% (510/1698)\rUpdating files:  31% (527/1698)\rUpdating files:  32% (544/1698)\rUpdating files:  33% (561/1698)\rUpdating files:  34% (578/1698)\rUpdating files:  35% (595/1698)\rUpdating files:  36% (612/1698)\rUpdating files:  37% (629/1698)\rUpdating files:  38% (646/1698)\rUpdating files:  39% (663/1698)\rUpdating files:  40% (680/1698)\rUpdating files:  41% (697/1698)\rUpdating files:  42% (714/1698)\rUpdating files:  43% (731/1698)\rUpdating files:  44% (748/1698)\rUpdating files:  45% (765/1698)\rUpdating files:  46% (782/1698)\rUpdating files:  47% (799/1698)\rUpdating files:  48% (816/1698)\rUpdating files:  49% (833/1698)\rUpdating files:  50% (849/1698)\rUpdating files:  51% (866/1698)\rUpdating files:  52% (883/1698)\rUpdating files:  53% (900/1698)\rUpdating files:  54% (917/1698)\rUpdating files:  55% (934/1698)\rUpdating files:  55% (940/1698)\rUpdating files:  56% (951/1698)\rUpdating files:  57% (968/1698)\rUpdating files:  58% (985/1698)\rUpdating files:  59% (1002/1698)\rUpdating files:  60% (1019/1698)\rUpdating files:  61% (1036/1698)\rUpdating files:  62% (1053/1698)\rUpdating files:  63% (1070/1698)\rUpdating files:  64% (1087/1698)\rUpdating files:  65% (1104/1698)\rUpdating files:  66% (1121/1698)\rUpdating files:  67% (1138/1698)\rUpdating files:  68% (1155/1698)\rUpdating files:  69% (1172/1698)\rUpdating files:  70% (1189/1698)\rUpdating files:  71% (1206/1698)\rUpdating files:  72% (1223/1698)\rUpdating files:  73% (1240/1698)\rUpdating files:  74% (1257/1698)\rUpdating files:  75% (1274/1698)\rUpdating files:  76% (1291/1698)\rUpdating files:  77% (1308/1698)\rUpdating files:  78% (1325/1698)\rUpdating files:  79% (1342/1698)\rUpdating files:  80% (1359/1698)\rUpdating files:  81% (1376/1698)\rUpdating files:  82% (1393/1698)\rUpdating files:  83% (1410/1698)\rUpdating files:  84% (1427/1698)\rUpdating files:  85% (1444/1698)\rUpdating files:  86% (1461/1698)\rUpdating files:  87% (1478/1698)\rUpdating files:  88% (1495/1698)\rUpdating files:  89% (1512/1698)\rUpdating files:  90% (1529/1698)\rUpdating files:  91% (1546/1698)\rUpdating files:  92% (1563/1698)\rUpdating files:  93% (1580/1698)\rUpdating files:  94% (1597/1698)\rUpdating files:  95% (1614/1698)\rUpdating files:  96% (1631/1698)\rUpdating files:  97% (1648/1698)\rUpdating files:  98% (1665/1698)\rUpdating files:  99% (1682/1698)\r"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCC2 正在部署数据到 Volume: /Volumes/workspace/default/course_data/ ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating files: 100% (1698/1698)\rUpdating files: 100% (1698/1698), done.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ 数据已同步: BDA1_Data\n   ✅ 数据已同步: BDA2_Data\n   ✅ 数据已同步: Database_Data\n\uD83D\uDCD8 正在部署课件到 Workspace: /Workspace/Users/bdaconsulting1098@gmail.com/Data_Course_101 ...\n   ✅ 课件已安装: BDA1\n   ✅ 课件已安装: BDA2\n========================================\n\uD83D\uDE80 全部完成！\n1. 数据路径: /Volumes/workspace/default/course_data/\n2. 课件路径: /Workspace/Users/bdaconsulting1098@gmail.com/Data_Course_101\n\uD83D\uDC49 请刷新左侧 Workspace 查看您的文件。\n"
     ]
    }
   ],
   "source": [
    "# ================= 配置区域 (无需修改) =================\n",
    "GITHUB_REPO_URL = \"https://github.com/bdaconsulting1098-creator/bda_course.git\"\n",
    "COURSE_NAME = \"Data_Course_101\"  # 用户文件夹里显示的项目名\n",
    "CATALOG_NAME = \"workspace\"       # 目标 Catalog\n",
    "SCHEMA_NAME = \"default\"          # 目标 Schema\n",
    "VOLUME_NAME = \"course_data\"      # 存放数据的 Volume\n",
    "\n",
    "# 定义数据映射: { GitHub里的路径 : Volume里的子文件夹名 }\n",
    "DATA_MAPPINGS = {\n",
    "    \"BDA1/data\": \"BDA1_Data\",\n",
    "    \"BDA2/data\": \"BDA2_Data\",\n",
    "    \"database_data\": \"Database_Data\"\n",
    "}\n",
    "\n",
    "# 定义需要复制给学生的课件文件夹 (排除 data)\n",
    "COURSE_FOLDERS = [\"BDA1\", \"BDA2\"]\n",
    "\n",
    "# ================= 自动执行逻辑 =================\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 1. 动态获取当前用户身份\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "print(f\"\uD83D\uDC4B 欢迎, {current_user}! 正在为您准备课程环境...\")\n",
    "\n",
    "# 2. 定义路径\n",
    "temp_clone_dir = f\"/tmp/{COURSE_NAME}_temp\"\n",
    "user_workspace_dir = f\"/Workspace/Users/{current_user}/{COURSE_NAME}\"\n",
    "volume_path = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}/\"\n",
    "\n",
    "# 3. 【新增功能】尝试自动创建 Volume\n",
    "print(f\"\uD83D\uDEE0️  正在检查 Volume 环境...\")\n",
    "try:\n",
    "    # 使用 Spark SQL 创建 Volume (如果不存)\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}.{VOLUME_NAME}\")\n",
    "    print(f\"   ✅ Volume 已准备就绪: {CATALOG_NAME}.{SCHEMA_NAME}.{VOLUME_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️ 自动创建 Volume 失败 (可能是权限不足)，尝试直接访问...\")\n",
    "    print(f\"   错误信息: {str(e)}\")\n",
    "\n",
    "# 4. 清理旧环境 & 克隆仓库\n",
    "print(f\"⬇️  正在从 GitHub 克隆资料...\")\n",
    "if os.path.exists(temp_clone_dir):\n",
    "    shutil.rmtree(temp_clone_dir, ignore_errors=True)\n",
    "\n",
    "# 使用 magic command 运行 shell 命令\n",
    "result = os.system(f\"git clone {GITHUB_REPO_URL} {temp_clone_dir}\")\n",
    "\n",
    "if result != 0:\n",
    "    raise Exception(\"❌ GitHub 克隆失败，请检查 URL 或网络连接。\")\n",
    "\n",
    "# 5. 部署数据到 Volume\n",
    "print(f\"\uD83D\uDCC2 正在部署数据到 Volume: {volume_path} ...\")\n",
    "# 再次确保 OS 层面的目录存在\n",
    "os.makedirs(volume_path, exist_ok=True)\n",
    "\n",
    "for repo_subpath, volume_subfolder in DATA_MAPPINGS.items():\n",
    "    src_path = os.path.join(temp_clone_dir, repo_subpath)\n",
    "    dst_path = os.path.join(volume_path, volume_subfolder)\n",
    "    \n",
    "    if os.path.exists(src_path):\n",
    "        # 如果目标文件夹已存在，先删除旧的\n",
    "        if os.path.exists(dst_path):\n",
    "            shutil.rmtree(dst_path)\n",
    "        # 复制整个文件夹到 Volume\n",
    "        shutil.copytree(src_path, dst_path)\n",
    "        print(f\"   ✅ 数据已同步: {volume_subfolder}\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ 警告: 在仓库中未找到 {repo_subpath}，跳过。\")\n",
    "\n",
    "# 6. 部署 Notebooks 到用户 Workspace\n",
    "print(f\"\uD83D\uDCD8 正在部署课件到 Workspace: {user_workspace_dir} ...\")\n",
    "\n",
    "# 过滤函数：复制时忽略名为 'data'、'__pycache__' 及隐藏文件夹\n",
    "def ignore_unwanted_folders(dir, files):\n",
    "    ignore_list = []\n",
    "    for f in files:\n",
    "        if f == 'data' or f == '__pycache__' or f.startswith('.'):\n",
    "            ignore_list.append(f)\n",
    "    return ignore_list\n",
    "\n",
    "for folder in COURSE_FOLDERS:\n",
    "    src_path = os.path.join(temp_clone_dir, folder)\n",
    "    dst_path = os.path.join(user_workspace_dir, folder)\n",
    "    \n",
    "    if os.path.exists(src_path):\n",
    "        if os.path.exists(dst_path):\n",
    "            shutil.rmtree(dst_path)\n",
    "        \n",
    "        # 复制文件夹 (自动剔除 data、__pycache__、隐藏文件夹)\n",
    "        shutil.copytree(src_path, dst_path, ignore=ignore_unwanted_folders)\n",
    "        print(f\"   ✅ 课件已安装: {folder}\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ 未找到课件文件夹: {folder}\")\n",
    "\n",
    "# 7. 清理临时文件\n",
    "shutil.rmtree(temp_clone_dir, ignore_errors=True)\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(f\"\uD83D\uDE80 全部完成！\")\n",
    "print(f\"1. 数据路径: {volume_path}\")\n",
    "print(f\"2. 课件路径: {user_workspace_dir}\")\n",
    "print(\"\uD83D\uDC49 请刷新左侧 Workspace 查看您的文件。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -8814701485042684,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae2ffc5-234d-4d88-b3af-da503d7491f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD04 正在将 Database_Data 中的文件转为 Delta 表...\n✅ 目标 Schema 已准备: workspace.course_db\n   -> 正在创建表: HumanResources_Department\n   -> 正在创建表: HumanResources_Employee\n   -> 正在创建表: HumanResources_EmployeeDepartmentHistory\n   -> 正在创建表: HumanResources_EmployeePayHistory\n   -> 正在创建表: HumanResources_JobCandidate\n   -> 正在创建表: HumanResources_Shift\n   -> 正在创建表: HumanResources_vEmployee\n   -> 正在创建表: HumanResources_vEmployeeDepartment\n      ✅ Delta 表 HumanResources_Shift 已创建并导入成功。\n   -> 正在创建表: HumanResources_vEmployeeDepartmentHistory\n      ✅ Delta 表 HumanResources_Employee 已创建并导入成功。\n   -> 正在创建表: HumanResources_vJobCandidate\n      ✅ Delta 表 HumanResources_EmployeePayHistory 已创建并导入成功。\n   -> 正在创建表: HumanResources_vJobCandidateEducation\n      ✅ Delta 表 HumanResources_Department 已创建并导入成功。\n   -> 正在创建表: HumanResources_vJobCandidateEmployment\n      ✅ Delta 表 HumanResources_EmployeeDepartmentHistory 已创建并导入成功。\n   -> 正在创建表: Person_Address\n      ✅ Delta 表 HumanResources_JobCandidate 已创建并导入成功。\n   -> 正在创建表: Person_AddressType\n      ✅ Delta 表 HumanResources_vEmployeeDepartment 已创建并导入成功。\n   -> 正在创建表: Person_BusinessEntity\n      ✅ Delta 表 HumanResources_vEmployee 已创建并导入成功。\n   -> 正在创建表: Person_BusinessEntityAddress\n      ✅ Delta 表 HumanResources_vEmployeeDepartmentHistory 已创建并导入成功。\n   -> 正在创建表: Person_BusinessEntityContact\n      ✅ Delta 表 HumanResources_vJobCandidateEmployment 已创建并导入成功。\n   -> 正在创建表: Person_ContactType\n      ✅ Delta 表 HumanResources_vJobCandidate 已创建并导入成功。\n   -> 正在创建表: Person_CountryRegion\n      ✅ Delta 表 HumanResources_vJobCandidateEducation 已创建并导入成功。\n   -> 正在创建表: Person_EmailAddress\n      ✅ Delta 表 Person_AddressType 已创建并导入成功。\n   -> 正在创建表: Person_Password\n      ✅ Delta 表 Person_Address 已创建并导入成功。\n   -> 正在创建表: Person_Person\n      ✅ Delta 表 Person_BusinessEntity 已创建并导入成功。\n   -> 正在创建表: Person_PersonPhone\n      ✅ Delta 表 Person_BusinessEntityAddress 已创建并导入成功。\n   -> 正在创建表: Person_PhoneNumberType\n      ✅ Delta 表 Person_BusinessEntityContact 已创建并导入成功。\n   -> 正在创建表: Person_StateProvince\n      ✅ Delta 表 Person_CountryRegion 已创建并导入成功。\n   -> 正在创建表: Person_vAdditionalContactInfo\n      ✅ Delta 表 Person_EmailAddress 已创建并导入成功。\n   -> 正在创建表: Person_vStateProvinceCountryRegion\n      ✅ Delta 表 Person_Password 已创建并导入成功。\n   -> 正在创建表: Production_BillOfMaterials\n      ✅ Delta 表 Person_ContactType 已创建并导入成功。\n   -> 正在创建表: Production_Culture\n      ✅ Delta 表 Person_Person 已创建并导入成功。\n   -> 正在创建表: Production_Document\n      ✅ Delta 表 Person_PhoneNumberType 已创建并导入成功。\n   -> 正在创建表: Production_Illustration\n      ✅ Delta 表 Person_PersonPhone 已创建并导入成功。\n   -> 正在创建表: Production_Location\n      ✅ Delta 表 Person_StateProvince 已创建并导入成功。\n   -> 正在创建表: Production_Product\n      ✅ Delta 表 Production_BillOfMaterials 已创建并导入成功。\n   -> 正在创建表: Production_ProductCategory\n      ✅ Delta 表 Person_vStateProvinceCountryRegion 已创建并导入成功。\n   -> 正在创建表: Production_ProductCostHistory\n      ✅ Delta 表 Person_vAdditionalContactInfo 已创建并导入成功。\n   -> 正在创建表: Production_ProductDescription\n      ✅ Delta 表 Production_Culture 已创建并导入成功。\n   -> 正在创建表: Production_ProductDocument\n      ✅ Delta 表 Production_Document 已创建并导入成功。\n   -> 正在创建表: Production_ProductInventory\n      ✅ Delta 表 Production_Illustration 已创建并导入成功。\n   -> 正在创建表: Production_ProductListPriceHistory\n      ✅ Delta 表 Production_Location 已创建并导入成功。\n   -> 正在创建表: Production_ProductModel\n      ✅ Delta 表 Production_ProductCategory 已创建并导入成功。\n   -> 正在创建表: Production_ProductModelIllustration\n      ✅ Delta 表 Production_Product 已创建并导入成功。\n   -> 正在创建表: Production_ProductModelProductDescriptionCulture\n      ✅ Delta 表 Production_ProductCostHistory 已创建并导入成功。\n   -> 正在创建表: Production_ProductPhoto\n      ✅ Delta 表 Production_ProductDocument 已创建并导入成功。\n   -> 正在创建表: Production_ProductProductPhoto\n      ✅ Delta 表 Production_ProductInventory 已创建并导入成功。\n   -> 正在创建表: Production_ProductReview\n      ✅ Delta 表 Production_ProductDescription 已创建并导入成功。\n   -> 正在创建表: Production_ProductSubcategory\n      ✅ Delta 表 Production_ProductListPriceHistory 已创建并导入成功。\n   -> 正在创建表: Production_ScrapReason\n      ✅ Delta 表 Production_ProductModel 已创建并导入成功。\n   -> 正在创建表: Production_TransactionHistory\n      ✅ Delta 表 Production_ProductModelIllustration 已创建并导入成功。\n   -> 正在创建表: Production_TransactionHistoryArchive\n"
     ]
    }
   ],
   "source": [
    "# ================= 自动执行逻辑 (第二阶段：入库) =================\n",
    "# 必须依赖 Cell 1 成功运行后定义的变量和路径\n",
    "\n",
    "import os\n",
    "import concurrent.futures\n",
    "from threading import Lock\n",
    "\n",
    "# 定义路径 (重定义关键变量)\n",
    "CATALOG_NAME = \"workspace\"\n",
    "SCHEMA_NAME = \"default\"\n",
    "VOLUME_NAME = \"course_data\"\n",
    "TARGET_CATALOG = \"workspace\"\n",
    "TARGET_SCHEMA = \"course_db\"\n",
    "DATABASE_DATA_VOLUME_FOLDER = \"Database_Data\" \n",
    "\n",
    "volume_base_path = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}\"\n",
    "database_volume_path = f\"{volume_base_path}/{DATABASE_DATA_VOLUME_FOLDER}/\"\n",
    "\n",
    "print(f\"\uD83D\uDD04 正在将 {DATABASE_DATA_VOLUME_FOLDER} 中的文件转为 Delta 表...\")\n",
    "\n",
    "progress_lock = Lock()\n",
    "\n",
    "def sanitize_column_names(df):\n",
    "    forbidden_chars = [' ', ',', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n",
    "    new_columns = []\n",
    "    col_map = {}\n",
    "    for col in df.columns:\n",
    "        new_col = col\n",
    "        for ch in forbidden_chars:\n",
    "            new_col = new_col.replace(ch, '_')\n",
    "        # Ensure uniqueness if duplicate after sanitization\n",
    "        orig_new_col = new_col\n",
    "        suffix = 1\n",
    "        while new_col in new_columns:\n",
    "            new_col = f\"{orig_new_col}_{suffix}\"\n",
    "            suffix += 1\n",
    "        new_columns.append(new_col)\n",
    "        col_map[col] = new_col\n",
    "    # Print warning if any columns were renamed\n",
    "    renamed = [(k, v) for k, v in col_map.items() if k != v]\n",
    "    if renamed:\n",
    "        print(f\"      ⚠️ 列名已重命名: {renamed}\")\n",
    "    return df.toDF(*new_columns)\n",
    "\n",
    "def process_file(file_name):\n",
    "    table_name = file_name.replace('.', '_').replace('-', '_').split('_csv')[0]\n",
    "    source_file_path = os.path.join(database_volume_path, file_name)\n",
    "    target_table = f\"{TARGET_CATALOG}.{TARGET_SCHEMA}.{table_name}\"\n",
    "    with progress_lock:\n",
    "        print(f\"   -> 正在创建表: {table_name}\")\n",
    "    try:\n",
    "        df = (spark.read\n",
    "              .option(\"header\", \"true\")\n",
    "              .option(\"inferSchema\", \"true\")\n",
    "              .option(\"delimiter\", \",\")\n",
    "              .csv(source_file_path))\n",
    "        if len(df.columns) == 0:\n",
    "            with progress_lock:\n",
    "                print(f\"      ⚠️ 文件 {file_name} 没有有效的列，已跳过 (可能是空文件或非数据文件)。\")\n",
    "            return\n",
    "        df = sanitize_column_names(df)\n",
    "        # 直接写入 Delta 表，覆盖原表\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "        with progress_lock:\n",
    "            print(f\"      ✅ Delta 表 {table_name} 已创建并导入成功。\")\n",
    "    except Exception as e:\n",
    "        with progress_lock:\n",
    "            print(f\"      ❌ 表 {table_name} 导入失败。请检查文件格式是否为 CSV/Parquet/JSON。\")\n",
    "            print(f\"         错误: {str(e)}\")\n",
    "\n",
    "# 确保目标 Schema 存在\n",
    "try:\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_CATALOG}.{TARGET_SCHEMA}\")\n",
    "    print(f\"✅ 目标 Schema 已准备: {TARGET_CATALOG}.{TARGET_SCHEMA}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Schema 创建失败: {e}\")\n",
    "\n",
    "# 确保目标文件夹存在\n",
    "if not os.path.exists(database_volume_path):\n",
    "    print(\"❌ 转换失败：database_data 文件夹在 Volume 中不存在。请检查 Cell 1 是否成功运行。\")\n",
    "else:\n",
    "    try:\n",
    "        file_names = os.listdir(database_volume_path)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 访问 Volume 路径失败，请检查集群是否挂载正常。错误: {e}\")\n",
    "        file_names = []\n",
    "    data_files = [f for f in file_names if not f.startswith('.') and os.path.isfile(os.path.join(database_volume_path, f))]\n",
    "    if not data_files:\n",
    "        print(\"❌ 转换失败：os.listdir 仍然未找到任何文件。请再次手动刷新 Catalog Explorer，并确认文件是否为空。\")\n",
    "    else:\n",
    "        # 并行处理文件\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            executor.map(process_file, data_files)\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(f\"\uD83C\uDF89 第二阶段完成！数据入库成功。\")\n",
    "print(f\"请刷新 Catalog Explorer，并在 {TARGET_CATALOG}.{TARGET_SCHEMA} 下查看您的新表格。\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "course_installation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}